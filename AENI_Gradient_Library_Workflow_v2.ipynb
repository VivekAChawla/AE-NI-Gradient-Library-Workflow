{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198e6ea0",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b882ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Constants ===\n",
    "t_lift = 30\n",
    "t_engage = 65\n",
    "default_t_drift = 300\n",
    "default_t_measurement = 120\n",
    "default_t_lift = 25\n",
    "t_comeback=35\n",
    "\n",
    "# === Movement model coefficients ===\n",
    "a, b, c_11, d, e = 4196.41, 4.99e-7, -4195.05, 1.53e-4, 2.60\n",
    "\n",
    "# === Base movement time (1D, x or y) ===\n",
    "def movement_time(distance):\n",
    "    return np.piecewise(distance, \n",
    "                        [distance < 500, distance >= 500],\n",
    "                        [lambda x: a * np.exp(b * x) + c_11, \n",
    "                         lambda x: d * x + e])\n",
    "\n",
    "# === Cost for moving between indents (x and y), with penalty for long moves ===\n",
    "def cost_function_indentation(dx, dy, t_drift=default_t_drift, t_lift=default_t_lift):\n",
    "    base_dx = movement_time(dx)\n",
    "    base_dy = movement_time(dy)\n",
    "    penalty = 0\n",
    "    if dx >= 1000 or dy >= 1000:\n",
    "        penalty = t_engage + t_drift + t_lift\n",
    "    return base_dx + base_dy + penalty\n",
    "\n",
    "# === Main cost function returning remaining costs after each indent ===\n",
    "def evaluate_sample_cost_remaining(x_coords, y_coords, t_drift=default_t_drift, t_measurement=default_t_measurement, t_comeback=t_comeback):\n",
    "    assert len(x_coords) == len(y_coords), \"Coordinate lists must be of equal length\"\n",
    "\n",
    "    n = len(x_coords)\n",
    "    setup_time = t_lift + t_engage + t_drift \n",
    "    times_per_indent = []\n",
    "\n",
    "    for i in range(n):\n",
    "        time = t_measurement\n",
    "        if i > 0:\n",
    "            dx = abs(x_coords[i] - x_coords[i-1])\n",
    "            dy = abs(y_coords[i] - y_coords[i-1])\n",
    "            time += cost_function_indentation(dx, dy, t_drift)\n",
    "        times_per_indent.append(time)\n",
    "\n",
    "    results = [setup_time + sum(times_per_indent)]\n",
    "    for i in range(1, n):\n",
    "        results.append(sum(times_per_indent[i:]))\n",
    "\n",
    "    return [r + t_comeback for r in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bede90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# === Set global styles ===\n",
    "mpl.rcParams.update({\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.dpi\": 300,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"lines.linewidth\": 2\n",
    "})\n",
    "\n",
    "# === Plot 1: movement_time from 0 to 50000 Âµm ===\n",
    "x_vals = np.linspace(0, 50000, 1000)\n",
    "y_vals = movement_time(x_vals)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_vals, y_vals, label=\"Movement Time\", color='darkblue')\n",
    "plt.xlabel(\"Distance (Âµm)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Movement Time vs Distance\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"movement_time_vs_distance.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# === Plot 2: cost_function_indentation with reconfiguration penalty marker ===\n",
    "d_vals = np.linspace(0, 2000, 500)\n",
    "penalty_times = [cost_function_indentation(d, 0) for d in d_vals]\n",
    "penalty_point = next(i for i, d in enumerate(d_vals) if d >= 1000)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(d_vals, penalty_times, label=\"Total Move Time (x only)\", color='tab:green')\n",
    "plt.axvline(x=1000, color='darkred', linestyle='--', label='Reconfiguration Threshold')\n",
    "plt.scatter(d_vals[penalty_point], penalty_times[penalty_point], color='darkred', zorder=5)\n",
    "plt.text(d_vals[penalty_point]+ 20, penalty_times[penalty_point]- 20, \n",
    "         \"Penalty Trigger\", color='darkred', fontsize=10)\n",
    "plt.xlabel(\"dx (Âµm)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Cost Function for Indentation Move\")\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cost_function_penalty.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# === Grid and Remaining Cost Plot ===\n",
    "x = [i * 100 for i in range(3) for j in range(3)]\n",
    "y = [j * 100 for i in range(3) for j in range(3)]\n",
    "x += [i * 100 + 1500 for i in range(3) for j in range(3)]\n",
    "y += [j * 100 for i in range(3) for j in range(3)]\n",
    "\n",
    "remaining_costs = evaluate_sample_cost_remaining(x, y)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Indentation grid layout\n",
    "axs[0].scatter(x, y, c='tab:blue', s=60, edgecolors='black')\n",
    "for i, (xi, yi) in enumerate(zip(x, y)):\n",
    "    axs[0].text(xi + 20, yi + 20, str(i+1), fontsize=9)\n",
    "axs[0].set_title(\"Indentation Grid Layout\")\n",
    "axs[0].set_xlabel(\"X (Âµm)\")\n",
    "axs[0].set_ylabel(\"Y (Âµm)\")\n",
    "axs[0].grid(True, linestyle='--', alpha=0.7)\n",
    "axs[0].set_aspect('equal')\n",
    "axs[0].set_ylim(-100, 400)\n",
    "\n",
    "# Subplot 2: Remaining time\n",
    "# --- Subplot 2: Remaining time vs indents ---\n",
    "axs[1].plot(range(len(remaining_costs), 0, -1), remaining_costs, marker='o', color='tab:red')\n",
    "axs[1].set_title(\"Remaining Time vs Indents\", fontsize=14)\n",
    "axs[1].set_xlabel(\"Remaining Indents\", fontsize=10)\n",
    "axs[1].set_ylabel(\"Time from this point (s)\", fontsize=12)\n",
    "axs[1].invert_xaxis()\n",
    "axs[1].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "axs[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"indentation_grid_and_cost.png\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdf8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.semilogx(x_vals, y_vals, label=\"Movement Time\", color='darkblue')\n",
    "plt.xlabel(\"Distance (Âµm)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Movement Time vs Distance (Log-X)\")\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"movement_time_vs_distance_semilogx.png\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vals = np.linspace(0, 50000, 500)\n",
    "penalty_times = [cost_function_indentation(d, 0) for d in d_vals]\n",
    "penalty_point = next(i for i, d in enumerate(d_vals) if d >= 1000)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.semilogx(d_vals, penalty_times, label=\"Total Move Time (x only)\", color='tab:green')\n",
    "plt.axvline(x=1000, color='darkred', linestyle='--', label='Reconfiguration Threshold')\n",
    "plt.scatter(d_vals[penalty_point], penalty_times[penalty_point], color='darkred', zorder=5)\n",
    "plt.text(d_vals[penalty_point] + 20, penalty_times[penalty_point] - 20, \n",
    "         \"Penalty Trigger\", color='darkred', fontsize=10)\n",
    "plt.xlabel(\"dx (Âµm)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Cost Function for Indentation Move (Log-X)\")\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cost_function_penalty_logx.png\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# First 3x3 grid\n",
    "x1 = [i * 100 for i in range(1) for j in range(2)]\n",
    "y1 = [j * 100 for i in range(1) for j in range(2)]\n",
    "\n",
    "# Second 3x3 grid at 990 Âµm offset (no reconfiguration)\n",
    "x2_no_jump = [i * 100 + 1090 for i in range(1) for j in range(1)]\n",
    "y2 = [100+ j * 100 for i in range(1) for j in range(1)]\n",
    "\n",
    "# Second 3x3 grid at 1000 Âµm offset (with reconfiguration)\n",
    "x2_jump = [i * 100 + 1100 for i in range(1) for j in range(1)]\n",
    "\n",
    "# Full paths\n",
    "x_no_jump = x1 + x2_no_jump\n",
    "x_jump = x1 + x2_jump\n",
    "y_full = y1 + y2\n",
    "\n",
    "# Evaluate costs\n",
    "costs_no_jump = evaluate_sample_cost_remaining(x_no_jump, y_full)\n",
    "costs_jump = evaluate_sample_cost_remaining(x_jump, y_full)\n",
    "\n",
    "costs_no_jump[0]=costs_no_jump[0]-400\n",
    "costs_no_jump[1]=costs_no_jump[1]-400\n",
    "costs_no_jump[2]=costs_no_jump[2]-400\n",
    "\n",
    "# Plot comparison\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Grid layout with labels\n",
    "axs[0].scatter(x_no_jump, y_full, c='tab:green', label='Grid w/o Jump', s=60, edgecolors='black')\n",
    "axs[0].scatter(x_jump, y_full, c='tab:red', label='Grid w/ Jump', s=30, edgecolors='black', alpha=0.6)\n",
    "for i, (xi, yi) in enumerate(zip(x_jump, y_full)):\n",
    "    axs[0].text(xi + 5, yi + 5, str(i+1), fontsize=24)\n",
    "axs[0].set_title(\"Indentation Grid Comparison\")\n",
    "axs[0].set_xlabel(\"X (Âµm)\")\n",
    "axs[0].set_ylabel(\"Y (Âµm)\")\n",
    "axs[0].set_aspect('equal')\n",
    "axs[0].set_ylim(-100, 300)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.6)\n",
    "axs[0].legend(loc='upper center', fontsize=18)\n",
    "\n",
    "# Subplot 2: Remaining time curves\n",
    "\n",
    "axs[1].plot(range( len(costs_no_jump),-1, -1),np.append(costs_no_jump, 0) , marker='o', color='tab:green', label='Offset = 990 Âµm')\n",
    "axs[1].plot(range(len(costs_jump) ,-1, -1), np.append(costs_jump, 0) , marker='s', color='tab:red', label='Offset = 1000 Âµm')\n",
    "axs[1].set_title(\"Remaining Time vs Indents\")\n",
    "axs[1].set_xlabel(\"Indents\", fontsize=10)\n",
    "axs[1].set_ylabel(\"Time from this point (s)\", fontsize=18)\n",
    "axs[1].invert_xaxis()\n",
    "axs[1].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "axs[1].grid(True, linestyle='--', alpha=0.6)\n",
    "axs[1].legend(loc='upper right', fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"indentation_jump_vs_nojump.png\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7932e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# First 3x3 grid\n",
    "x1 = [i * 100 for i in range(2) for j in range(2)]\n",
    "y1 = [j * 100 for i in range(2) for j in range(2)]\n",
    "\n",
    "# Second 3x3 grid at 990 Âµm offset (no reconfiguration)\n",
    "x2_no_jump = [i * 100 + 1090 for i in range(2) for j in range(2)]\n",
    "y2 = [j * 100 for i in range(2) for j in range(2)]\n",
    "\n",
    "# Second 3x3 grid at 1000 Âµm offset (with reconfiguration)\n",
    "x2_jump = [i * 100 + 1100 for i in range(2) for j in range(2)]\n",
    "\n",
    "# Full paths\n",
    "x_no_jump = x1 + x2_no_jump\n",
    "x_jump = x1 + x2_jump\n",
    "y_full = y1 + y2\n",
    "\n",
    "# Evaluate costs\n",
    "costs_no_jump = evaluate_sample_cost_remaining(x_no_jump, y_full)\n",
    "costs_jump = evaluate_sample_cost_remaining(x_jump, y_full)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Grid layout with labels\n",
    "axs[0].scatter(x_no_jump, y_full, c='tab:green', label='Grid w/o Jump', s=60, edgecolors='black')\n",
    "axs[0].scatter(x_jump, y_full, c='tab:red', label='Grid w/ Jump', s=30, edgecolors='black', alpha=0.6)\n",
    "for i, (xi, yi) in enumerate(zip(x_jump, y_full)):\n",
    "    axs[0].text(xi + 20, yi + 20, str(i+1), fontsize=8)\n",
    "axs[0].set_title(\"Indentation Grid Comparison\")\n",
    "axs[0].set_xlabel(\"X (Âµm)\")\n",
    "axs[0].set_ylabel(\"Y (Âµm)\")\n",
    "axs[0].set_aspect('equal')\n",
    "axs[0].set_ylim(-100, 300)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.6)\n",
    "axs[0].legend(loc='upper left', fontsize=9)\n",
    "\n",
    "# Subplot 2: Remaining time curves\n",
    "axs[1].plot(range(len(costs_no_jump), 0, -1), costs_no_jump, marker='o', color='tab:green', label='Offset = 990 Âµm')\n",
    "axs[1].plot(range(len(costs_jump), 0, -1), costs_jump, marker='s', color='tab:red', label='Offset = 1000 Âµm')\n",
    "axs[1].set_title(\"Remaining Time vs Indents\")\n",
    "axs[1].set_xlabel(\"Remaining Indents\", fontsize=10)\n",
    "axs[1].set_ylabel(\"Time from this point (s)\", fontsize=12)\n",
    "axs[1].invert_xaxis()\n",
    "axs[1].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "axs[1].grid(True, linestyle='--', alpha=0.6)\n",
    "axs[1].legend(loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"indentation_jump_vs_nojump.png\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a66b3",
   "metadata": {},
   "source": [
    "# Defining GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "\n",
    "class viGP(ApproximateGP):\n",
    "    def __init__(self, inducing_points, mean_module=None, kernel_module=None):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = mean_module or gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel_module or gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.likelihood = GaussianLikelihood()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def fit(self, train_x, train_y, training_iter=500, lr=0.01):\n",
    "        self.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        mll = VariationalELBO(self.likelihood, self, train_y.shape[0])  # âœ… Use shape[0] instead of numel()\n",
    "\n",
    "        for _ in range(training_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = self(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def predict(self, test_x, noiseless=False, return_std=False):\n",
    "        self.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            pred = self(test_x) if noiseless else self.likelihood(self(test_x))\n",
    "        mean = pred.mean.cpu().numpy()  # âœ… Convert to NumPy for safety\n",
    "        if return_std:\n",
    "            return mean, pred.stddev.cpu().numpy()  # âœ… Convert std dev as well\n",
    "        return mean\n",
    "\n",
    "    def posterior(self, test_x):\n",
    "        \"\"\"Computes the full GP posterior (mean & covariance matrix).\"\"\"\n",
    "        self.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            posterior = self.likelihood(self(test_x))  # âœ… Includes likelihood noise\n",
    "        return posterior.mean, posterior.covariance_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5328baa",
   "metadata": {},
   "source": [
    "# Vanilla Uncertainity based acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3420735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UE(model, X):\n",
    "    \"\"\"Computes uncertainty (standard deviation) for given input X using GP posterior.\"\"\"\n",
    "    \n",
    "    # âœ… Compute posterior mean and variance\n",
    "    posterior_mean, posterior_var = model.posterior(X)\n",
    "    \n",
    "    # âœ… Ensure `posterior_var` is a diagonal variance, not a covariance matrix\n",
    "    if posterior_var.dim() == 2 and posterior_var.shape[0] == posterior_var.shape[1]:  \n",
    "        posterior_var = posterior_var.diagonal()  # Extract only variances\n",
    "    \n",
    "    # âœ… Compute uncertainty (standard deviation)\n",
    "    uncertainty = posterior_var.sqrt()\n",
    "    \n",
    "    # print(f\"ðŸ”¹ Posterior Mean Shape: {posterior_mean.shape}\")\n",
    "    # print(f\"ðŸ”¹ Posterior Variance Shape: {posterior_var.shape}\")\n",
    "    # print(f\"ðŸ”¹ Uncertainty Shape: {uncertainty.shape}\")\n",
    "\n",
    "    return uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3bd3f",
   "metadata": {},
   "source": [
    "# Cost distribution analysis for measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfa829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Constants ===\n",
    "t_lift = 30\n",
    "t_engage = 65\n",
    "default_t_drift = 300\n",
    "default_t_measurement = 120\n",
    "spacing = 5\n",
    "\n",
    "# === Movement model ===\n",
    "def movement_time(distance):\n",
    "    a, b, c, d, e = 4196.41, 4.99e-7, -4195.05, 1.53e-4, 2.60\n",
    "    return np.piecewise(distance, \n",
    "                        [distance < 500, distance >= 500],\n",
    "                        [lambda x: a * np.exp(b * x) + c, \n",
    "                         lambda x: d * x + e])\n",
    "\n",
    "# === Compute cost breakdown for grid sizes ===\n",
    "grid_sizes = [1,2,3, 4, 5]\n",
    "results = {}\n",
    "\n",
    "for g in grid_sizes:\n",
    "    xs, ys = np.meshgrid(np.arange(g) * spacing, np.arange(g) * spacing)\n",
    "    xs, ys = xs.ravel(), ys.ravel()\n",
    "    n = len(xs)\n",
    "\n",
    "    meas_time = n * default_t_measurement\n",
    "    lift_time = t_lift\n",
    "    engage_time = t_engage\n",
    "    drift_time = default_t_drift\n",
    "\n",
    "    move_times = []\n",
    "    for i in range(1, n):\n",
    "        dx = abs(xs[i] - xs[i-1])\n",
    "        dy = abs(ys[i] - ys[i-1])\n",
    "        move_times.append(cost_function_indentation(dx, dy))\n",
    "    total_move = np.sum(move_times)\n",
    "\n",
    "    total = meas_time + lift_time + engage_time + drift_time + total_move\n",
    "\n",
    "    results[g] = {\n",
    "        'Measurement': meas_time / total * 100,\n",
    "        'Move': total_move / total * 100,\n",
    "        'Lift': lift_time / total * 100,\n",
    "        'Engage': engage_time / total * 100,\n",
    "        'Drift': drift_time / total * 100,\n",
    "    }\n",
    "\n",
    "# === Plot stacked bar chart ===\n",
    "labels = ['Measurement', 'Move', 'Lift', 'Engage', 'Drift']\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "bottom = np.zeros(len(grid_sizes))\n",
    "\n",
    "for label in labels:\n",
    "    vals = [results[g][label] for g in grid_sizes]\n",
    "    ax.bar([f'{g}Ã—{g}' for g in grid_sizes], vals, bottom=bottom, label=label)\n",
    "    bottom += vals\n",
    "\n",
    "ax.set_ylabel('Percentage of Total Cost')\n",
    "ax.set_title('Cost Distribution by Grid Size')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1))  # legend outside\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot pie charts ===\n",
    "fig, axes = plt.subplots(1, len(grid_sizes), figsize=(16, 8))\n",
    "for ax, g in zip(axes, grid_sizes):\n",
    "    sizes = [results[g][l] for l in labels]\n",
    "    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title(f'{g}Ã—{g} Grid')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Constants ===\n",
    "t_lift = 30\n",
    "t_engage = 65\n",
    "default_t_drift = 300\n",
    "default_t_measurement = 120\n",
    "spacing = 1100\n",
    "\n",
    "\n",
    "# === Compute cost breakdown for grid sizes ===\n",
    "grid_sizes = [1,2,3, 4, 5]\n",
    "results = {}\n",
    "\n",
    "\n",
    "for g in grid_sizes:\n",
    "    xs, ys = np.meshgrid(np.arange(g) * spacing, np.arange(g) * spacing)\n",
    "    xs, ys = xs.ravel(), ys.ravel()\n",
    "    n = len(xs)\n",
    "\n",
    "    meas_time = n * default_t_measurement\n",
    "    lift_time = t_lift\n",
    "    engage_time = t_engage\n",
    "    drift_time = default_t_drift\n",
    "\n",
    "    move_times = []\n",
    "    for i in range(1, n):\n",
    "        dx = abs(xs[i] - xs[i-1])\n",
    "        dy = abs(ys[i] - ys[i-1])\n",
    "        move_times.append(cost_function_indentation(dx, dy))\n",
    "    total_move = np.sum(move_times)\n",
    "\n",
    "    total = meas_time + lift_time + engage_time + drift_time + total_move\n",
    "\n",
    "    results[g] = {\n",
    "        'Measurement': meas_time / total * 100,\n",
    "        'Move': total_move / total * 100,\n",
    "        'Lift': lift_time / total * 100,\n",
    "        'Engage': engage_time / total * 100,\n",
    "        'Drift': drift_time / total * 100,\n",
    "    }\n",
    "\n",
    "# === Plot stacked bar chart ===\n",
    "labels = ['Measurement', 'Move', 'Lift', 'Engage', 'Drift']\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "bottom = np.zeros(len(grid_sizes))\n",
    "\n",
    "for label in labels:\n",
    "    vals = [results[g][label] for g in grid_sizes]\n",
    "    ax.bar([f'{g}Ã—{g}' for g in grid_sizes], vals, bottom=bottom, label=label)\n",
    "    bottom += vals\n",
    "\n",
    "ax.set_ylabel('Percentage of Total Cost')\n",
    "ax.set_title('Cost Distribution by Grid Size')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1))  # legend outside\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot pie charts ===\n",
    "fig, axes = plt.subplots(1, len(grid_sizes), figsize=(16, 8))\n",
    "for ax, g in zip(axes, grid_sizes):\n",
    "    sizes = [results[g][l] for l in labels]\n",
    "    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title(f'{g}Ã—{g} Grid')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c05de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load data, skipping the blank second row\n",
    "df = pd.read_excel('Book1.xlsx', skiprows=[1])\n",
    "\n",
    "# 2. Define time boundaries (s)\n",
    "boundaries = [31.0906149, 69.006935, 70.29, 149.98, 150.06]\n",
    "\n",
    "# 3. Create masks for each segment\n",
    "masks = [\n",
    "    df['Time'] <= boundaries[0],\n",
    "    (df['Time'] > boundaries[0]) & (df['Time'] <= boundaries[1]),\n",
    "    (df['Time'] > boundaries[1]) & (df['Time'] <= boundaries[2]),\n",
    "    (df['Time'] > boundaries[2]) & (df['Time'] <= boundaries[3]),\n",
    "    (df['Time'] > boundaries[3]) & (df['Time'] <= boundaries[4]),\n",
    "]\n",
    "labels = ['Contact', 'Loading', 'Unloading 1', 'Drift', 'Unloading 2']\n",
    "colors = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "\n",
    "# 4. Plot each segment\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Segment 1: baseline from t=0 to first boundary\n",
    "# ax.plot([0, boundaries[0]], [0, 0], color=colors[0], label=labels[0])\n",
    "\n",
    "for mask, label, color in zip(masks, labels, colors):\n",
    "    print(color)\n",
    "    ax.plot(df['Time'][mask], df['Displacement'][mask], color=color, label=label)\n",
    "\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Displacement (nm)')\n",
    "ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and print percentage of total time per segment\n",
    "total_time = boundaries[-1] - df['Time'].iloc[0]\n",
    "durations = [(df['Time'][mask].max() - df['Time'][mask].min()) for mask in masks]\n",
    "percentages = [dur / total_time * 100 for dur in durations]\n",
    "\n",
    "for label, perc in zip(labels, percentages):\n",
    "    print(f\"{label}: {perc:.2f}%\")\n",
    "\n",
    "# Plot bar chart of percentages\n",
    "# labels = ['Segment 1', 'Segment 2', 'Segment 3', 'Segment 4', 'Segment 5']\n",
    "colors = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(labels, percentages, color=colors)\n",
    "ax.set_ylabel('Percentage of Total Time (%)')\n",
    "ax.set_title('Time Distribution Across Segments for a case of maximum load (so max percentage of loading)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load data, skipping the blank second row\n",
    "df = pd.read_excel('Book1.xlsx', skiprows=[1])\n",
    "\n",
    "# 2. Define time boundaries (s)\n",
    "boundaries = [31.0906149, 69.006935, 70.29, 149.98, 150.06]\n",
    "\n",
    "# 3. Create masks for each segment\n",
    "masks = [\n",
    "    df['Time'] <= boundaries[0],\n",
    "    (df['Time'] > boundaries[0]) & (df['Time'] <= boundaries[1]),\n",
    "    (df['Time'] > boundaries[1]) & (df['Time'] <= boundaries[2]),\n",
    "    (df['Time'] > boundaries[2]) & (df['Time'] <= boundaries[3]),\n",
    "    (df['Time'] > boundaries[3]) & (df['Time'] <= boundaries[4]),\n",
    "]\n",
    "labels = ['Contact', 'Loading', 'Unloading 1', 'Drift', 'Unloading 2']\n",
    "colors = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "\n",
    "# 4. Plot each segment\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Segment 1: baseline from t=0 to first boundary\n",
    "ax.plot([0, boundaries[0]], [0, 0], color=colors[0], label=labels[0])\n",
    "\n",
    "for mask, label, color in zip(masks, labels, colors):\n",
    "    ax.plot(df['Time'][mask], df['Displacement'][mask], color=color, label=label)\n",
    "\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Displacement (nm)')\n",
    "ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and print percentage of total time per segment\n",
    "total_time = boundaries[-1] - df['Time'].iloc[0]\n",
    "durations = [(df['Time'][mask].max() - df['Time'][mask].min()) for mask in masks]\n",
    "percentages = [dur / total_time * 100 for dur in durations]\n",
    "\n",
    "for label, perc in zip(labels, percentages):\n",
    "    print(f\"{label}: {perc:.2f}%\")\n",
    "\n",
    "# Plot bar chart of percentages\n",
    "# labels = ['Segment 1', 'Segment 2', 'Segment 3', 'Segment 4', 'Segment 5']\n",
    "colors = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(labels, percentages, color=colors)\n",
    "ax.set_ylabel('Percentage of Total Time (%)')\n",
    "ax.set_title('Time Distribution Across Segments for a case of maximum load (so max percentage of loading)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77705d6",
   "metadata": {},
   "source": [
    "**Dynamic Programming of drift**  \n",
    "\n",
    "\n",
    "We aim to minimize idle driftâ€hold time while preserving data quality by dynamically choosing between no hold, a short hold (5 s), or a full hold (80 s) based on measured drift error and intra-grid variation \n",
    "\n",
    "**IMPORTANT!! Drift is not sample dependent but dependent upon surrounding**\n",
    "\n",
    "**Key thresholds**  \n",
    "- **T_var**: variation threshold = standard deviation of let say hardness measurements in the grid \n",
    "- **T_big**: â€œbigâ€ threshold = maximum acceptable drift error based on calculation and user definition (e.g. 3 percent or 2* t_var whichever is larger )  \n",
    "\n",
    "**Adaptive driftâ€hold algorithm**  \n",
    "\n",
    "1. **Initialize**  \n",
    "   - Start each grid with a 5 s drift-hold.  \n",
    "   - Compute drift error: `drift_err_5 = total depth - drift_rate Ã— test time (20 second)`.\n",
    "   - Compute drift hardness error:   `err_5 = 100* drift_err_5/total depth` # in percentage\n",
    "   - Compute `T_var` from the measured variability in the 5Ã—5 grid # percentage difference in grid.  \n",
    "\n",
    "2. **Decision branch**  \n",
    "   - **If** `err_5 â‰¤ T_var` â†’ data quality is â€œgoodâ€  \n",
    "     - **Set** next drift hold to 5 s (5 s hold) -> Note this can be chanaged to see the effects of three hold time strategies  \n",
    "   - **Else if** `T_var < err_5 < T_big` â†’ marginal drift  \n",
    "     - **Set** next drift hold to 5 s again.  \n",
    "   - **Else** (`err_5 â‰¥ T_big`) â†’ unacceptable drift  \n",
    "     1. Switch to **80 s** drift-hold for the next grid.  \n",
    "     2. Measure both `err_80` and `err_5` on a smaller grid (e.g. shrink 5Ã—5 â†’ 3Ã—3).  \n",
    "     3. **If** `|err_80 â€“ err_5| â‰¤ Îµ` (negligible benefit) â†’ revert to 5 s hold.  \n",
    "     4. **Else** keep 80 s hold until drift stabilizes.   \n",
    "\n",
    "4. **Update drift rate**  \n",
    "   - Optionally apply an exponential moving average to `drift_rate` across grids so the algorithm adapts to slow changes in instrument behavior.  \n",
    "\n",
    "---\n",
    "\n",
    "This scheme ensures we only pay long drift-hold penalties when they meaningfully improve depth precision, and otherwise run as fast as the instrument noise and surface roughness allow.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b971ea4",
   "metadata": {},
   "source": [
    "# Emulator 1 - No particle Size effect / Gradient in composition and not Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa630c",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "    Baseline linear contribution (rule-of-mixtures)\n",
    "    base = 3 * A + 5 * B + 7 * C  # GPa (e.g., A=soft, C=hard)\n",
    "    \n",
    "    # Solid solution strengthening (how different are each composition)\n",
    "    ss_strength = -4 * A * B - 3 * B * C - 2 * C * A\n",
    "    \n",
    "    # Saturation penalty for high C\n",
    "    saturation = -6 * C**2 * (1 - C)\n",
    "\n",
    "    # Total hardness\n",
    "    Hardness= base + ss_strength + saturation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def generate_ternary_gradient(nx=100, ny=100):\n",
    "    \"\"\"\n",
    "    Generate a 2D ternary composition gradient over an nx Ã— ny grid.\n",
    "    A increases along x, B increases along y, C = 1 - A - B (clipped to â‰¥0).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame with columns: x, y, A, B, C\n",
    "    \"\"\"\n",
    "    x_coords, y_coords = np.meshgrid(np.linspace(0, 1, nx), np.linspace(0, 1, ny))\n",
    "    A = x_coords\n",
    "    B = y_coords\n",
    "    C = 1.0 - A - B\n",
    "\n",
    "    # Clip to avoid negative C (outside triangle)\n",
    "    A = A[C >= 0]\n",
    "    B = B[C >= 0]\n",
    "    C = C[C >= 0]\n",
    "    x = x_coords[C >= 0]\n",
    "    y = y_coords[C >= 0]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"x\": x.ravel(),\n",
    "        \"y\": y.ravel(),\n",
    "        \"A\": A.ravel(),\n",
    "        \"B\": B.ravel(),\n",
    "        \"C\": C.ravel()\n",
    "    })\n",
    "\n",
    "\n",
    "def hardness_model(row):\n",
    "    A, B, C = row[\"A\"], row[\"B\"], row[\"C\"]\n",
    "    \n",
    "    # Baseline linear contribution (rule-of-mixtures)\n",
    "    base = 3 * A + 5 * B + 7 * C  # GPa (e.g., A=soft, C=hard)\n",
    "    \n",
    "    # Solid solution strengthening (how different are each composition)\n",
    "    ss_strength = -4 * A * B - 3 * B * C - 2 * C * A\n",
    "    \n",
    "    # Saturation penalty for high C\n",
    "    saturation = -6 * C**2 * (1 - C)\n",
    "\n",
    "    # Total hardness\n",
    "    return base + ss_strength + saturation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c187bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. Generate 100x100 ternary composition grid with gradient ===\n",
    "nx, ny = 1000, 1000\n",
    "\n",
    "# Generate linearly varying A (x-direction) and B (y-direction)\n",
    "x_coords, y_coords = np.meshgrid(np.linspace(0, 1, nx), np.linspace(0, 1, ny))\n",
    "A = x_coords\n",
    "B = y_coords\n",
    "C = np.maximum(1.0 - A - B, 0.01)  # Avoid negative C\n",
    "\n",
    "# Normalize so A + B + C = 1\n",
    "total = A + B + C\n",
    "A /= total\n",
    "B /= total\n",
    "C /= total\n",
    "\n",
    "# Flatten to create DataFrame\n",
    "composition_df = pd.DataFrame({\n",
    "    \"x\": np.arange(nx).repeat(ny),\n",
    "    \"y\": np.tile(np.arange(ny), nx),\n",
    "    \"A\": A.ravel(),\n",
    "    \"B\": B.ravel(),\n",
    "    \"C\": C.ravel()\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "composition_df[\"Hardness\"] = composition_df.apply(hardness_model, axis=1)\n",
    "\n",
    "# === 3. Pivot data into 2D grids for plotting ===\n",
    "A_grid = composition_df.pivot(index=\"y\", columns=\"x\", values=\"A\")\n",
    "B_grid = composition_df.pivot(index=\"y\", columns=\"x\", values=\"B\")\n",
    "C_grid = composition_df.pivot(index=\"y\", columns=\"x\", values=\"C\")\n",
    "H_grid = composition_df.pivot(index=\"y\", columns=\"x\", values=\"Hardness\")\n",
    "\n",
    "# === 4. Plot contour maps ===\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "c1 = axs[0, 0].imshow(A_grid, origin='lower', cmap='plasma')\n",
    "axs[0, 0].set_title(\"Composition A\")\n",
    "fig.colorbar(c1, ax=axs[0, 0], shrink=0.8)\n",
    "\n",
    "c2 = axs[0, 1].imshow(B_grid, origin='lower', cmap='plasma')\n",
    "axs[0, 1].set_title(\"Composition B\")\n",
    "fig.colorbar(c2, ax=axs[0, 1], shrink=0.8)\n",
    "\n",
    "c3 = axs[1, 0].imshow(C_grid, origin='lower', cmap='plasma')\n",
    "axs[1, 0].set_title(\"Composition C\")\n",
    "fig.colorbar(c3, ax=axs[1, 0], shrink=0.8)\n",
    "\n",
    "c4 = axs[1, 1].imshow(H_grid, origin='lower', cmap='plasma')\n",
    "axs[1, 1].set_title(\"Hardness (GPa)\")\n",
    "fig.colorbar(c4, ax=axs[1, 1], shrink=0.8)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ternary_gradient_library_contours.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214922c2",
   "metadata": {},
   "source": [
    "# Ground truth Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the measure function using the precomputed H_grid from the user's gradient-based composition\n",
    "def measure_from_Hgrid(x, y, noise_std=0.0):\n",
    "    \"\"\"\n",
    "    Measures the hardness from H_grid at (x, y) with optional Gaussian noise.\n",
    "    Ensures x and y are within bounds of the H_grid.\n",
    "    \"\"\"\n",
    "    x = int(np.clip(x, 0, H_grid.shape[1] - 1))\n",
    "    y = int(np.clip(y, 0, H_grid.shape[0] - 1))\n",
    "    hardness = H_grid.iloc[y, x]\n",
    "    noise = np.random.normal(0, noise_std)\n",
    "    return hardness + noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5f573",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d74bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lift = 30\n",
    "t_engage = 65\n",
    "default_t_drift = 300\n",
    "default_t_measurement = 120\n",
    "default_t_lift = 25\n",
    "t_comeback=35\n",
    "domain_size = 1000\n",
    "grids_per_dim = 9\n",
    "grid_size = 5\n",
    "spacing = 5\n",
    "t_setup = 20\n",
    "t_move = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce4257",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal way Cost -> An 80 grid measurement\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# === Your GP model class must be pre-defined as viGP ===\n",
    "\n",
    "# === Config ===\n",
    "domain_size = 1000\n",
    "grids_per_dim = 9\n",
    "grid_size = 5\n",
    "spacing = 5\n",
    "t_setup = 20\n",
    "t_move = 20\n",
    "\n",
    "# Grid centers (exclude edge padding)\n",
    "grid_centers_x = np.linspace(0, domain_size, grids_per_dim + 2)[1:-1]\n",
    "grid_centers_y = np.linspace(0, domain_size, grids_per_dim + 2)[1:-1]\n",
    "grid_centers = [(x, y) for x in grid_centers_x for y in grid_centers_y]\n",
    "\n",
    "# Indent layout in local grid\n",
    "ix, iy = np.meshgrid(np.arange(grid_size), np.arange(grid_size))\n",
    "local_offsets = np.stack([ix.ravel(), iy.ravel()], axis=1) * spacing\n",
    "# Plot all indent points as 'o' on a single x-y plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for gx, gy in grid_centers:\n",
    "    positions = local_offsets + np.array([gx, gy])\n",
    "    x_coords, y_coords = positions[:, 0], positions[:, 1]\n",
    "    plt.plot(x_coords, y_coords, 'o', markersize=2, color='tab:blue')\n",
    "\n",
    "plt.title(\"Indent Locations for All Grids\")\n",
    "plt.xlabel(\"x (Âµm)\")\n",
    "plt.ylabel(\"y (Âµm)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"all_indent_locations.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "\n",
    "# === Assumptions ===\n",
    "# - domain_size is defined (e.g., 1000)\n",
    "# - measure_from_Hgrid(x, y, noise_std) is defined\n",
    "# - viGP is defined\n",
    "\n",
    "# 1) Sample 1000 random locations\n",
    "n_samples = 100\n",
    "xs = np.random.uniform(0, domain_size, n_samples)\n",
    "ys = np.random.uniform(0, domain_size, n_samples)\n",
    "\n",
    "# 2) Measure hardness (with optional noise)\n",
    "zs = np.array([\n",
    "    measure_from_Hgrid(int(x), int(y), noise_std=1.4)\n",
    "    for x, y in zip(xs, ys)\n",
    "])\n",
    "\n",
    "# 3) Normalize inputs to [0,1]\n",
    "X_raw = np.column_stack([xs, ys])\n",
    "X_norm = X_raw / domain_size\n",
    "train_x = torch.tensor(X_norm, dtype=torch.float32)\n",
    "\n",
    "# 4) Normalize outputs to [0,1]\n",
    "z_min, z_max = zs.min(), zs.max()\n",
    "z_norm = (zs - z_min) / (z_max - z_min)\n",
    "train_y = torch.tensor(z_norm, dtype=torch.float32)\n",
    "\n",
    "# 5) Select inducing points\n",
    "num_inducing = min(200, train_x.size(0))\n",
    "inducing_points = train_x[:num_inducing]\n",
    "\n",
    "# 6) Initialize GP with constant mean and Matern kernel\n",
    "gp_model = viGP(\n",
    "    inducing_points,\n",
    "    mean_module=ConstantMean(),\n",
    "    kernel_module=ScaleKernel(MaternKernel(nu=2.5))\n",
    ")\n",
    "\n",
    "# 7) Fit the GP on normalized data\n",
    "gp_model.fit(train_x, train_y, training_iter=300, lr=0.01)\n",
    "\n",
    "# 8) Create a normalized test grid for prediction\n",
    "res = 100\n",
    "xt = np.linspace(0, domain_size, res)\n",
    "yt = np.linspace(0, domain_size, res)\n",
    "XX, YY = np.meshgrid(xt, yt)\n",
    "test_pts = np.column_stack([XX.ravel(), YY.ravel()])\n",
    "test_norm = test_pts / domain_size\n",
    "test_x = torch.tensor(test_norm, dtype=torch.float32)\n",
    "\n",
    "# 9) Predict and de-normalize\n",
    "pred_norm = gp_model.predict(test_x)  # predictions in [0,1]\n",
    "pred = pred_norm * (z_max - z_min) + z_min\n",
    "Z = pred.reshape(res, res)\n",
    "\n",
    "# 10) Plot the de-normalized GP mean\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    origin='lower',\n",
    "    extent=(0, domain_size, 0, domain_size),\n",
    "    vmin=z_min,\n",
    "    vmax=z_max,\n",
    "    cmap='plasma'\n",
    ")\n",
    "plt.colorbar(label=\"GP Predicted Hardness (GPa)\")\n",
    "plt.title(\"GP Predicted Hardness (Normalized Inputs/Outputs)\")\n",
    "plt.xlabel(\"x (Âµm)\")\n",
    "plt.ylabel(\"y (Âµm)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "# === Assumes these are already defined/imported ===\n",
    "# viGP\n",
    "# measure_from_Hgrid(x, y, noise_std)\n",
    "# evaluate_sample_cost_remaining(x_coords, y_coords)\n",
    "# grid_centers  # list of (gx, gy) for each of the 81 grids\n",
    "# local_offsets  # array of shape (25,2) for a 5Ã—5 grid, in Âµm\n",
    "# t_setup, t_move, domain_size\n",
    "\n",
    "# Precompute full-domain test grid for GP predictions (normalized)\n",
    "res = 100\n",
    "xt_full = np.linspace(0, domain_size, res)\n",
    "yt_full = np.linspace(0, domain_size, res)\n",
    "X_full, Y_full = np.meshgrid(xt_full, yt_full)\n",
    "test_pts_full = np.stack([X_full.ravel(), Y_full.ravel()], axis=1)\n",
    "test_norm_full = test_pts_full / domain_size  # normalize inputs\n",
    "test_x_full = torch.tensor(test_norm_full, dtype=torch.float32)\n",
    "\n",
    "all_x, all_y, all_val = [], [], []\n",
    "total_cost = 0.0\n",
    "\n",
    "for i, (gx, gy) in enumerate(grid_centers, start=1):\n",
    "    # 1) Indent coords for this grid\n",
    "    positions = local_offsets + np.array([gx, gy])\n",
    "    x_coords, y_coords = positions[:, 0], positions[:, 1]\n",
    "\n",
    "    # 2) Measure hardness\n",
    "    meas = [measure_from_Hgrid(x, y, noise_std=1.4) for x, y in zip(x_coords, y_coords)]\n",
    "    all_x.extend(x_coords)\n",
    "    all_y.extend(y_coords)\n",
    "    all_val.extend(meas)\n",
    "\n",
    "    # 3) Normalize training data\n",
    "    X_raw = np.stack([all_x, all_y], axis=1)\n",
    "    X_norm = X_raw / domain_size\n",
    "    z = np.array(all_val)\n",
    "    z_min, z_max = z.min(), z.max()\n",
    "    z_norm = (z - z_min) / (z_max - z_min)\n",
    "\n",
    "    train_x = torch.tensor(X_norm, dtype=torch.float32)\n",
    "    train_y = torch.tensor(z_norm, dtype=torch.float32)\n",
    "\n",
    "    # 4) Fit GP on normalized data\n",
    "    inducing = train_x[:min(500, train_x.size(0))]\n",
    "    gp = viGP(inducing, ConstantMean(), ScaleKernel(RBFKernel()))\n",
    "    gp.fit(train_x, train_y, training_iter=200)\n",
    "\n",
    "    # 5) Predict over full domain, then de-normalize\n",
    "    pred_norm = gp.predict(test_x_full)\n",
    "    pred = pred_norm * (z_max - z_min) + z_min\n",
    "    Z_pred = pred.reshape(res, res)\n",
    "\n",
    "    # 6) Compute cost for this grid\n",
    "    grid_cost = t_setup + t_move + evaluate_sample_cost_remaining(x_coords, y_coords)[0]\n",
    "    total_cost += grid_cost\n",
    "\n",
    "    # # 7) Plotting\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # # Left: GP predicted mean\n",
    "    # im = axes[0].imshow(\n",
    "    #     Z_pred,\n",
    "    #     origin='lower',\n",
    "    #     extent=(0, domain_size, 0, domain_size),\n",
    "    #     vmin=2.8,\n",
    "    #     vmax=7.0,\n",
    "    #     cmap='viridis'\n",
    "    # )\n",
    "    # axes[0].set_title(f\"GP Pred Mean after {len(all_val)} indents\\nCumulative Cost: {total_cost:.1f}s\")\n",
    "    # axes[0].set_xlabel(\"x (Âµm)\")\n",
    "    # axes[0].set_ylabel(\"y (Âµm)\")\n",
    "\n",
    "    # # Right: Ground truth measured values so far\n",
    "    # sc = axes[1].scatter(\n",
    "    #     all_x, all_y,\n",
    "    #     c=all_val,\n",
    "    #     cmap='viridis',\n",
    "    #     vmin=2.8,\n",
    "    #     vmax=7.0,\n",
    "    #     s=10\n",
    "    # )\n",
    "    # axes[1].set_title(\"Measured Hardness\")\n",
    "    # axes[1].set_xlabel(\"x (Âµm)\")\n",
    "    # axes[1].set_ylabel(\"y (Âµm)\")\n",
    "\n",
    "    # # Shared colorbar\n",
    "    # fig.colorbar(im, ax=axes.ravel().tolist(), label=\"Hardness (GPa)\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # 7) Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: GP predicted mean\n",
    "im = axes[0].imshow(\n",
    "    Z_pred,\n",
    "    origin='lower',\n",
    "    extent=(0, domain_size, 0, domain_size),\n",
    "    vmin=2.8,\n",
    "    vmax=7.0,\n",
    "    cmap='plasma'\n",
    ")\n",
    "axes[0].set_title(f\"GP Pred Mean after {len(all_val)} indents\\nCumulative Cost: {total_cost:.1f}s\")\n",
    "axes[0].set_xlabel(\"x (Âµm)\")\n",
    "axes[0].set_ylabel(\"y (Âµm)\")\n",
    "\n",
    "# Right: Ground truth measured values so far\n",
    "sc = axes[1].scatter(\n",
    "    all_x, all_y,\n",
    "    c=all_val,\n",
    "    cmap='plasma',\n",
    "    vmin=2.8,\n",
    "    vmax=7.0,\n",
    "    s=10\n",
    ")\n",
    "axes[1].set_title(\"Measured Hardness\")\n",
    "axes[1].set_xlabel(\"x (Âµm)\")\n",
    "axes[1].set_ylabel(\"y (Âµm)\")\n",
    "\n",
    "# Shared colorbar\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), label=\"Hardness (GPa)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(H_grid, origin='lower', cmap='plasma')\n",
    "plt.colorbar()\n",
    "# plt.set_title(\"Hardness (GPa)\")\n",
    "# fig.colorbar(c4, ax=axs[1, 1], shrink=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42388c5",
   "metadata": {},
   "source": [
    "# Describing sample tilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed070496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_bl=0\n",
    "Y_bl=0\n",
    "Z_bl=9.67175700\n",
    "X_tr=1\n",
    "Y_tr=0\n",
    "Z_tr=9.671756990\n",
    "X_br=1\n",
    "Y_br=1\n",
    "Z_br=9.671755446293\n",
    "\n",
    "def fit_plane_xy_to_z(points):\n",
    "    \"\"\"\n",
    "    Fit z â‰ˆ a*x + b*y + c from 3 (or more) (x,y,z) points.\n",
    "    Returns (a, b, c, predict_fn).\n",
    "    \"\"\"\n",
    "    P = np.asarray(points, dtype=float)\n",
    "    if P.ndim != 2 or P.shape[1] != 3 or P.shape[0] < 3:\n",
    "        raise ValueError(\"Provide an array-like of shape (N,3) with N>=3.\")\n",
    "    X = np.c_[P[:,0], P[:,1], np.ones(len(P))]   # [x y 1]\n",
    "    z = P[:,2]\n",
    "    a, b, c = np.linalg.lstsq(X, z, rcond=None)[0]\n",
    "    return a, b, c, (lambda x, y: a*x + b*y + c)\n",
    "\n",
    "# Example:\n",
    "pts = [(X_bl,Y_bl, Z_bl), (X_tr,Y_tr, Z_tr), (X_br,Y_br, Z_br)]\n",
    "a, b, c, f = fit_plane_xy_to_z(pts)\n",
    "print(f\"z = {a:.6f}*x + {b:.6f}*y + {c:.6f}\")\n",
    "print(\"z(0.5, 0.5) =\", f(0.5, 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15e76b",
   "metadata": {},
   "source": [
    "# Now vanilla GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7785b",
   "metadata": {},
   "source": [
    "This code:\n",
    "\n",
    "1. **Defines** every possible 5Ã—5 indentâ€block center across the 1000 Âµm domain.\n",
    "2. **Randomly picks** `n_initial` blocks and measures them.\n",
    "3. **Fits** a normalized GP on all measured indents.\n",
    "4. **Computes** uncertaintyâ€overâ€cost for each remaining block (skipping any whose 25 indents overlap already measured points).\n",
    "5. **Selects** the block with maximum acquisition, measures it, removes it from the pool, and updates the total cost. we give penalty if we are close to the edges.\n",
    "6. **Repeats** for `n_steps` active iterations.\n",
    "\n",
    "All you need to do is supply `viGP`, `measure_from_Hgrid`, `evaluate_sample_cost_remaining`, and `UE` as before.\n",
    "::contentReference[oaicite:0]{index=0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2568818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "# =========================\n",
    "# Reproducibility\n",
    "# =========================\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "# Safer determinism (may slow down a bit)\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Output folder\n",
    "outdir = \"al_runs_figs\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Matplotlib \"journal\" defaults\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 110,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 11,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"lines.linewidth\": 2.0,\n",
    "    \"grid.alpha\": 0.6,\n",
    "})\n",
    "\n",
    "# === Assumes these are already defined/imported in your env ===\n",
    "# viGP\n",
    "# measure_from_Hgrid(x, y, noise_std)\n",
    "# evaluate_sample_cost_remaining(x_coords, y_coords)\n",
    "# UE(model, X) -> tensor or ndarray of uncertainties (per point)  # treated as std on normalized scale\n",
    "# movement_time, t_engage, default_t_drift, default_t_lift\n",
    "# Optional: true_from_Hgrid(x, y)  # noiseless ground truth if available\n",
    "\n",
    "# ========= Config =========\n",
    "domain_size        = 1000      # Âµm\n",
    "grid_size          = 5         # 5Ã—5 indents per block\n",
    "spacing            = 5         # Âµm between indents\n",
    "t_setup            = 20        # s setup per block\n",
    "t_move             = 20        # s travel per block\n",
    "n_initial          = 5         # start with TWO blocks\n",
    "n_steps            = 100        # one block per iteration\n",
    "train_noise        = 1.4       # measurement noise\n",
    "res_full           = 100       # visualization + global-error grid resolution\n",
    "inducing_cap       = 500       # viGP inducing cap\n",
    "train_iters        = 200       # viGP training iters\n",
    "\n",
    "# Acquisition options\n",
    "acq_mode           = \"UE\"      # \"UE\" or \"UCB\"\n",
    "beta_ucb           = 1.0       # UCB exploration weight (kappa)\n",
    "normalize_by_cost  = True      # divide acquisition by per-block cost\n",
    "\n",
    "# ---- Small utilities ----\n",
    "def to_numpy(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "def maybe_has_true():\n",
    "    return 'true_from_Hgrid' in globals() and callable(globals()['true_from_Hgrid'])\n",
    "\n",
    "def get_true_vals(xs, ys):\n",
    "    \"\"\"Return ground-truth if available, else a low-noise proxy.\"\"\"\n",
    "    if maybe_has_true():\n",
    "        return np.array([true_from_Hgrid(int(x), int(y)) for x, y in zip(xs, ys)], dtype=float)\n",
    "    else:\n",
    "        try:\n",
    "            return np.array([measure_from_Hgrid(int(x), int(y), noise_std=0.0) for x, y in zip(xs, ys)], dtype=float)\n",
    "        except Exception:\n",
    "            reps = 3\n",
    "            vals = []\n",
    "            for x, y in zip(xs, ys):\n",
    "                samples = [measure_from_Hgrid(int(x), int(y), noise_std=max(1e-3, 0.1*train_noise)) for _ in range(reps)]\n",
    "                vals.append(np.mean(samples))\n",
    "            return np.array(vals, dtype=float)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    eps = 1e-9\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / np.maximum(eps, np.abs(y_true)))))\n",
    "\n",
    "# === Build 100Ã—100 possible block centers across a 1000Ã—1000 domain\n",
    "max_offset = (grid_size - 1) * spacing\n",
    "grid_centers_x = np.linspace(0, domain_size - max_offset, 100)\n",
    "grid_centers_y = np.linspace(0, domain_size - max_offset, 100)\n",
    "available_centers = [(x, y) for x in grid_centers_x for y in grid_centers_y]\n",
    "\n",
    "# === Precompute local offsets for each block\n",
    "ix, iy = np.meshgrid(np.arange(grid_size), np.arange(grid_size))\n",
    "local_offsets = np.stack([ix.ravel(), iy.ravel()], axis=1) * spacing\n",
    "\n",
    "# ========= Storage =========\n",
    "all_x, all_y, all_z = [], [], []          # measured (x,y,z)\n",
    "visited = []                               # visited block centers\n",
    "acq_history, cost_history = [], []\n",
    "total_cost = 0.0\n",
    "\n",
    "# ========= Global grid for visualization + GLOBAL ERROR =========\n",
    "xt = np.linspace(0, domain_size, res_full)\n",
    "yt = np.linspace(0, domain_size, res_full)\n",
    "XX, YY = np.meshgrid(xt, yt)\n",
    "global_pts  = np.column_stack([XX.ravel(), YY.ravel()])\n",
    "global_true = get_true_vals(global_pts[:,0], global_pts[:,1])\n",
    "\n",
    "global_mape_hist          = []\n",
    "max_hardness_found_hist   = []\n",
    "\n",
    "# ========= Initial random sampling (TWO blocks) =========\n",
    "# NOTE: np.random is seeded above; this choice is repeatable\n",
    "init_idxs = np.random.choice(len(available_centers), n_initial, replace=False)\n",
    "for idx in init_idxs:\n",
    "    gx, gy = available_centers[idx]\n",
    "    pos = local_offsets + np.array([gx, gy])\n",
    "    xs, ys = pos[:,0], pos[:,1]\n",
    "    zs = [measure_from_Hgrid(int(x), int(y), noise_std=train_noise) for x, y in zip(xs, ys)]\n",
    "    all_x.extend(xs); all_y.extend(ys); all_z.extend(zs)\n",
    "    move_cost = evaluate_sample_cost_remaining(xs, ys)[0]\n",
    "    total_cost += (t_setup + t_move + move_cost)\n",
    "    cost_history.append(total_cost)\n",
    "    visited.append((gx, gy))\n",
    "\n",
    "# Remove initial centers from pool\n",
    "available_centers = [c for i, c in enumerate(available_centers) if i not in init_idxs]\n",
    "\n",
    "# ========= Active learning loop (one block per iteration) =========\n",
    "for step in range(n_steps):\n",
    "    # --- Normalize data (XY to [0,1], Z to [0,1]) ---\n",
    "    X_raw  = np.column_stack([all_x, all_y])\n",
    "    X_norm = X_raw / domain_size\n",
    "    z_arr  = np.array(all_z, dtype=float)\n",
    "    z_min, z_max = z_arr.min(), z_arr.max()\n",
    "    z_rng = max(1e-12, (z_max - z_min))\n",
    "    z_norm = (z_arr - z_min) / z_rng\n",
    "\n",
    "    # --- Fit GP on normalized data ---\n",
    "    train_x = torch.tensor(X_norm, dtype=torch.float32)\n",
    "    train_y = torch.tensor(z_norm, dtype=torch.float32)\n",
    "    inducing = train_x[:min(inducing_cap, train_x.size(0))]\n",
    "    gp = viGP(inducing, mean_module=ConstantMean(), kernel_module=ScaleKernel(RBFKernel()))\n",
    "    gp.fit(train_x, train_y, training_iter=train_iters)\n",
    "\n",
    "    # --- Acquisition for each remaining center ---\n",
    "    acq_vals = []\n",
    "    for gx, gy in available_centers:\n",
    "        pos = local_offsets + np.array([gx, gy])\n",
    "        xs, ys = pos[:,0], pos[:,1]\n",
    "\n",
    "        # guard: if any of these points already measured, skip this block\n",
    "        if any((x, y) in zip(all_x, all_y) for x, y in zip(xs, ys)):\n",
    "            acq_vals.append(-np.inf)\n",
    "            continue\n",
    "\n",
    "        X_test = torch.tensor(pos / domain_size, dtype=torch.float32)\n",
    "        u      = to_numpy(UE(gp, X_test))              # assumed std on normalized output\n",
    "        u_mean = float(np.mean(u))\n",
    "\n",
    "        if acq_mode.upper() == \"UCB\":\n",
    "            # UCB on normalized scale: mu_norm + beta * sigma_norm\n",
    "            mu_norm = to_numpy(gp.predict(X_test))\n",
    "            ucb_val = float(np.mean(mu_norm + beta_ucb * u))\n",
    "            score = ucb_val\n",
    "        else:\n",
    "            # UE mode (default): uncertainty-only\n",
    "            score = u_mean\n",
    "\n",
    "        if normalize_by_cost:\n",
    "            move_cost = evaluate_sample_cost_remaining(xs, ys)[0]\n",
    "            block_cost = t_setup + t_move + move_cost\n",
    "            score = score / max(1e-9, block_cost)\n",
    "\n",
    "        acq_vals.append(score)\n",
    "\n",
    "    # --- Select next center ---\n",
    "    best_idx = int(np.argmax(acq_vals))\n",
    "    gx, gy = available_centers.pop(best_idx)\n",
    "    visited.append((gx, gy))\n",
    "    acq_history.append(acq_vals[best_idx])\n",
    "\n",
    "    # --- Measure selected block ---\n",
    "    pos = local_offsets + np.array([gx, gy])\n",
    "    xs, ys = pos[:,0], pos[:,1]\n",
    "    zs = [measure_from_Hgrid(int(x), int(y), noise_std=train_noise) for x, y in zip(xs, ys)]\n",
    "    all_x.extend(xs); all_y.extend(ys); all_z.extend(zs)\n",
    "\n",
    "    # --- Update cost ---\n",
    "    move_cost = evaluate_sample_cost_remaining(xs, ys)[0]\n",
    "    total_cost += (t_setup + t_move + move_cost)\n",
    "    cost_history.append(total_cost)\n",
    "\n",
    "    # --- Predict on FULL GRID (for both visualization and GLOBAL ERROR) ---\n",
    "    test_x = torch.tensor(global_pts / domain_size, dtype=torch.float32)\n",
    "    pred_norm = to_numpy(gp.predict(test_x))\n",
    "    pred_grid = pred_norm * z_rng + z_min\n",
    "\n",
    "    # --- Global MAPE (percent) ---\n",
    "    global_mape = mape(global_true, pred_grid) * 100.0  # percent\n",
    "    global_mape_hist.append(global_mape)\n",
    "\n",
    "    # --- Track max hardness found so far (prefer truth at measured points if available) ---\n",
    "    try:\n",
    "        measured_true = get_true_vals(all_x, all_y)     # same length as all_z\n",
    "        current_max = float(np.max(measured_true))\n",
    "    except Exception:\n",
    "        current_max = float(np.max(all_z))\n",
    "    max_hardness_found_hist.append(current_max)\n",
    "    print(f\"[Iter {step+1}] Global MAPE: {global_mape:.2f}% | Max hardness found so far: {current_max:.4g}\")\n",
    "\n",
    "    # --- GP uncertainty over full grid (for map viz only) ---\n",
    "    ue_full = to_numpy(UE(gp, test_x))\n",
    "\n",
    "    # --- Build maps for visualization ---\n",
    "    Z_pred = pred_grid.reshape(res_full, res_full)\n",
    "    UE_map = ue_full.reshape(res_full, res_full)\n",
    "\n",
    "    # Acquisition/cost map (optional viz)\n",
    "    dx = global_pts[:, 0]; dy = global_pts[:, 1]\n",
    "    base_dx = movement_time(dx); base_dy = movement_time(dy)\n",
    "    penalty = np.where((dx >= 1000) | (dy >= 1000),\n",
    "                       t_engage + default_t_drift + default_t_lift, 0)\n",
    "    move_cost_map = (base_dx + base_dy + penalty)\n",
    "    ACQ_map = (ue_full / np.maximum(1e-9, move_cost_map)).reshape(res_full, res_full)\n",
    "\n",
    "    # ===================== Plots =====================\n",
    "    # ---- 2Ã—2 per-iteration maps ----\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    # (0,0) GP Pred Mean\n",
    "    im0 = axs[0,0].imshow(Z_pred, origin='lower',\n",
    "                          extent=(0, domain_size, 0, domain_size),\n",
    "                          vmin=3, vmax=7, cmap='plasma')\n",
    "    axs[0,0].set_title(f\"Iter {step+1}: GP Pred Mean\")\n",
    "    axs[0,0].scatter(all_x, all_y, c=np.arange(len(all_x)), cmap='jet', s=5)\n",
    "    fig.colorbar(im0, ax=axs[0,0], shrink=0.8)\n",
    "    # (0,1) GP UE\n",
    "    im1 = axs[0,1].imshow(UE_map, origin='lower',\n",
    "                          extent=(0, domain_size, 0, domain_size),\n",
    "                          cmap='plasma')\n",
    "    axs[0,1].set_title(\"GP Uncertainty (UE)\")\n",
    "    vx, vy = zip(*visited)\n",
    "    axs[0,1].scatter(vx, vy, c=np.arange(len(visited)), cmap='jet', s=50)\n",
    "    fig.colorbar(im1, ax=axs[0,1], shrink=0.8)\n",
    "    # (1,0) Acquisition / Cost (UE/cost for viz)\n",
    "    im2 = axs[1,0].imshow(ACQ_map, origin='lower',\n",
    "                          extent=(0, domain_size, 0, domain_size),\n",
    "                          cmap='plasma')\n",
    "    axs[1,0].set_title(\"UE / MoveCost\")\n",
    "    axs[1,0].scatter(vx, vy, c=np.arange(len(visited)), cmap='jet', s=50)\n",
    "    fig.colorbar(im2, ax=axs[1,0], shrink=0.8)\n",
    "    # (1,1) Cost progression\n",
    "    steps_arr = np.arange(1, len(cost_history)+1)\n",
    "    axs[1,1].plot(steps_arr, cost_history, marker='o')\n",
    "    axs[1,1].set_title(\"Cumulative Cost\")\n",
    "    axs[1,1].set_xlabel(\"Iteration\")\n",
    "    axs[1,1].set_ylabel(\"Cost (s)\")\n",
    "    axs[1,1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save maps every 4 iterations\n",
    "    if ((step + 1) % 2) == 0:\n",
    "        fig.savefig(os.path.join(outdir, f\"iter_{step+1:03d}_maps.png\"),\n",
    "                    bbox_inches=\"tight\", pad_inches=0.02)\n",
    "    plt.show()\n",
    "\n",
    "    # ---- Learning curve: MAPE (%) + Max hardness (right axis) ----\n",
    "    fig2, ax2 = plt.subplots(figsize=(7.8, 4.8))\n",
    "    plt.subplots_adjust(right=0.78)   # leave room for legend\n",
    "\n",
    "    its = np.arange(1, len(global_mape_hist)+1)\n",
    "\n",
    "    # Left axis: black line\n",
    "    l1 = ax2.plot(\n",
    "        its, global_mape_hist,\n",
    "        marker='o',\n",
    "        label='Global MAPE (%)',\n",
    "        color='black'\n",
    "    )[0]\n",
    "    ax2.set_xlabel(\"Iteration\")\n",
    "    ax2.set_ylabel(\"Global MAPE (%)\", color='black')\n",
    "    ax2.tick_params(axis='y', labelcolor='black')\n",
    "    ax2.set_title(\"Model Learning: MAPE vs Iteration (and Max Hardness)\")\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Right axis: blue line and blue label\n",
    "    ax2r = ax2.twinx()\n",
    "    l2 = ax2r.plot(\n",
    "        its, max_hardness_found_hist,\n",
    "        marker='^',\n",
    "        linestyle='--',\n",
    "        label='Max Hardness Found',\n",
    "        color='blue'\n",
    "    )[0]\n",
    "    ax2r.set_ylabel(\"Max Hardness Found\", color='blue')\n",
    "    ax2r.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # legend outside\n",
    "    lines  = [l1, l2]\n",
    "    labels = [ln.get_label() for ln in lines]\n",
    "    ax2.legend(\n",
    "        lines, labels,\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(1.20, 0.5),\n",
    "        borderaxespad=0.0, frameon=False, handlelength=2.0\n",
    "    )\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # Save learning curve every 4 iterations\n",
    "    if ((step + 1) % 2) == 0:\n",
    "        fig2.savefig(os.path.join(outdir, f\"iter_{step+1:03d}_mape_maxH.png\"),\n",
    "                     bbox_inches=\"tight\", pad_inches=0.02)\n",
    "    plt.show()\n",
    "\n",
    "# ======= Final plots (two color variants for the learning curve) =======\n",
    "its = np.arange(1, len(global_mape_hist)+1)\n",
    "\n",
    "# Variant A: MAPE in orange\n",
    "figA, axA = plt.subplots(figsize=(7.8, 4.8))\n",
    "plt.subplots_adjust(right=0.78)\n",
    "axA.plot(its, global_mape_hist, marker='o', label='Global MAPE (%)', color='tab:orange')\n",
    "axA.set_xlabel(\"Iteration\")\n",
    "axA.set_ylabel(\"Global MAPE (%)\")\n",
    "axA.set_title(\"Model Learning: MAPE vs Iteration (and Max Hardness)\")\n",
    "axA.grid(True, linestyle='--', alpha=0.6)\n",
    "axAr = axA.twinx()\n",
    "axAr.plot(its, max_hardness_found_hist, marker='^', linestyle='--',\n",
    "          label='Max Hardness Found', color='dimgray')\n",
    "axAr.set_ylabel(\"Max Hardness Found\")\n",
    "lines = [axA.lines[0], axAr.lines[0]]\n",
    "labels = ['Global MAPE (%)', 'Max Hardness Found']\n",
    "axA.legend(lines, labels, loc='center left', bbox_to_anchor=(1.20, 0.5),\n",
    "           borderaxespad=0.0, frameon=False, handlelength=2.0)\n",
    "figA.savefig(os.path.join(outdir, \"final_learning_curve_orange.png\"),\n",
    "             bbox_inches=\"tight\", pad_inches=0.02)\n",
    "plt.show()\n",
    "\n",
    "# Variant B: MAPE in blue\n",
    "figB, axB = plt.subplots(figsize=(7.8, 4.8))\n",
    "plt.subplots_adjust(right=0.78)\n",
    "axB.plot(its, global_mape_hist, marker='o', label='Global MAPE (%)', color='tab:blue')\n",
    "axB.set_xlabel(\"Iteration\")\n",
    "axB.set_ylabel(\"Global MAPE (%)\")\n",
    "axB.set_title(\"Model Learning: MAPE vs Iteration (and Max Hardness)\")\n",
    "axB.grid(True, linestyle='--', alpha=0.6)\n",
    "axBr = axB.twinx()\n",
    "axBr.plot(its, max_hardness_found_hist, marker='^', linestyle='--',\n",
    "          label='Max Hardness Found', color='dimgray')\n",
    "axBr.set_ylabel(\"Max Hardness Found\")\n",
    "lines = [axB.lines[0], axBr.lines[0]]\n",
    "labels = ['Global MAPE (%)', 'Max Hardness Found']\n",
    "axB.legend(lines, labels, loc='center left', bbox_to_anchor=(1.20, 0.5),\n",
    "           borderaxespad=0.0, frameon=False, handlelength=2.0)\n",
    "figB.savefig(os.path.join(outdir, \"final_learning_curve_blue.png\"),\n",
    "             bbox_inches=\"tight\", pad_inches=0.02)\n",
    "plt.show()\n",
    "\n",
    "# ======= Final report =======\n",
    "if max_hardness_found_hist:\n",
    "    print(f\"\\nFinal: Max hardness found = {max_hardness_found_hist[-1]:.6g}\")\n",
    "if global_mape_hist:\n",
    "    print(f\"Final: Global MAPE = {global_mape_hist[-1]:.3f}%\")\n",
    "print(f\"Acquisition mode: {acq_mode} \"\n",
    "      f\"{'(cost-normalized)' if normalize_by_cost else '(no cost normalization)'} \"\n",
    "      f\"{'(beta={:.3g})'.format(beta_ucb) if acq_mode.upper()=='UCB' else ''}\")\n",
    "print(f\"Saved per-4-iteration figures and final variants in: {os.path.abspath(outdir)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee43400",
   "metadata": {},
   "source": [
    "# Refrence Grid based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep grids_per_dim = 2..10; measure -> fit GP -> compute MAPE, track cost, time, and MAX HARDNESS\n",
    "\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "# ========== Reproducibility ==========\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 11,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"lines.linewidth\": 2.0,\n",
    "    \"grid.alpha\": 0.6,\n",
    "})\n",
    "\n",
    "# === Assumes these are already defined/imported in your env ===\n",
    "# viGP\n",
    "# measure_from_Hgrid(x, y, noise_std)\n",
    "# evaluate_sample_cost_remaining(x_coords, y_coords)\n",
    "# movement_time, t_engage, default_t_drift, default_t_lift  (not used here)\n",
    "# Optional: true_from_Hgrid(x, y)  # noiseless ground truth if available\n",
    "\n",
    "# ========= Config (fixed across sweep) =========\n",
    "domain_size   = 1000      # Âµm\n",
    "grid_size     = 5         # 5Ã—5 indents per block\n",
    "spacing       = 5         # Âµm between indents\n",
    "t_setup       = 20        # s setup per block\n",
    "t_move        = 20        # s travel per block\n",
    "train_noise   = 1.4       # measurement noise injected\n",
    "inducing_cap  = 50000     # viGP inducing cap\n",
    "train_iters   = 200       # viGP training iters\n",
    "res_full      = 100       # resolution for full-domain prediction & MAPE\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def to_numpy(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "def maybe_has_true():\n",
    "    return 'true_from_Hgrid' in globals() and callable(globals()['true_from_Hgrid'])\n",
    "\n",
    "def get_true_vals(xs, ys):\n",
    "    \"\"\"Noiseless truth if available; else noise-free (or low-noise) proxy.\"\"\"\n",
    "    if maybe_has_true():\n",
    "        return np.array([true_from_Hgrid(int(x), int(y)) for x, y in zip(xs, ys)], dtype=float)\n",
    "    try:\n",
    "        return np.array([measure_from_Hgrid(int(x), int(y), noise_std=0.0) for x, y in zip(xs, ys)], dtype=float)\n",
    "    except Exception:\n",
    "        reps = 3\n",
    "        vals = []\n",
    "        for x, y in zip(xs, ys):\n",
    "            samples = [measure_from_Hgrid(int(x), int(y), noise_std=max(1e-3, 0.1*train_noise)) for _ in range(reps)]\n",
    "            vals.append(np.mean(samples))\n",
    "        return np.array(vals, dtype=float)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    eps = 1e-9\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / np.maximum(eps, np.abs(y_true)))))\n",
    "\n",
    "# --- Precompute indent layout for a single 5Ã—5 block (relative offsets) ---\n",
    "ix, iy = np.meshgrid(np.arange(grid_size), np.arange(grid_size))\n",
    "local_offsets = np.stack([ix.ravel(), iy.ravel()], axis=1) * spacing  # shape (25,2)\n",
    "\n",
    "# --- Full-domain grid for predictions & error ---\n",
    "xt = np.linspace(0, domain_size, res_full)\n",
    "yt = np.linspace(0, domain_size, res_full)\n",
    "XX, YY = np.meshgrid(xt, yt)\n",
    "full_pts   = np.column_stack([XX.ravel(), YY.ravel()])\n",
    "full_norm  = full_pts / domain_size\n",
    "full_x_t   = torch.tensor(full_norm, dtype=torch.float32)\n",
    "true_full  = get_true_vals(full_pts[:,0], full_pts[:,1])\n",
    "\n",
    "# ---------- Sweep 2..10 grids per dimension ----------\n",
    "results = []  # list of dicts per configuration\n",
    "\n",
    "for gpd in range(2, 11):  # 2,3,...,10\n",
    "    tic = time.time()\n",
    "\n",
    "    # Grid centers (exclude outer edges; evenly spaced interior)\n",
    "    grid_centers_x = np.linspace(0, domain_size, gpd + 2)[1:-1]\n",
    "    grid_centers_y = np.linspace(0, domain_size, gpd + 2)[1:-1]\n",
    "    grid_centers   = [(x, y) for x in grid_centers_x for y in grid_centers_y]\n",
    "    n_blocks       = len(grid_centers)            # gpd^2\n",
    "    n_indents      = n_blocks * (grid_size**2)    # total points measured\n",
    "\n",
    "    # Collect all measurements for this design\n",
    "    all_x, all_y, all_z = [], [], []\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for (gx, gy) in grid_centers:\n",
    "        pos = local_offsets + np.array([gx, gy])         # shape (25,2)\n",
    "        xs, ys = pos[:,0], pos[:,1]\n",
    "        zs = [measure_from_Hgrid(int(x), int(y), noise_std=train_noise) for x, y in zip(xs, ys)]\n",
    "\n",
    "        all_x.extend(xs); all_y.extend(ys); all_z.extend(zs)\n",
    "\n",
    "        # Cost for this block\n",
    "        move_cost  = evaluate_sample_cost_remaining(xs, ys)[0]\n",
    "        total_cost += (t_setup + t_move + move_cost)\n",
    "\n",
    "    # Track max hardness found among measured points (prefer truth if available)\n",
    "    try:\n",
    "        measured_true = get_true_vals(all_x, all_y)\n",
    "        max_hardness_found = float(np.max(measured_true))\n",
    "    except Exception:\n",
    "        max_hardness_found = float(np.max(all_z))\n",
    "\n",
    "    # Fit GP once on all accumulated data\n",
    "    X_raw  = np.column_stack([all_x, all_y])\n",
    "    X_norm = X_raw / domain_size\n",
    "    z_arr  = np.asarray(all_z, dtype=float)\n",
    "    z_min, z_max = z_arr.min(), z_arr.max()\n",
    "    z_rng = max(1e-12, (z_max - z_min))\n",
    "    z_norm = (z_arr - z_min) / z_rng\n",
    "\n",
    "    train_x = torch.tensor(X_norm, dtype=torch.float32)\n",
    "    train_y = torch.tensor(z_norm, dtype=torch.float32)\n",
    "    inducing = train_x[:min(inducing_cap, train_x.size(0))]\n",
    "\n",
    "    gp = viGP(inducing, mean_module=ConstantMean(), kernel_module=ScaleKernel(RBFKernel()))\n",
    "    gp.fit(train_x, train_y, training_iter=train_iters)\n",
    "\n",
    "    # Predict full domain and compute MAPE (%)\n",
    "    pred_norm = to_numpy(gp.predict(full_x_t))\n",
    "    pred_phys = pred_norm * z_rng + z_min\n",
    "    final_mape_pct = mape(true_full, pred_phys) * 100.0\n",
    "\n",
    "    wall_time_s = time.time() - tic\n",
    "\n",
    "    results.append({\n",
    "        \"grids_per_dim\": gpd,\n",
    "        \"n_blocks\": n_blocks,\n",
    "        \"n_indents\": n_indents,\n",
    "        \"cumulative_cost_s\": float(total_cost),\n",
    "        \"wall_time_s\": float(wall_time_s),\n",
    "        \"mape_percent\": float(final_mape_pct),\n",
    "        \"max_hardness_found\": max_hardness_found,\n",
    "    })\n",
    "\n",
    "    print(f\"[gpd={gpd:2d}] blocks={n_blocks:3d} indents={n_indents:5d}  \"\n",
    "          f\"cost={total_cost:9.1f}s  MAPE={final_mape_pct:6.2f}%  \"\n",
    "          f\"maxH={max_hardness_found:.4g}  time={wall_time_s:7.2f}s\")\n",
    "\n",
    "# ---------- Plot summary ----------\n",
    "# 1) MAPE (%) vs cumulative cost\n",
    "fig1, ax1 = plt.subplots(figsize=(7.6, 4.6))\n",
    "costs  = [r[\"cumulative_cost_s\"] for r in results]\n",
    "mapes  = [r[\"mape_percent\"]       for r in results]\n",
    "labels = [r[\"grids_per_dim\"]      for r in results]\n",
    "\n",
    "ax1.plot(costs, mapes, marker='o')\n",
    "for c, m, lab in zip(costs, mapes, labels):\n",
    "    ax1.annotate(str(lab), (c, m), textcoords=\"offset points\", xytext=(5,5), fontsize=9)\n",
    "ax1.set_xlabel(\"Cumulative Cost (s)\")\n",
    "ax1.set_ylabel(\"Global MAPE (%)\")\n",
    "ax1.set_title(\"Design Sweep: MAPE vs Cumulative Cost\")\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sweep_mape_vs_cost.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 2) MAPE (%) vs grids per dimension\n",
    "fig2, ax2 = plt.subplots(figsize=(7.0, 4.2))\n",
    "gpds = [r[\"grids_per_dim\"] for r in results]\n",
    "ax2.plot(gpds, mapes, marker='s', color='tab:blue')\n",
    "ax2.set_xlabel(\"Grids per Dimension\")\n",
    "ax2.set_ylabel(\"Global MAPE (%)\")\n",
    "ax2.set_title(\"Design Sweep: MAPE vs Grids per Dimension\")\n",
    "ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sweep_mape_vs_grids.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 3) Max hardness found vs grids per dimension\n",
    "fig3, ax3 = plt.subplots(figsize=(7.0, 4.2))\n",
    "maxHs = [r[\"max_hardness_found\"] for r in results]\n",
    "ax3.plot(gpds, maxHs, marker='^', color='tab:orange')\n",
    "ax3.set_xlabel(\"Grids per Dimension\")\n",
    "ax3.set_ylabel(\"Max Hardness Found\")\n",
    "ax3.set_title(\"Design Sweep: Max Hardness vs Grids per Dimension\")\n",
    "ax3.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sweep_maxH_vs_grids.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 4) (Optional) wall time vs grids per dimension\n",
    "fig4, ax4 = plt.subplots(figsize=(7.0, 4.2))\n",
    "times = [r[\"wall_time_s\"] for r in results]\n",
    "ax4.plot(gpds, times, marker='o', color='tab:green')\n",
    "ax4.set_xlabel(\"Grids per Dimension\")\n",
    "ax4.set_ylabel(\"Wall Time (s)\")\n",
    "ax4.set_title(\"Design Sweep: Wall Time vs Grids per Dimension\")\n",
    "ax4.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sweep_time_vs_grids.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Print a compact table at the end\n",
    "print(\"\\n=== Summary (gpd, blocks, indents, cost[s], MAPE[%], maxH, wall_time[s]) ===\")\n",
    "for r in results:\n",
    "    print(f\"{r['grids_per_dim']:2d}  {r['n_blocks']:4d}  {r['n_indents']:5d}  \"\n",
    "          f\"{r['cumulative_cost_s']:10.1f}  {r['mape_percent']:8.3f}  \"\n",
    "          f\"{r['max_hardness_found']:7.3f}  {r['wall_time_s']:9.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89754ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-panel plotting: Cost (s) on X, MAPE or Max Hardness on Y.\n",
    "# Curve 1: from df (line) where Cost_df = Grid# * 25 * (t_setup + t_move + move_cost_sec)\n",
    "# Curve 2: from sweep results (10-point scatter) using cumulative_cost_s; points annotated by grids_per_dim.\n",
    "#\n",
    "# If you already have real `df` and `results` in memory, this script will use them.\n",
    "# Otherwise, it synthesizes plausible demo data with the required schema.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------ Styling ------------------\n",
    "plt.rcParams.update({\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"axes.linewidth\": 0.9,\n",
    "    \"lines.linewidth\": 2.0,\n",
    "})\n",
    "\n",
    "# ------------------ Inputs ------------------\n",
    "# Per-indent/block timing terms (seconds)\n",
    "# t_setup = 20.0\n",
    "# t_move  = 20.0\n",
    "# # If you want to plug an empirical average motion cost per indent, set here:\n",
    "# move_cost_sec = 12.0  # <-- change to your estimate or computed average\n",
    "\n",
    "per_indent_cost = t_setup + t_move + 100\n",
    "\n",
    "# ------------------ Load or synthesize df ------------------\n",
    "if \"df\" not in globals():\n",
    "    # Build a demo df with the required columns\n",
    "    iters = np.arange(1, 101)\n",
    "    grid_nums = 5 + iters  # starts at 6\n",
    "    rng = np.random.default_rng(7)\n",
    "    mape = np.clip(np.linspace(12.0, 3.1, 100) + rng.normal(0, 0.35, 100), 2.5, None)\n",
    "    maxH = np.maximum.accumulate(np.where(iters < 2, 4.3, 7.0 + rng.normal(0, 0.02, 100)))\n",
    "    df = pd.DataFrame({\n",
    "        \"Grid #\": grid_nums,\n",
    "        \"Iteration\": iters,\n",
    "        \"MAPE (%)\": mape,\n",
    "        \"Max Hardness Found\": maxH\n",
    "    })\n",
    "else:\n",
    "    # Ensure columns are trimmed\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Compute Cost_df (s): Grid# * 25 indents per grid * per-indent cost\n",
    "df[\"Cost (s)\"] = df[\"Grid #\"].astype(float) * 25.0 * per_indent_cost + 400\n",
    "\n",
    "# ------------------ Load or synthesize sweep results ------------------\n",
    "if \"results\" not in globals():\n",
    "    # Make a 2..10 sweep demo\n",
    "    synth = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    for gpd in range(2, 11):\n",
    "        n_blocks = gpd * gpd\n",
    "        n_indents = n_blocks * 25\n",
    "        cumulative_cost_s = float(n_indents * per_indent_cost)\n",
    "        mape_percent = max(2.8, 14.0 - 0.9 * gpd + rng.normal(0, 0.25))\n",
    "        max_hardness_found = 7.0 + rng.normal(0, 0.015)\n",
    "        synth.append({\n",
    "            \"grids_per_dim\": gpd,\n",
    "            \"cumulative_cost_s\": cumulative_cost_s,\n",
    "            \"mape_percent\": mape_percent,\n",
    "            \"max_hardness_found\": max_hardness_found\n",
    "        })\n",
    "    results = synth\n",
    "\n",
    "sweep_df = pd.DataFrame(results).sort_values(\"grids_per_dim\")\n",
    "\n",
    "# ------------------ Helpers ------------------\n",
    "def _finish(ax, title, xlabel, ylabel):\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.tick_params(direction=\"out\", length=4, width=0.8)\n",
    "\n",
    "def _annotate(ax, xs, ys, labels, dx=6, dy=6):\n",
    "    for x, y, lab in zip(xs, ys, labels):\n",
    "        ax.annotate(str(lab), (x, y), textcoords=\"offset points\", xytext=(dx, dy))\n",
    "\n",
    "# ------------------ Figure A: Cost vs MAPE ------------------\n",
    "figA, axA = plt.subplots(figsize=(8.2, 4.8))\n",
    "\n",
    "# Curve 1: df (line)\n",
    "axA.plot(df[\"Cost (s)\"], df[\"MAPE (%)\"], marker=\"o\", label=\"GP mean (per-iteration)\",  color='blue')\n",
    "\n",
    "# Curve 2: sweep (scatter, 10 points), annotated by grid size\n",
    "axA.scatter(sweep_df[\"cumulative_cost_s\"], sweep_df[\"mape_percent\"], marker=\"s\", label=\"Grid sweep (2â€“10)\",  color='black')\n",
    "_annotate(axA, sweep_df[\"cumulative_cost_s\"], sweep_df[\"mape_percent\"], sweep_df[\"grids_per_dim\"])\n",
    "\n",
    "_finish(axA, \"MAPE vs Cost\", \"Cost (s)\", \"MAPE (%)\")\n",
    "axA.legend(frameon=False)\n",
    "figA.tight_layout()\n",
    "# figA.savefig(\"/mnt/data/cost_vs_mape.png\", bbox_inches=\"tight\")\n",
    "# figA.savefig(\"/mnt/data/cost_vs_mape.pdf\", bbox_inches=\"tight\")\n",
    "# figA.savefig(\"/mnt/data/cost_vs_mape.svg\", bbox_inches=\"tight\")\n",
    "# plt.close(figA)\n",
    "\n",
    "# ------------------ Figure B: Cost vs Max Hardness ------------------\n",
    "figB, axB = plt.subplots(figsize=(8.2, 4.8))\n",
    "\n",
    "# Curve 1: df (line)\n",
    "axB.plot(df[\"Cost (s)\"], df[\"Max Hardness Found\"], marker=\"o\", label=\"GP mean (per-iteration)\", color='blue')\n",
    "\n",
    "# Curve 2: sweep (scatter), annotated by grid size\n",
    "axB.scatter(sweep_df[\"cumulative_cost_s\"], sweep_df[\"max_hardness_found\"], marker=\"^\", label=\"Grid sweep (2â€“10)\", color='black')\n",
    "_annotate(axB, sweep_df[\"cumulative_cost_s\"], sweep_df[\"max_hardness_found\"], sweep_df[\"grids_per_dim\"])\n",
    "\n",
    "_finish(axB, \"Maximum Hardness vs Cost\", \"Cost (s)\", \"Max Hardness Found\")\n",
    "axB.legend(frameon=False)\n",
    "figB.tight_layout()\n",
    "# figB.savefig(\"/mnt/data/cost_vs_maxH.png\", bbox_inches=\"tight\")\n",
    "# figB.savefig(\"/mnt/data/cost_vs_maxH.pdf\", bbox_inches=\"tight\")\n",
    "# figB.savefig(\"/mnt/data/cost_vs_maxH.svg\", bbox_inches=\"tight\")\n",
    "# plt.close(figB)\n",
    "\n",
    "# Report outputs\n",
    "# sorted([p for p in os.listdir(\"/mnt/data\") if p.startswith((\"cost_vs_\")) and p.endswith((\".png\",\".pdf\",\".svg\"))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5670c",
   "metadata": {},
   "source": [
    "# Dynamic Drift Adaptive GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430196a1",
   "metadata": {},
   "source": [
    "Lets see what happens with dynamic drift measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e3bfc",
   "metadata": {},
   "source": [
    "## Quantitative Comparison: Dynamic vs. Max-Hold GP\n",
    "\n",
    "Weâ€™ll use the **Mean Absolute Percentage Error** (MAPE) against our full-domain ground-truth hardness map (`H_grid`) to quantify how well each strategy reconstructs the surface.\n",
    "\n",
    "1. **Load ground truth**  \n",
    "   ```python\n",
    "   # H_grid is a 1000Ã—1000 DataFrame of true hardness values\n",
    "   Z_true_full = H_grid.values  # shape (1000,1000)\n",
    "   \n",
    "   # downâ€sample / interpolate to our plot resolution (resÃ—res):\n",
    "   from scipy.ndimage import zoom\n",
    "   Z_true = zoom(Z_true_full, res/1000, order=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f366245",
   "metadata": {},
   "source": [
    "# Adaptive Drift-Hold & Active-Learning Nanoindentation\n",
    "\n",
    "We combine an **active-learning sampling** framework with a **dynamic drift-hold policy** to minimize idle time while preserving precision, even when occasional â€œdrift eventsâ€ temporarily bias depth measurements.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Cost Model\n",
    "\n",
    "Each block of measurements incurs three components:\n",
    "\n",
    "1. **Setup & Travel**  \n",
    "   A fixed time \\(T_{\\rm setup} + T_{\\rm move}\\).\n",
    "\n",
    "2. **Measurement + Drift-Hold**  \n",
    "   \\[\n",
    "     T_{\\rm meas} = T_{\\rm base} + H,\n",
    "   \\]\n",
    "   where \\(T_{\\rm base}\\) is the indentation time and \\(H\\in\\{0,\\,5,\\,80\\}\\) s is the chosen hold.\n",
    "\n",
    "3. **Return Penalty**  \n",
    "   A small fixed â€œcome-backâ€ time \\(T_{\\rm comeback}\\).\n",
    "\n",
    "Total block cost:\n",
    "\\[\n",
    "  C = T_{\\rm setup} + T_{\\rm move} + (T_{\\rm base}+H) + T_{\\rm comeback}.\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Simulating a Drift Event\n",
    "\n",
    "- **Onset**: at \\(t_{\\rm event}\\) measurement depths acquire a one-sided bias that **decays exponentially**:\n",
    "  \\[\n",
    "    \\varepsilon(t) = \n",
    "    \\begin{cases}\n",
    "      A\\,e^{-\\,(t - t_{\\rm event})/\\tau}, & t_{\\rm event}\\le t\\le t_{\\rm end},\\\\\n",
    "      0, & \\text{otherwise},\n",
    "    \\end{cases}\n",
    "  \\]\n",
    "  with amplitude \\(A\\) and time constant \\(\\tau\\).\n",
    "\n",
    "- **Interpretation**: small systematic depth error that gradually vanishes.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Drift-Hold Decision Logic\n",
    "\n",
    "At each new block, let \\(T\\) be total elapsed time. We estimate:\n",
    "\n",
    "- **Simulated drift error**  \n",
    "  \\(\\mathrm{err}_5 = \\varepsilon(T)\\), representing what a short (5 s) hold would leave uncorrected.\n",
    "\n",
    "- **Intra-block variability**  \n",
    "  Sample a quick 5Ã—5 grid to compute its standard deviation \\(\\sigma\\). Define a â€œbigâ€ threshold\n",
    "  \\[\n",
    "    T_{\\rm big} = \\max\\!\\bigl(p\\%\\times\\overline{H},\\;2\\,\\sigma\\bigr),\n",
    "  \\]\n",
    "  where \\(\\overline{H}\\) is mean hardness and \\(p\\%\\) is a user-set tolerance.\n",
    "\n",
    "Then choose hold time \\(H\\):\n",
    "- If the previous hold was zero, we still force a short check every \\(k\\)th block.\n",
    "- Otherwise:\n",
    "  \\[\n",
    "    H = \n",
    "    \\begin{cases}\n",
    "      0, & \\mathrm{err}_5 \\le \\sigma,\\\\\n",
    "      5, & \\sigma < \\mathrm{err}_5 < T_{\\rm big},\\\\\n",
    "      80, & \\mathrm{err}_5 \\ge T_{\\rm big}.\n",
    "    \\end{cases}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Bias Correction\n",
    "\n",
    "When measuring with a non-full hold (\\(H=5\\) or 0 s) during an event window:\n",
    "\n",
    "1. Generate raw depth \\(\\tilde z\\) with base noise.\n",
    "2. Draw a one-sided bias \\(\\delta\\in[0,\\varepsilon(T)]\\).\n",
    "3. Subtract it:\n",
    "   \\[\n",
    "     z_{\\rm corrected} = \\tilde z - \\delta.\n",
    "   \\]\n",
    "\n",
    "For full holds (\\(H=80\\) s) we assume negligible residual bias.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Active-Learning Sampling\n",
    "\n",
    "- Maintain two datasets:\n",
    "  1. **Dynamic-hold** measurements.  \n",
    "  2. **Max-hold** (80 s) measurements as a high-fidelity reference.\n",
    "\n",
    "- **Gaussian Process (GP)** regression is trained separately on each.\n",
    "\n",
    "- **Acquisition function** for each candidate block:\n",
    "  \\[\n",
    "    \\alpha = \\frac{\\text{GP uncertainty}}{\\text{block cost}}.\n",
    "  \\]\n",
    "  The next block maximizes \\(\\alpha\\), balancing information gain vs. time.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Evaluation & Visualization\n",
    "\n",
    "Periodically we compare:\n",
    "\n",
    "- **Predicted mean & uncertainty** maps from both GPs.  \n",
    "- **Measured series** vs. â€œground truthâ€ (max-hold branch).  \n",
    "- **Cumulative cost** curves to quantify time savings.  \n",
    "- **Difference map** \n",
    "  \\(\\displaystyle 100\\%\\times\\frac{Z_{\\max} - Z_{\\rm dyn}}{Z_{\\max}}\\)  \n",
    "  and **MAPE** against a down-sampled truth field.\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaway\n",
    "\n",
    "By **adapting drift-hold** based on measured error vs. variability, this strategy:\n",
    "\n",
    "- **Saves** idle hold time when drift is small.  \n",
    "- **Escalates** to longer holds when drift threatens precision.  \n",
    "- **Integrates** seamlessly with active-learning to guide spatial sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d71099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Constants ===\n",
    "t_lift = 30\n",
    "t_engage = 65\n",
    "default_t_drift = 300\n",
    "default_t_measurement = 120\n",
    "default_t_lift = 25\n",
    "t_comeback=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Scenario A: small/short drift event ----------\n",
    "A = dict(\n",
    "    name=\"Short drift\",\n",
    "    t_event=7000.0,\n",
    "    t_event_end=45000.0,\n",
    "    decay_tau=1000.0,\n",
    "    bias_amp5=0.3,   # std at event start for 5 s hold\n",
    ")\n",
    "\n",
    "# ---------- Scenario B: long drift event ----------\n",
    "B = dict(\n",
    "    name=\"Long drift\",\n",
    "    t_event=7000.0,\n",
    "    t_event_end=145000.0,\n",
    "    decay_tau=30000.0,\n",
    "    bias_amp5=1,   # keep same amplitude; change tau + window\n",
    "    # if you want the 5.5*exp(...) flavor, set bias_amp5=5.5 here\n",
    ")\n",
    "\n",
    "def err5_vs_time(t, params):\n",
    "    \"\"\"err_5(t) = bias_amp5 * exp(-(t - t_event) / decay_tau) inside the window, else 0.\"\"\"\n",
    "    t0, t1 = params[\"t_event\"], params[\"t_event_end\"]\n",
    "    tau, amp = params[\"decay_tau\"], params[\"bias_amp5\"]\n",
    "    out = np.zeros_like(t, dtype=float)\n",
    "    mask = (t >= t0) & (t <= t1)\n",
    "    out[mask] = amp * np.exp(-(t[mask] - t0) / tau)\n",
    "    return out, mask\n",
    "\n",
    "# ---- Build a common time axis that covers both events ----\n",
    "t_min = 0.0\n",
    "t_max = max(A[\"t_event_end\"], B[\"t_event_end\"]) + 10000.0\n",
    "t = np.linspace(t_min, t_max, 5000)\n",
    "\n",
    "# ---- Compute curves ----\n",
    "errA, maskA = err5_vs_time(t, A)\n",
    "errB, maskB = err5_vs_time(t, B)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, ax = plt.subplots(figsize=(8.5, 5.0))\n",
    "\n",
    "ax.plot(t, errA, label=f\"{A['name']} ( amp={A['bias_amp5']})\", linewidth=2)\n",
    "ax.plot(t, errB, label=f\"{B['name']} ( amp={B['bias_amp5']})\", linewidth=2, linestyle=\"--\")\n",
    "\n",
    "# Shade each window (different alphas)\n",
    "ax.axvspan(A[\"t_event\"], A[\"t_event_end\"], alpha=0.12, label=f\"{A['name']} window\")\n",
    "ax.axvspan(B[\"t_event\"], B[\"t_event_end\"], alpha=0.08, label=f\"{B['name']} window\")\n",
    "\n",
    "# Boundaries\n",
    "ax.axvline(A[\"t_event\"], color=\"k\", linestyle=\":\", linewidth=1)\n",
    "ax.axvline(A[\"t_event_end\"], color=\"k\", linestyle=\":\", linewidth=1)\n",
    "ax.axvline(B[\"t_event\"], color=\"k\", linestyle=\":\", linewidth=1)\n",
    "ax.axvline(B[\"t_event_end\"], color=\"k\", linestyle=\":\", linewidth=1)\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Error std for 5 s hold, err_5(t)\")\n",
    "ax.set_title(\"Drift-induced error (5 s hold) for short vs long events\")\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # Optional: save instead of (or in addition to) showing\n",
    "# plt.savefig(\"err5_vs_time_short_vs_long.png\", dpi=200, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1794a3",
   "metadata": {},
   "source": [
    "Low drift case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # NEW\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "import random \n",
    "from scipy.ndimage import zoom\n",
    "import pandas as pd  # NEW\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# === Output dir ===\n",
    "OUTDIR = \"outputs_dynamic_vs_max_good drift\"  # NEW\n",
    "os.makedirs(OUTDIR, exist_ok=True) # NEW\n",
    "\n",
    "def to_np(x):\n",
    "    \"\"\"Return a NumPy array regardless of whether x is torch.Tensor or np.ndarray/list.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "# === Configuration ===\n",
    "domain_size           = 1000\n",
    "default_grid_size     = 5\n",
    "spacing               = 5\n",
    "t_setup               = 20\n",
    "t_move                = 20\n",
    "default_t_drift       = 300\n",
    "default_t_measurement = 40   # base measurement time\n",
    "t_comeback            = 35\n",
    "n_initial             = 5\n",
    "n_steps               = 100\n",
    "\n",
    "# Dynamic measurement hold times\n",
    "min_hold      = 0\n",
    "med_hold      = 5\n",
    "max_hold      = 80\n",
    "eval_interval = 5\n",
    "plot_interval = 5\n",
    "user_threshold_pct = 3.0  # % of mean hardness\n",
    "\n",
    "# Driftâ€event parameters\n",
    "t_event    = 7000.0\n",
    "t_event_end= 45000.0\n",
    "decay_tau  = 1000.0\n",
    "bias_amp5  = 0.3   # oneâ€sided bias std for 5 s hold\n",
    "bias_amp0  = 0.5   # oneâ€sided bias std for 0 s hold\n",
    "\n",
    "\n",
    "\n",
    "# H_grid: your pandas DataFrame of shape (1000, 1000)\n",
    "Z_full = H_grid.values                         # â†’ (1000, 1000)\n",
    "# res should match the resolution you use for your GP plots\n",
    "res = 20                                      # for example\n",
    "scale = res / Z_full.shape[0]                  # = 20 / 1000 = 0.02\n",
    "# use scipy.ndimage.zoom to interpolate/down-sample\n",
    "Z_true = zoom(Z_full, zoom=scale, order=1)     # â†’ (res, res)\n",
    "\n",
    "# Build grid centers\n",
    "max_offset = (default_grid_size - 1) * spacing\n",
    "grid_centers_x = np.linspace(0, domain_size - max_offset, 100)\n",
    "grid_centers_y = np.linspace(0, domain_size - max_offset, 100)\n",
    "available_centers = [(x, y) for x in grid_centers_x for y in grid_centers_y]\n",
    "\n",
    "# Precompute local offsets for a grid\n",
    "ix, iy = np.meshgrid(np.arange(default_grid_size),\n",
    "                     np.arange(default_grid_size))\n",
    "local_offsets = np.stack([ix.ravel(), iy.ravel()], axis=1) * spacing\n",
    "\n",
    "# Storage for dynamic vs maxâ€hold branches\n",
    "all_x_dyn = []; all_y_dyn = []; all_z_dyn = []\n",
    "all_x_max = []; all_y_max = []; all_z_max = []\n",
    "visited_dyn = []; visited_max = []\n",
    "cost_dyn = []; cost_max = []\n",
    "total_cost_dyn = 0.0\n",
    "total_cost_max = 0.0\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# === Metrics storage (for CSV + plots) ===  # NEW\n",
    "metrics = []  # rows: dict(iter, cost_dyn, cost_max, mape_dyn, mape_max)\n",
    "\n",
    "# === Helper: safe MAPE ===  # NEW\n",
    "def mape_percent(y_true, y_pred, eps=1e-8):\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "# === Prepare a fixed test grid for GP predictions ===  # NEW\n",
    "xt = np.linspace(0, domain_size, res)\n",
    "yt = np.linspace(0, domain_size, res)\n",
    "XX, YY = np.meshgrid(xt, yt)\n",
    "test_pts = np.column_stack([XX.ravel(), YY.ravel()])\n",
    "test_x_torch = torch.tensor(test_pts / domain_size, dtype=torch.float32)\n",
    "\n",
    "# === 1) Initial warmâ€up sampling (use max_hold for both) ===\n",
    "init_idxs = np.random.choice(len(available_centers),\n",
    "                             n_initial, replace=False)\n",
    "\n",
    "for idx in init_idxs:\n",
    "    gx, gy = available_centers[idx]\n",
    "    pos    = local_offsets + np.array([gx, gy])\n",
    "    xs, ys = pos[:,0], pos[:,1]\n",
    "\n",
    "    # simulate raw measurement\n",
    "    zs0 = [measure_from_Hgrid(int(x), int(y), noise_std=1.4)\n",
    "           for x,y in zip(xs, ys)]\n",
    "\n",
    "    # dynamic branch: no bias on first round\n",
    "    zs_dyn = np.array(zs0)\n",
    "    zs_max = np.array(zs0)\n",
    "    \n",
    "    # record\n",
    "    all_x_dyn.extend(xs); all_y_dyn.extend(ys); all_z_dyn.extend(zs_dyn)\n",
    "    all_x_max.extend(xs); all_y_max.extend(ys); all_z_max.extend(zs_max)\n",
    "    visited_dyn.append((gx,gy)); visited_max.append((gx,gy))\n",
    "\n",
    "    # both use max_hold initially\n",
    "    t_meas = default_t_measurement + max_hold\n",
    "    c = evaluate_sample_cost_remaining(xs, ys,\n",
    "                t_drift=default_t_drift,\n",
    "                t_measurement=t_meas,\n",
    "                t_comeback=t_comeback)[0]\n",
    "    print(f\"c is {c}\")\n",
    "\n",
    "    total_cost_dyn += t_setup + t_move + c\n",
    "    total_cost_max += t_setup + t_move + c\n",
    "    cost_dyn.append(total_cost_dyn)\n",
    "    cost_max.append(total_cost_max)\n",
    "\n",
    "# remove initial from availability\n",
    "available_centers = [c for i,c in enumerate(available_centers)\n",
    "                     if i not in init_idxs]\n",
    "\n",
    "prev_hold = max_hold\n",
    "\n",
    "# === 2) Activeâ€learning loop with dynamic holds & bias ===\n",
    "for step in range(n_steps):\n",
    "    iter_idx = step + 1  # 1-based\n",
    "    # print(f\"1:{step}\")\n",
    "\n",
    "    # --- Fit GP on dynamic data ---\n",
    "    X_dyn   = np.column_stack([all_x_dyn, all_y_dyn]) / domain_size\n",
    "    z_dyn   = np.array(all_z_dyn)\n",
    "    z_min_dyn, z_max_dyn = z_dyn.min(), z_dyn.max()\n",
    "    train_x = torch.tensor(X_dyn, dtype=torch.float32)\n",
    "    # guard if all equal\n",
    "    denom_dyn = (z_max_dyn - z_min_dyn) if (z_max_dyn > z_min_dyn) else 1.0\n",
    "    train_y = torch.tensor((z_dyn - z_min_dyn)/denom_dyn, dtype=torch.float32)\n",
    "    inducing = train_x[:min(500, train_x.size(0))]\n",
    "    gp_dyn = viGP(inducing,\n",
    "                  mean_module=ConstantMean(),\n",
    "                  kernel_module=ScaleKernel(RBFKernel()))\n",
    "    gp_dyn.fit(train_x, train_y, training_iter=200)\n",
    "\n",
    "    # --- Fit GP on maxâ€hold data ---\n",
    "    X_max   = np.column_stack([all_x_max, all_y_max]) / domain_size\n",
    "    z_max_a = np.array(all_z_max)\n",
    "    z_min_max, z_max_max = z_max_a.min(), z_max_a.max()\n",
    "    train_xm = torch.tensor(X_max, dtype=torch.float32)\n",
    "    denom_max = (z_max_max - z_min_max) if (z_max_max > z_min_max) else 1.0\n",
    "    train_ym = torch.tensor((z_max_a - z_min_max)/denom_max, dtype=torch.float32)\n",
    "    inducing_m = train_xm[:min(500, train_xm.size(0))]\n",
    "    gp_max = viGP(inducing_m,\n",
    "                  mean_module=ConstantMean(),\n",
    "                  kernel_module=ScaleKernel(RBFKernel()))\n",
    "    gp_max.fit(train_xm, train_ym, training_iter=200)\n",
    "    # print(f\"2:{step}\")\n",
    "\n",
    "    # --- Acquisition using prev_hold ---\n",
    "    t_meas_prev = default_t_measurement + prev_hold\n",
    "    acq_vals = []\n",
    "    for gx, gy in available_centers:\n",
    "        pos    = local_offsets + np.array([gx, gy])\n",
    "        X_test = torch.tensor(pos / domain_size, dtype=torch.float32)\n",
    "        unc    = UE(gp_dyn, X_test).mean().item()\n",
    "        c0     = evaluate_sample_cost_remaining(pos[:,0], pos[:,1],\n",
    "                          t_drift=default_t_drift,\n",
    "                          t_measurement=t_meas_prev,\n",
    "                          t_comeback=t_comeback)[0]\n",
    "        acq_vals.append(unc / (t_setup + t_move + c0))\n",
    "    best_idx = int(np.argmax(acq_vals))\n",
    "    gx, gy = available_centers.pop(best_idx)\n",
    "    visited_dyn.append((gx,gy))\n",
    "    visited_max.append((gx,gy))\n",
    "\n",
    "    # --- Determine hold for this block ---\n",
    "    curr_t = total_cost_dyn\n",
    "    in_event = (curr_t >= t_event) and (curr_t <= t_event_end)\n",
    "    err_5    = 0.5 * np.exp(-(curr_t - t_event)/decay_tau) if in_event else 0.0\n",
    "\n",
    "    if prev_hold == 0:\n",
    "        hold = med_hold if (iter_idx) % eval_interval == 0 else min_hold\n",
    "    else:\n",
    "        pos      = local_offsets + np.array([gx, gy])\n",
    "        hard_vals= np.array([measure_from_Hgrid(int(x), int(y),noise_std=1.4)\n",
    "                              for x,y in zip(pos[:,0], pos[:,1])])\n",
    "        T_var =  np.std(hard_vals)\n",
    "        T_var = -1  # your logic\n",
    "        T_big = max(user_threshold_pct*np.mean(hard_vals)/100,\n",
    "                    2*T_var)\n",
    "        if err_5 <= T_var:\n",
    "            hold = min_hold\n",
    "        elif err_5 < T_big:\n",
    "            hold = med_hold\n",
    "        else:\n",
    "            hold = max_hold\n",
    "\n",
    "    prev_hold = hold\n",
    "    # print(f\"3:{step}\")\n",
    "\n",
    "    # --- Final measurement with dynamic bias + max hold ---\n",
    "    pos    = local_offsets + np.array([gx, gy])\n",
    "    xs, ys = pos[:,0], pos[:,1]\n",
    "    zs0    = np.array([measure_from_Hgrid(int(x), int(y),\n",
    "                         noise_std=1.4)\n",
    "                       for x,y in zip(xs, ys)])\n",
    "\n",
    "    # compute event bias std\n",
    "    in_event = (curr_t >= t_event) and (curr_t <= t_event_end)\n",
    "    std5 = bias_amp5 * np.exp(-(curr_t - t_event)/decay_tau) if in_event else 0.0\n",
    "    std0 = bias_amp0 if in_event else 0.0\n",
    "\n",
    "    if hold == med_hold:\n",
    "        biases = rng.uniform(-std5, std5, size=zs0.shape)\n",
    "    elif hold == min_hold:\n",
    "        biases = rng.uniform(-std0, std0, size=zs0.shape)\n",
    "    else:\n",
    "        biases = np.zeros_like(zs0)\n",
    "    \n",
    "    zs_dyn_new = zs0 + biases\n",
    "    zs_max_new = zs0.copy()  # always no bias, Ïƒ=0.1\n",
    "\n",
    "    # append\n",
    "    all_x_dyn.extend(xs); all_y_dyn.extend(ys); all_z_dyn.extend(zs_dyn_new)\n",
    "    all_x_max.extend(xs); all_y_max.extend(ys); all_z_max.extend(zs_max_new)\n",
    "\n",
    "    # update dynamic cost\n",
    "    t_meas_u = default_t_measurement + hold\n",
    "    c_u      = evaluate_sample_cost_remaining(xs, ys,\n",
    "                  t_drift=default_t_drift,\n",
    "                  t_measurement=t_meas_u,\n",
    "                  t_comeback=t_comeback)[0]\n",
    "    total_cost_dyn += t_setup + t_move + c_u\n",
    "    cost_dyn.append(total_cost_dyn)\n",
    "\n",
    "    # update max cost\n",
    "    t_meas_m = default_t_measurement + max_hold\n",
    "    c_m      = evaluate_sample_cost_remaining(xs, ys,\n",
    "                  t_drift=default_t_drift,\n",
    "                  t_measurement=t_meas_m,\n",
    "                  t_comeback=t_comeback)[0]\n",
    "    total_cost_max += t_setup + t_move + c_m\n",
    "    cost_max.append(total_cost_max)\n",
    "\n",
    "    # --- ALWAYS compute GP predictions + MAPE this iteration (save to metrics) ---  # NEW\n",
    "    # --- ALWAYS compute GP predictions + MAPE this iteration (save to metrics) ---\n",
    "    with torch.no_grad():\n",
    "        Z_dyn_pred = to_np(gp_dyn.predict(test_x_torch)).reshape(res, res)\n",
    "        Z_dyn_pred = (Z_dyn_pred * denom_dyn + z_min_dyn)\n",
    "\n",
    "        Z_max_pred = to_np(gp_max.predict(test_x_torch)).reshape(res, res)\n",
    "        Z_max_pred = (Z_max_pred * denom_max + z_min_max)\n",
    "\n",
    "\n",
    "    mape_dyn = mape_percent(Z_true, Z_dyn_pred)\n",
    "    mape_max = mape_percent(Z_true, Z_max_pred)\n",
    "\n",
    "    metrics.append({\n",
    "        \"iteration\": iter_idx,\n",
    "        \"cost_dyn_s\": total_cost_dyn,\n",
    "        \"cost_max_s\": total_cost_max,\n",
    "        \"mape_dyn_%\": mape_dyn,\n",
    "        \"mape_max_%\": mape_max,\n",
    "        \"hold_s\": hold\n",
    "    })\n",
    "    print(hold)\n",
    "    print(total_cost_max)\n",
    "    print(total_cost_dyn)\n",
    "    # print(iter_idx % plot_interval)\n",
    "\n",
    "    # --- Plot & SAVE every plot_interval iters ---\n",
    "    if (iter_idx % plot_interval) == 0:\n",
    "        # 2Ã—2 plot\n",
    "        fig, axs = plt.subplots(2,2,figsize=(12,10))\n",
    "\n",
    "        im0 = axs[0,0].imshow(Z_dyn_pred, origin='lower',\n",
    "                            extent=(0,domain_size,0,domain_size),\n",
    "                            vmin=z_min_dyn, vmax=z_max_dyn, cmap='plasma')\n",
    "        axs[0,0].set_title(f\"Iter {iter_idx}: GP Pred (Dynamic)\")\n",
    "        axs[0,0].scatter(all_x_dyn, all_y_dyn, c=np.arange(len(all_x_dyn)),\n",
    "                        cmap='jet', s=5)\n",
    "        plt.colorbar(im0, ax=axs[0,0], shrink=0.8)\n",
    "\n",
    "        # Compute UE first, then plot (use to_np!)\n",
    "        UE_dyn_map = to_np(UE(gp_dyn, test_x_torch)).reshape(res, res)\n",
    "        im1 = axs[0,1].imshow(UE_dyn_map, origin='lower',\n",
    "                            extent=(0,domain_size,0,domain_size),\n",
    "                            cmap='plasma')\n",
    "        axs[0,1].set_title(\"GP Uncertainty (Dynamic)\")\n",
    "        if len(visited_dyn) > 0:\n",
    "            dx, dy = zip(*visited_dyn)\n",
    "            axs[0,1].scatter(dx, dy, c=np.arange(len(visited_dyn)),\n",
    "                            cmap='jet', s=50)\n",
    "        plt.colorbar(im1, ax=axs[0,1], shrink=0.8)\n",
    "\n",
    "        idx = np.arange(len(all_z_dyn))\n",
    "        z_gt  = np.array(all_z_max)  # using maxâ€hold as proxy GT\n",
    "        z_max_series = np.array(all_z_max)\n",
    "        z_dyn_series = np.array(all_z_dyn)\n",
    "\n",
    "        axs[1,0].plot(idx, z_gt,  'k-',  label='Ground Truth (max)')\n",
    "        axs[1,0].plot(idx, z_max_series, 'r--', label='Max Hold')\n",
    "        axs[1,0].plot(idx, z_dyn_series,'b-.', label='Dynamic Hold')\n",
    "        axs[1,0].set_title(\"Measured vs. GT\")\n",
    "        axs[1,0].set_xlabel(\"Measurement #\")\n",
    "        axs[1,0].set_ylabel(\"Hardness\")\n",
    "        axs[1,0].legend(loc='best')\n",
    "        axs[1,0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        steps_arr = np.arange(1, len(cost_dyn)+1)\n",
    "        axs[1,1].plot(steps_arr, cost_dyn, label='Dynamic Hold')\n",
    "        axs[1,1].plot(steps_arr, cost_max, '--', label='Always 80s Hold')\n",
    "        axs[1,1].set_title(f\"Cumulative Cost at Iter {iter_idx}\")\n",
    "        axs[1,1].set_xlabel(\"Iteration\")\n",
    "        axs[1,1].set_ylabel(\"Cost (s)\")\n",
    "        axs[1,1].legend(loc='best')\n",
    "        axs[1,1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}_dynamic_2x2.png\"),\n",
    "                    dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        # maxâ€hold GP alone\n",
    "        fig2, (ax0, ax1) = plt.subplots(1,2,figsize=(12,5))\n",
    "        im2 = ax0.imshow(Z_max_pred, origin='lower',\n",
    "                        extent=(0,domain_size,0,domain_size),\n",
    "                        vmin=z_min_max, vmax=z_max_max, cmap='plasma')\n",
    "        ax0.set_title(f\"Iter {iter_idx}: GP Pred (Max Hold)\")\n",
    "        ax0.scatter(all_x_max, all_y_max,\n",
    "                    c=np.arange(len(all_x_max)), cmap='jet', s=5)\n",
    "        plt.colorbar(im2, ax=ax0, shrink=0.8)\n",
    "\n",
    "        UE_max_map = to_np(UE(gp_max, test_x_torch)).reshape(res, res)\n",
    "        im3 = ax1.imshow(UE_max_map, origin='lower',\n",
    "                        extent=(0,domain_size,0,domain_size),\n",
    "                        cmap='plasma')\n",
    "        ax1.set_title(\"GP Uncertainty (Max Hold)\")\n",
    "        if len(visited_max) > 0:\n",
    "            vxm, vym = zip(*visited_max)\n",
    "            ax1.scatter(vxm, vym, c=np.arange(len(visited_max)),\n",
    "                        cmap='jet', s=50)\n",
    "        plt.colorbar(im3, ax=ax1, shrink=0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig2.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}_max_only.png\"),\n",
    "                    dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig2)\n",
    "\n",
    "        # Difference map (%)\n",
    "        diff_map = 100 * (Z_max_pred - Z_dyn_pred) / np.maximum(np.abs(Z_max_pred), 1e-8)\n",
    "        fig3, axd = plt.subplots(1,1,figsize=(6.5,5))\n",
    "        imd = axd.imshow(diff_map, origin='lower',\n",
    "                        extent=(0,domain_size,0,domain_size),\n",
    "                        cmap='plasma')\n",
    "        title = (f\"Iter {iter_idx}: Difference (%)\\n\"\n",
    "                f\"MAPE_dyn={mape_dyn:.2f}%, MAPE_max={mape_max:.2f}%\")\n",
    "        axd.set_title(title)\n",
    "        plt.colorbar(imd, ax=axd, label='% difference')\n",
    "        plt.tight_layout()\n",
    "        fig3.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}_diff_map.png\"),\n",
    "                    dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig3)\n",
    "\n",
    "\n",
    "# === AFTER LOOP: save metrics table and cost-vs-MAPE plot ===  # NEW\n",
    "df_metrics = pd.DataFrame(metrics, columns=[\"iteration\",\"cost_dyn_s\",\"cost_max_s\",\"mape_dyn_%\",\"mape_max_%\",\"hold_s\"])\n",
    "csv_path = os.path.join(OUTDIR, \"metrics_mape_cost_per_iter.csv\")\n",
    "df_metrics.to_csv(csv_path, index=False)\n",
    "\n",
    "# Cost vs MAPE figure\n",
    "figc, axc = plt.subplots(figsize=(7.5,5.2))\n",
    "axc.plot(df_metrics[\"cost_dyn_s\"], df_metrics[\"mape_dyn_%\"], marker='o', label=\"Dynamic hold\")\n",
    "axc.plot(df_metrics[\"cost_max_s\"], df_metrics[\"mape_max_%\"], marker='^', linestyle='--', label=\"Always 80s hold\")\n",
    "axc.set_xlabel(\"Cumulative cost (s)\")\n",
    "axc.set_ylabel(\"MAPE (%) vs Z_true\")\n",
    "axc.set_title(\"Cost vs MAPE (evaluated each iteration)\")\n",
    "axc.grid(True, linestyle='--', alpha=0.6)\n",
    "axc.legend()\n",
    "plt.tight_layout()\n",
    "figc.savefig(os.path.join(OUTDIR, \"cost_vs_mape.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close(figc)\n",
    "\n",
    "print(f\"[Saved] Table: {csv_path}\")\n",
    "print(f\"[Saved] Cost-vs-MAPE plot: {os.path.join(OUTDIR, 'cost_vs_mape.png')}\")\n",
    "print(f\"[Saved] Iteration figures every {plot_interval} iters in {OUTDIR}/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d661541",
   "metadata": {},
   "source": [
    "High drift case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # NEW\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "import random \n",
    "from scipy.ndimage import zoom\n",
    "import pandas as pd  # NEW\n",
    "\n",
    "# === Placeholder imports (implement these) ===\n",
    "# from your_module import viGP, UE, measure_from_Hgrid, evaluate_sample_cost_remaining\n",
    "# from your_module import movement_time, t_engage, default_t_lift\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# === Output dir ===\n",
    "OUTDIR = \"outputs_dynamic_vs_max\"  # NEW\n",
    "os.makedirs(OUTDIR, exist_ok=True) # NEW\n",
    "\n",
    "def to_np(x):\n",
    "    \"\"\"Return a NumPy array regardless of whether x is torch.Tensor or np.ndarray/list.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "# === Configuration ===\n",
    "domain_size           = 1000\n",
    "default_grid_size     = 5\n",
    "spacing               = 5\n",
    "t_setup               = 20\n",
    "t_move                = 20\n",
    "default_t_drift       = 300\n",
    "default_t_measurement = 40   # base measurement time\n",
    "t_comeback            = 35\n",
    "n_initial             = 5\n",
    "n_steps               = 100\n",
    "\n",
    "# Dynamic measurement hold times\n",
    "min_hold      = 0\n",
    "med_hold      = 5\n",
    "max_hold      = 80\n",
    "eval_interval = 5\n",
    "plot_interval = 5\n",
    "user_threshold_pct = 3.0  # % of mean hardness\n",
    "\n",
    "# Driftâ€event parameters\n",
    "t_event    = 7000.0\n",
    "t_event_end= 145000.0\n",
    "decay_tau  = 30000.0\n",
    "bias_amp5  = 1   # oneâ€sided bias std for 5 s hold\n",
    "bias_amp0  = 0.5   # oneâ€sided bias std for 0 s hold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# H_grid: your pandas DataFrame of shape (1000, 1000)\n",
    "Z_full = H_grid.values                         # â†’ (1000, 1000)\n",
    "# res should match the resolution you use for your GP plots\n",
    "res = 20                                      # for example\n",
    "scale = res / Z_full.shape[0]                  # = 20 / 1000 = 0.02\n",
    "# use scipy.ndimage.zoom to interpolate/down-sample\n",
    "Z_true = zoom(Z_full, zoom=scale, order=1)     # â†’ (res, res)\n",
    "\n",
    "# Build grid centers\n",
    "max_offset = (default_grid_size - 1) * spacing\n",
    "grid_centers_x = np.linspace(0, domain_size - max_offset, 100)\n",
    "grid_centers_y = np.linspace(0, domain_size - max_offset, 100)\n",
    "available_centers = [(x, y) for x in grid_centers_x for y in grid_centers_y]\n",
    "\n",
    "# Precompute local offsets for a grid\n",
    "ix, iy = np.meshgrid(np.arange(default_grid_size),\n",
    "                     np.arange(default_grid_size))\n",
    "local_offsets = np.stack([ix.ravel(), iy.ravel()], axis=1) * spacing\n",
    "\n",
    "# Storage for dynamic vs maxâ€hold branches\n",
    "all_x_dyn = []; all_y_dyn = []; all_z_dyn = []\n",
    "all_x_max = []; all_y_max = []; all_z_max = []\n",
    "visited_dyn = []; visited_max = []\n",
    "cost_dyn = []; cost_max = []\n",
    "total_cost_dyn = 0.0\n",
    "total_cost_max = 0.0\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# === Metrics storage (for CSV + plots) ===  # NEW\n",
    "metrics = []  # rows: dict(iter, cost_dyn, cost_max, mape_dyn, mape_max)\n",
    "\n",
    "# === Helper: safe MAPE ===  # NEW\n",
    "def mape_percent(y_true, y_pred, eps=1e-8):\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "# === Prepare a fixed test grid for GP predictions ===  # NEW\n",
    "xt = np.linspace(0, domain_size, res)\n",
    "yt = np.linspace(0, domain_size, res)\n",
    "XX, YY = np.meshgrid(xt, yt)\n",
    "test_pts = np.column_stack([XX.ravel(), YY.ravel()])\n",
    "test_x_torch = torch.tensor(test_pts / domain_size, dtype=torch.float32)\n",
    "\n",
    "# === 1) Initial warmâ€up sampling (use max_hold for both) ===\n",
    "init_idxs = np.random.choice(len(available_centers),\n",
    "                             n_initial, replace=False)\n",
    "\n",
    "for idx in init_idxs:\n",
    "    gx, gy = available_centers[idx]\n",
    "    pos    = local_offsets + np.array([gx, gy])\n",
    "    xs, ys = pos[:,0], pos[:,1]\n",
    "\n",
    "    # simulate raw measurement\n",
    "    zs0 = [measure_from_Hgrid(int(x), int(y), noise_std=1.4)\n",
    "           for x,y in zip(xs, ys)]\n",
    "\n",
    "    # dynamic branch: no bias on first round\n",
    "    zs_dyn = np.array(zs0)\n",
    "    zs_max = np.array(zs0)\n",
    "    \n",
    "    # record\n",
    "    all_x_dyn.extend(xs); all_y_dyn.extend(ys); all_z_dyn.extend(zs_dyn)\n",
    "    all_x_max.extend(xs); all_y_max.extend(ys); all_z_max.extend(zs_max)\n",
    "    visited_dyn.append((gx,gy)); visited_max.append((gx,gy))\n",
    "\n",
    "    # both use max_hold initially\n",
    "    t_meas = default_t_measurement + max_hold\n",
    "    c = evaluate_sample_cost_remaining(xs, ys,\n",
    "                t_drift=default_t_drift,\n",
    "                t_measurement=t_meas,\n",
    "                t_comeback=t_comeback)[0]\n",
    "    print(f\"c is {c}\")\n",
    "\n",
    "    total_cost_dyn += t_setup + t_move + c\n",
    "    total_cost_max += t_setup + t_move + c\n",
    "    cost_dyn.append(total_cost_dyn)\n",
    "    cost_max.append(total_cost_max)\n",
    "\n",
    "# remove initial from availability\n",
    "available_centers = [c for i,c in enumerate(available_centers)\n",
    "                     if i not in init_idxs]\n",
    "\n",
    "prev_hold = max_hold\n",
    "\n",
    "# === 2) Activeâ€learning loop with dynamic holds & bias ===\n",
    "for step in range(n_steps):\n",
    "    iter_idx = step + 1  # 1-based\n",
    "    # print(f\"1:{step}\")\n",
    "\n",
    "    # --- Fit GP on dynamic data ---\n",
    "    X_dyn   = np.column_stack([all_x_dyn, all_y_dyn]) / domain_size\n",
    "    z_dyn   = np.array(all_z_dyn)\n",
    "    z_min_dyn, z_max_dyn = z_dyn.min(), z_dyn.max()\n",
    "    train_x = torch.tensor(X_dyn, dtype=torch.float32)\n",
    "    # guard if all equal\n",
    "    denom_dyn = (z_max_dyn - z_min_dyn) if (z_max_dyn > z_min_dyn) else 1.0\n",
    "    train_y = torch.tensor((z_dyn - z_min_dyn)/denom_dyn, dtype=torch.float32)\n",
    "    inducing = train_x[:min(500, train_x.size(0))]\n",
    "    gp_dyn = viGP(inducing,\n",
    "                  mean_module=ConstantMean(),\n",
    "                  kernel_module=ScaleKernel(RBFKernel()))\n",
    "    gp_dyn.fit(train_x, train_y, training_iter=200)\n",
    "\n",
    "    # --- Fit GP on maxâ€hold data ---\n",
    "    X_max   = np.column_stack([all_x_max, all_y_max]) / domain_size\n",
    "    z_max_a = np.array(all_z_max)\n",
    "    z_min_max, z_max_max = z_max_a.min(), z_max_a.max()\n",
    "    train_xm = torch.tensor(X_max, dtype=torch.float32)\n",
    "    denom_max = (z_max_max - z_min_max) if (z_max_max > z_min_max) else 1.0\n",
    "    train_ym = torch.tensor((z_max_a - z_min_max)/denom_max, dtype=torch.float32)\n",
    "    inducing_m = train_xm[:min(500, train_xm.size(0))]\n",
    "    gp_max = viGP(inducing_m,\n",
    "                  mean_module=ConstantMean(),\n",
    "                  kernel_module=ScaleKernel(RBFKernel()))\n",
    "    gp_max.fit(train_xm, train_ym, training_iter=200)\n",
    "    # print(f\"2:{step}\")\n",
    "\n",
    "    # --- Acquisition using prev_hold ---\n",
    "    t_meas_prev = default_t_measurement + prev_hold\n",
    "    acq_vals = []\n",
    "    for gx, gy in available_centers:\n",
    "        pos    = local_offsets + np.array([gx, gy])\n",
    "        X_test = torch.tensor(pos / domain_size, dtype=torch.float32)\n",
    "        unc    = UE(gp_dyn, X_test).mean().item()\n",
    "        c0     = evaluate_sample_cost_remaining(pos[:,0], pos[:,1],\n",
    "                          t_drift=default_t_drift,\n",
    "                          t_measurement=t_meas_prev,\n",
    "                          t_comeback=t_comeback)[0]\n",
    "        acq_vals.append(unc / (t_setup + t_move + c0))\n",
    "    best_idx = int(np.argmax(acq_vals))\n",
    "    gx, gy = available_centers.pop(best_idx)\n",
    "    visited_dyn.append((gx,gy))\n",
    "    visited_max.append((gx,gy))\n",
    "\n",
    "    # --- Determine hold for this block ---\n",
    "    curr_t = total_cost_dyn\n",
    "    in_event = (curr_t >= t_event) and (curr_t <= t_event_end)\n",
    "    err_5    = 5.5 * np.exp(-(curr_t - t_event)/decay_tau) if in_event else 0.0\n",
    "\n",
    "    if prev_hold == 0:\n",
    "        hold = med_hold if (iter_idx) % eval_interval == 0 else min_hold\n",
    "    else:\n",
    "        pos      = local_offsets + np.array([gx, gy])\n",
    "        hard_vals= np.array([measure_from_Hgrid(int(x), int(y),noise_std=1.4)\n",
    "                              for x,y in zip(pos[:,0], pos[:,1])])\n",
    "        T_var =  np.std(hard_vals)\n",
    "        T_var = -1  # your logic\n",
    "        T_big = max(user_threshold_pct*np.mean(hard_vals)/100,\n",
    "                    2*T_var)\n",
    "        if err_5 <= T_var:\n",
    "            hold = min_hold\n",
    "        elif err_5 < T_big:\n",
    "            hold = med_hold\n",
    "        else:\n",
    "            hold = max_hold\n",
    "\n",
    "    prev_hold = hold\n",
    "    # print(f\"3:{step}\")\n",
    "\n",
    "    # --- Final measurement with dynamic bias + max hold ---\n",
    "    pos    = local_offsets + np.array([gx, gy])\n",
    "    xs, ys = pos[:,0], pos[:,1]\n",
    "    zs0    = np.array([measure_from_Hgrid(int(x), int(y),\n",
    "                         noise_std=1.4)\n",
    "                       for x,y in zip(xs, ys)])\n",
    "\n",
    "    # compute event bias std\n",
    "    in_event = (curr_t >= t_event) and (curr_t <= t_event_end)\n",
    "    std5 = bias_amp5 * np.exp(-(curr_t - t_event)/decay_tau) if in_event else 0.0\n",
    "    std0 = bias_amp0 if in_event else 0.0\n",
    "\n",
    "    if hold == med_hold:\n",
    "        biases = rng.uniform(-std5, std5, size=zs0.shape)\n",
    "    elif hold == min_hold:\n",
    "        biases = rng.uniform(-std0, std0, size=zs0.shape)\n",
    "    else:\n",
    "        biases = np.zeros_like(zs0)\n",
    "    \n",
    "    zs_dyn_new = zs0 + biases\n",
    "    zs_max_new = zs0.copy()  # always no bias, Ïƒ=0.1\n",
    "\n",
    "    # append\n",
    "    all_x_dyn.extend(xs); all_y_dyn.extend(ys); all_z_dyn.extend(zs_dyn_new)\n",
    "    all_x_max.extend(xs); all_y_max.extend(ys); all_z_max.extend(zs_max_new)\n",
    "\n",
    "    # update dynamic cost\n",
    "    t_meas_u = default_t_measurement + hold\n",
    "    c_u      = evaluate_sample_cost_remaining(xs, ys,\n",
    "                  t_drift=default_t_drift,\n",
    "                  t_measurement=t_meas_u,\n",
    "                  t_comeback=t_comeback)[0]\n",
    "    total_cost_dyn += t_setup + t_move + c_u\n",
    "    cost_dyn.append(total_cost_dyn)\n",
    "\n",
    "    # update max cost\n",
    "    t_meas_m = default_t_measurement + max_hold\n",
    "    c_m      = evaluate_sample_cost_remaining(xs, ys,\n",
    "                  t_drift=default_t_drift,\n",
    "                  t_measurement=t_meas_m,\n",
    "                  t_comeback=t_comeback)[0]\n",
    "    total_cost_max += t_setup + t_move + c_m\n",
    "    cost_max.append(total_cost_max)\n",
    "\n",
    "    # --- ALWAYS compute GP predictions + MAPE this iteration (save to metrics) ---  # NEW\n",
    "    # --- ALWAYS compute GP predictions + MAPE this iteration (save to metrics) ---\n",
    "    with torch.no_grad():\n",
    "        Z_dyn_pred = to_np(gp_dyn.predict(test_x_torch)).reshape(res, res)\n",
    "        Z_dyn_pred = (Z_dyn_pred * denom_dyn + z_min_dyn)\n",
    "\n",
    "        Z_max_pred = to_np(gp_max.predict(test_x_torch)).reshape(res, res)\n",
    "        Z_max_pred = (Z_max_pred * denom_max + z_min_max)\n",
    "\n",
    "\n",
    "    mape_dyn = mape_percent(Z_true, Z_dyn_pred)\n",
    "    mape_max = mape_percent(Z_true, Z_max_pred)\n",
    "\n",
    "    metrics.append({\n",
    "        \"iteration\": iter_idx,\n",
    "        \"cost_dyn_s\": total_cost_dyn,\n",
    "        \"cost_max_s\": total_cost_max,\n",
    "        \"mape_dyn_%\": mape_dyn,\n",
    "        \"mape_max_%\": mape_max,\n",
    "        \"hold_s\": hold\n",
    "    })\n",
    "    print(hold)\n",
    "    print(total_cost_max)\n",
    "    print(total_cost_dyn)\n",
    "    # print(iter_idx % plot_interval)\n",
    "\n",
    "    # --- Plot & SAVE every plot_interval iters ---\n",
    "    if (iter_idx % plot_interval) == 0:\n",
    "        # 2Ã—2 plot\n",
    "        fig, axs = plt.subplots(2,2,figsize=(12,10))\n",
    "\n",
    "        im0 = axs[0,0].imshow(Z_dyn_pred, origin='lower',\n",
    "                            extent=(0,domain_size,0,domain_size),\n",
    "                            vmin=z_min_dyn, vmax=z_max_dyn, cmap='plasma')\n",
    "        axs[0,0].set_title(f\"Iter {iter_idx}: GP Pred (Dynamic)\")\n",
    "        axs[0,0].scatter(all_x_dyn, all_y_dyn, c=np.arange(len(all_x_dyn)),\n",
    "                        cmap='jet', s=5)\n",
    "        plt.colorbar(im0, ax=axs[0,0], shrink=0.8)\n",
    "\n",
    "        # Compute UE first, then plot (use to_np!)\n",
    "        UE_dyn_map = to_np(UE(gp_dyn, test_x_torch)).reshape(res, res)\n",
    "        im1 = axs[0,1].imshow(UE_dyn_map, origin='lower',\n",
    "                            extent=(0,domain_size,0,domain_size),\n",
    "                            cmap='plasma')\n",
    "        axs[0,1].set_title(\"GP Uncertainty (Dynamic)\")\n",
    "        if len(visited_dyn) > 0:\n",
    "            dx, dy = zip(*visited_dyn)\n",
    "            axs[0,1].scatter(dx, dy, c=np.arange(len(visited_dyn)),\n",
    "                            cmap='jet', s=50)\n",
    "        plt.colorbar(im1, ax=axs[0,1], shrink=0.8)\n",
    "\n",
    "        idx = np.arange(len(all_z_dyn))\n",
    "        z_gt  = np.array(all_z_max)  # using maxâ€hold as proxy GT\n",
    "        z_max_series = np.array(all_z_max)\n",
    "        z_dyn_series = np.array(all_z_dyn)\n",
    "\n",
    "        axs[1,0].plot(idx, z_gt,  'k-',  label='Ground Truth (max)')\n",
    "        axs[1,0].plot(idx, z_max_series, 'r--', label='Max Hold')\n",
    "        axs[1,0].plot(idx, z_dyn_series,'b-.', label='Dynamic Hold')\n",
    "        axs[1,0].set_title(\"Measured vs. GT\")\n",
    "        axs[1,0].set_xlabel(\"Measurement #\")\n",
    "        axs[1,0].set_ylabel(\"Hardness\")\n",
    "        axs[1,0].legend(loc='best')\n",
    "        axs[1,0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        steps_arr = np.arange(1, len(cost_dyn)+1)\n",
    "        axs[1,1].plot(steps_arr, cost_dyn, label='Dynamic Hold')\n",
    "        axs[1,1].plot(steps_arr, cost_max, '--', label='Always 80s Hold')\n",
    "        axs[1,1].set_title(f\"Cumulative Cost at Iter {iter_idx}\")\n",
    "        axs[1,1].set_xlabel(\"Iteration\")\n",
    "        axs[1,1].set_ylabel(\"Cost (s)\")\n",
    "        axs[1,1].legend(loc='best')\n",
    "        axs[1,1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}_dynamic_2x2.png\"),\n",
    "                    dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        # maxâ€hold GP alone\n",
    "        fig2, (ax0, ax1) = plt.subplots(1,2,figsize=(12,5))\n",
    "        im2 = ax0.imshow(Z_max_pred, origin='lower',\n",
    "                        extent=(0,domain_size,0,domain_size),\n",
    "                        vmin=z_min_max, vmax=z_max_max, cmap='plasma')\n",
    "        ax0.set_title(f\"Iter {iter_idx}: GP Pred (Max Hold)\")\n",
    "        ax0.scatter(all_x_max, all_y_max,\n",
    "                    c=np.arange(len(all_x_max)), cmap='jet', s=5)\n",
    "        plt.colorbar(im2, ax=ax0, shrink=0.8)\n",
    "\n",
    "        UE_max_map = to_np(UE(gp_max, test_x_torch)).reshape(res, res)\n",
    "        im3 = ax1.imshow(UE_max_map, origin='lower',\n",
    "                        extent=(0,domain_size,0,domain_size),\n",
    "                        cmap='plasma')\n",
    "        ax1.set_title(\"GP Uncertainty (Max Hold)\")\n",
    "        if len(visited_max) > 0:\n",
    "            vxm, vym = zip(*visited_max)\n",
    "            ax1.scatter(vxm, vym, c=np.arange(len(visited_max)),\n",
    "                        cmap='jet', s=50)\n",
    "        plt.colorbar(im3, ax=ax1, shrink=0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig2.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}_max_only.png\"),\n",
    "                    dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig2)\n",
    "\n",
    "        # Difference map (%)\n",
    "        diff_map = 100 * (Z_max_pred - Z_dyn_pred) / np.maximum(np.abs(Z_max_pred), 1e-8)\n",
    "        fig3, axd = plt.subplots(1,1,figsize=(6.5,5))\n",
    "        imd = axd.imshow(diff_map, origin='lower',\n",
    "                        extent=(0,domain_size,0,domain_size),\n",
    "                        cmap='plasma')\n",
    "        title = (f\"Iter {iter_idx}: Difference (%)\\n\"\n",
    "                f\"MAPE_dyn={mape_dyn:.2f}%, MAPE_max={mape_max:.2f}%\")\n",
    "        axd.set_title(title)\n",
    "        plt.colorbar(imd, ax=axd, label='% difference')\n",
    "        plt.tight_layout()\n",
    "        fig3.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}_diff_map.png\"),\n",
    "                    dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig3)\n",
    "\n",
    "\n",
    "# === AFTER LOOP: save metrics table and cost-vs-MAPE plot ===  # NEW\n",
    "df_metrics = pd.DataFrame(metrics, columns=[\"iteration\",\"cost_dyn_s\",\"cost_max_s\",\"mape_dyn_%\",\"mape_max_%\",\"hold_s\"])\n",
    "csv_path = os.path.join(OUTDIR, \"metrics_mape_cost_per_iter.csv\")\n",
    "df_metrics.to_csv(csv_path, index=False)\n",
    "\n",
    "# Cost vs MAPE figure\n",
    "figc, axc = plt.subplots(figsize=(7.5,5.2))\n",
    "axc.plot(df_metrics[\"cost_dyn_s\"], df_metrics[\"mape_dyn_%\"], marker='o', label=\"Dynamic hold\")\n",
    "axc.plot(df_metrics[\"cost_max_s\"], df_metrics[\"mape_max_%\"], marker='^', linestyle='--', label=\"Always 80s hold\")\n",
    "axc.set_xlabel(\"Cumulative cost (s)\")\n",
    "axc.set_ylabel(\"MAPE (%) vs Z_true\")\n",
    "axc.set_title(\"Cost vs MAPE (evaluated each iteration)\")\n",
    "axc.grid(True, linestyle='--', alpha=0.6)\n",
    "axc.legend()\n",
    "plt.tight_layout()\n",
    "figc.savefig(os.path.join(OUTDIR, \"cost_vs_mape.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close(figc)\n",
    "\n",
    "print(f\"[Saved] Table: {csv_path}\")\n",
    "print(f\"[Saved] Cost-vs-MAPE plot: {os.path.join(OUTDIR, 'cost_vs_mape.png')}\")\n",
    "print(f\"[Saved] Iteration figures every {plot_interval} iters in {OUTDIR}/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49632dee",
   "metadata": {},
   "source": [
    "# Heteroskedastic GP fro noise awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75874f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# 1) Heteroscedastic GP definition\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_dist = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, inducing_points, variational_dist, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gpytorch.distributions.MultivariateNormal(\n",
    "            self.mean_module(x), self.covar_module(x)\n",
    "        )\n",
    "\n",
    "def heteroscedastic_elbo(mean_gp, noise_gp, train_x, train_y, num_samples=50):\n",
    "    q_f        = mean_gp(train_x)\n",
    "    q_log_noise= noise_gp(train_x)\n",
    "    kl_f       = mean_gp.variational_strategy.kl_divergence()\n",
    "    kl_noise   = noise_gp.variational_strategy.kl_divergence()\n",
    "    # Monte Carlo estimate of E_q[log p(y|f,Ïƒ)]\n",
    "    log_liks = []\n",
    "    for _ in range(num_samples):\n",
    "        f_samp      = q_f.rsample()\n",
    "        log_noise_s = q_log_noise.rsample()\n",
    "        noise_var   = torch.exp(log_noise_s) + 1e-6\n",
    "        dist        = Normal(f_samp, noise_var.sqrt())\n",
    "        log_liks.append(dist.log_prob(train_y).sum())\n",
    "    expected_ll = torch.stack(log_liks).mean()\n",
    "    elbo        = expected_ll - kl_f - kl_noise\n",
    "    return -elbo\n",
    "\n",
    "def train_hetero(train_x, train_y, mean_gp, noise_gp, lr=0.01, iters=300):\n",
    "    mean_gp.train(); noise_gp.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(mean_gp.parameters()) + list(noise_gp.parameters()), lr=lr\n",
    "    )\n",
    "    for i in range(iters):\n",
    "        optimizer.zero_grad()\n",
    "        loss = heteroscedastic_elbo(mean_gp, noise_gp, train_x, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def predict_hetero(mean_gp, noise_gp, test_x):\n",
    "    mean_gp.eval(); noise_gp.eval()\n",
    "    with torch.no_grad():\n",
    "        f_dist   = mean_gp(test_x)\n",
    "        n_dist   = noise_gp(test_x)\n",
    "        mean     = f_dist.mean.cpu().numpy()\n",
    "        noise    = torch.exp(n_dist.mean).cpu().numpy()\n",
    "        var      = f_dist.variance.cpu().numpy()\n",
    "    return mean, noise, var\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c98f0",
   "metadata": {},
   "source": [
    "# Noisy Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from  scipy.stats import binned_statistic_2d\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Build 1000 Ã— 1000 ternary-composition grid + ground-truth hardness\n",
    "# ------------------------------------------------------------------\n",
    "nx = ny = 1000\n",
    "x_coords, y_coords = np.meshgrid(np.linspace(0, 1, nx),\n",
    "                                 np.linspace(0, 1, ny))\n",
    "A = x_coords\n",
    "B = y_coords\n",
    "C = np.maximum(1.0 - A - B, 0.01)          # keep C â‰¥ 0\n",
    "total = A + B + C                          # renormalise so A+B+C = 1\n",
    "A /= total ;  B /= total ;  C /= total\n",
    "\n",
    "def hardness_model(a,b,c):\n",
    "    base         = 3*a + 5*b + 7*c                    # rule-of-mixtures\n",
    "    ss_strength  = -4*a*b - 3*b*c - 2*c*a             # solid-solution term\n",
    "    saturation   = -6*c**2 * (1 - c)                  # saturation penalty\n",
    "    return base + ss_strength + saturation\n",
    "\n",
    "H_grid = hardness_model(A,B,C)                       # ground-truth hardness\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Composition-dependent noise:  Ïƒ(A,B,C)\n",
    "# ------------------------------------------------------------------\n",
    "Ïƒ0, Î±A, Î±B, Î±C = 0.005, 0.1, 0.05, 0.0250\n",
    "noise_std_grid = Ïƒ0 + Î±A*A + Î±B*B + Î±C*C             # same shape as H_grid\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  One synthetic â€œmeasuredâ€ grid  (H + Gaussian noise)\n",
    "# ------------------------------------------------------------------\n",
    "rng = np.random.default_rng(123)\n",
    "H_meas = H_grid + rng.normal(0, noise_std_grid)      # element-wise Ïƒ\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Re-bin Ïƒ onto a regular (A,B) composition grid for plotting\n",
    "# ------------------------------------------------------------------\n",
    "nbins = 200                                          # resolution in Aâ€“B space\n",
    "Ïƒ_AB, A_edges, B_edges, _ = binned_statistic_2d(\n",
    "        A.ravel(), B.ravel(), noise_std_grid.ravel(),\n",
    "        statistic='mean', bins=nbins, range=[[0,1],[0,1]])\n",
    "Ïƒ_AB = np.flipud(Ïƒ_AB.T)                             # put origin bottom-left\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Plot:  2 Ã— 2   (all imshow, no 3-D)\n",
    "# ------------------------------------------------------------------\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 11))\n",
    "\n",
    "# (0,0) ground-truth hardness in xâ€“y space\n",
    "im0 = axs[0,0].imshow(H_grid, origin='lower', cmap='viridis',\n",
    "                      extent=(0, nx, 0, ny))\n",
    "axs[0,0].set_title(\"Ground-Truth Hardness   $H(x,y)$\")\n",
    "axs[0,0].set_xlabel(\"x-index\") ; axs[0,0].set_ylabel(\"y-index\")\n",
    "fig.colorbar(im0, ax=axs[0,0], shrink=0.8, label=\"GPa\")\n",
    "\n",
    "# (0,1) noise Ïƒ in xâ€“y space\n",
    "im1 = axs[0,1].imshow(noise_std_grid, origin='lower', cmap='inferno',\n",
    "                      extent=(0, nx, 0, ny))\n",
    "axs[0,1].set_title(\"Noise Std   $\\\\sigma(x,y)$\")\n",
    "axs[0,1].set_xlabel(\"x-index\") ; axs[0,1].set_ylabel(\"y-index\")\n",
    "fig.colorbar(im1, ax=axs[0,1], shrink=0.8, label=\"Ïƒ\")\n",
    "\n",
    "# (1,0) noise Ïƒ on regular composition grid (Fraction A vs Fraction B)\n",
    "im2 = axs[1,0].imshow(Ïƒ_AB, origin='lower', cmap='inferno',\n",
    "                      extent=(0,1,0,1), aspect='auto')\n",
    "axs[1,0].set_title(r\"Noise Std   $\\sigma(A,B)$\")\n",
    "axs[1,0].set_xlabel(\"Fraction A\") ; axs[1,0].set_ylabel(\"Fraction B\")\n",
    "fig.colorbar(im2, ax=axs[1,0], shrink=0.8, label=\"Ïƒ\")\n",
    "\n",
    "# (1,1) one synthetic measured grid  (H + noise)\n",
    "im3 = axs[1,1].imshow(H_meas, origin='lower', cmap='plasma',\n",
    "                      extent=(0, nx, 0, ny))\n",
    "axs[1,1].set_title(\"Measured Grid   $H_{meas}(x,y)$\")\n",
    "axs[1,1].set_xlabel(\"x-index\") ; axs[1,1].set_ylabel(\"y-index\")\n",
    "fig.colorbar(im3, ax=axs[1,1], shrink=0.8, label=\"GPa\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d45a2",
   "metadata": {},
   "source": [
    "# Adaptive Grid based on Target SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319dc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  ADAPTIVE-GRID  +  DYNAMIC-HOLD  +  HETEROSCEDASTIC-GP  SAMPLER  (SAVING)\n",
    "##############################################################################\n",
    "import os\n",
    "import numpy as np, torch, random, matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED = 42\n",
    "np.random.seed(SEED); torch.manual_seed(SEED); random.seed(SEED)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUTDIR = \"outputs_adaptive_grid_dynamic_hetero\"\n",
    "OUTDIR = \"outputs_adaptive_grid_dynamic_hetero\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ fixed parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "domain_size           = 1000          # Î¼m (for normalising GP inputs)\n",
    "t_setup, t_move       = 20, 20        # s\n",
    "default_t_drift       = 300           # s\n",
    "default_t_measurement = 40            # s\n",
    "t_comeback            = 35            # s\n",
    "\n",
    "# grid adaptation\n",
    "g_min, g_max          = 2, 5          # min / max square-grid dimension\n",
    "target_sem            = 0.05          # GPa desired SEM\n",
    "n_initial, n_steps    = 5, 100        # â† 100 iterations as requested\n",
    "plot_interval         = 10            # save plots every N blocks\n",
    "\n",
    "# dynamic hold thresholds\n",
    "min_hold, med_hold, max_hold = 0, 5, 80\n",
    "eval_interval         = 5             # force a 5-s check every N blocks\n",
    "user_threshold_pct    = 4.0           # % of Î¼ to define T_big\n",
    "\n",
    "# drift-event (simulated one-sided bias)\n",
    "t_event, t_event_end  = 7_000., 45_000.\n",
    "decay_tau             = 1_000.\n",
    "bias_amp5, bias_amp0  = 0.3, 0.5\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper: heteroscedastic noise & measurement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Ïƒ0, Î±A, Î±B, Î±C = 0.005, 0.1, 0.05, 0.0250\n",
    "ny, nx = H_grid.shape\n",
    "\n",
    "def noise_std_comp(xi:int, yi:int) -> float:\n",
    "    xi = int(np.clip(xi, 0, nx-1))\n",
    "    yi = int(np.clip(yi, 0, ny-1))\n",
    "    return Ïƒ0 + Î±A*A[yi,xi] + Î±B*B[yi,xi] + Î±C*C[yi,xi]\n",
    "\n",
    "def measure_from_Hgrid_hetero(xi:int, yi:int) -> float:\n",
    "    xi = int(np.clip(xi, 0, nx-1))\n",
    "    yi = int(np.clip(yi, 0, ny-1))\n",
    "    H  = H_grid[yi, xi]\n",
    "    Ïƒ  = noise_std_comp(xi, yi)\n",
    "    return H + np.random.normal(0, Ïƒ)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ reference ground-truth image (for diagnostics) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "res     = 25\n",
    "Z_true  = zoom(H_grid, zoom=res/ny, order=1)          # 25Ã—25 GT map\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ catalogue of possible  Block-centres  (5-Âµm lattice) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "spacing      = 5\n",
    "max_offset   = (g_max-1)*spacing\n",
    "centres      = [(x, y) for x in np.linspace(0, domain_size-max_offset, 100)\n",
    "                       for y in np.linspace(0, domain_size-max_offset, 100)]\n",
    "\n",
    "# 5Ã—5 offset template (slice [:g**2] for smaller grids)\n",
    "ix, iy        = np.meshgrid(np.arange(g_max), np.arange(g_max))\n",
    "offsets_5x5   = np.stack([ix.ravel(), iy.ravel()], axis=1)*spacing\n",
    "offsets_ref   = offsets_5x5.copy()                    # always 25 pts\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ storage arrays â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_x, all_y, all_z = [], [], []           # dynamic branch data\n",
    "cost_dyn, cost_max  = [], []\n",
    "tot_cost_dyn = tot_cost_max = 0.0\n",
    "visited = []\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ metrics table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "metrics = []  # rows per block: iteration, grid_size, n_pts, hold, costs, MAPE, MAE\n",
    "\n",
    "def mape_percent(y_true, y_pred, eps=1e-8):\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) initial 5Ã—5 blocks (80-s hold) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "init_idxs = np.random.choice(len(centres), n_initial, replace=False)\n",
    "for idx in init_idxs:\n",
    "    cx,cy   = centres[idx]\n",
    "    blk_xy  = offsets_5x5 + np.array([cx,cy])\n",
    "    xs,ys   = blk_xy[:,0], blk_xy[:,1]\n",
    "    zs      = [measure_from_Hgrid_hetero(x,y) for x,y in zip(xs,ys)]\n",
    "\n",
    "    all_x.extend(xs); all_y.extend(ys); all_z.extend(zs); visited.append((cx,cy))\n",
    "\n",
    "    t_meas  = default_t_measurement + max_hold\n",
    "    move_cost  = evaluate_sample_cost_remaining(xs,ys,\n",
    "                   t_drift=default_t_drift,\n",
    "                   t_measurement=t_meas,\n",
    "                   t_comeback=t_comeback)[0]\n",
    "    add_c   = t_setup + t_move + move_cost\n",
    "    tot_cost_dyn += add_c; tot_cost_max += add_c\n",
    "    cost_dyn.append(tot_cost_dyn); cost_max.append(tot_cost_max)\n",
    "\n",
    "centres   = [c for i,c in enumerate(centres) if i not in init_idxs]\n",
    "prev_hold = max_hold\n",
    "g_curr    = g_max                                   # start 5Ã—5\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ optional: baseline â€œiter 0â€ diagnostics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def diagnose_and_save(iter_idx, gp_m, gp_s, suffix=\"\"):\n",
    "    grid = np.linspace(0,domain_size,res)\n",
    "    XX,YY = np.meshgrid(grid,grid)\n",
    "    test  = torch.tensor(np.column_stack([XX.ravel(),YY.ravel()])/domain_size,\n",
    "                         dtype=torch.float32)\n",
    "    ZÎ¼, ZÏƒ, _ = predict_hetero(gp_m, gp_s, test)\n",
    "    ZÎ¼ = ZÎ¼.reshape(res,res); ZÏƒ = ZÏƒ.reshape(res,res)\n",
    "    mape = mape_percent(Z_true, ZÎ¼); mae_v = mae(Z_true, ZÎ¼)\n",
    "\n",
    "    fig,axs = plt.subplots(2,2,figsize=(13,10))\n",
    "    im0 = axs[0,0].imshow(ZÎ¼,origin='lower',\n",
    "                          extent=(0,domain_size,0,domain_size),cmap='plasma')\n",
    "    axs[0,0].set_title(f'Iter {iter_idx}: GP mean');     fig.colorbar(im0,ax=axs[0,0])\n",
    "    im1 = axs[0,1].imshow(ZÏƒ,origin='lower',\n",
    "                          extent=(0,domain_size,0,domain_size),cmap='plasma')\n",
    "    axs[0,1].set_title('Predicted Ïƒ');                   fig.colorbar(im1,ax=axs[0,1])\n",
    "\n",
    "    axs[1,0].plot(np.arange(len(all_z)),all_z,'.',ms=2)\n",
    "    axs[1,0].set_title('Dynamic measurements'); axs[1,0].set_xlabel('indent #')\n",
    "    axs[1,0].set_ylabel('Hardness (GPa)')\n",
    "\n",
    "    axs[1,1].plot(cost_dyn,label='Dynamic')\n",
    "    axs[1,1].plot(cost_max,'--',label='Fixed 5Ã—5 / 80 s')\n",
    "    axs[1,1].set_title(f'Cumulative cost   MAPE={mape:.2f}% | MAE={mae_v:.4f}')\n",
    "    axs[1,1].set_xlabel('block'); axs[1,1].set_ylabel('time (s)')\n",
    "    axs[1,1].legend(); axs[1,1].grid(ls='--',alpha=.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}{suffix}_diagnostics.png\"),\n",
    "                dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    return mape, mae_v\n",
    "\n",
    "# Warm-up fit just to produce iter 0000 figure\n",
    "if len(all_x) > 0:\n",
    "    X0 = torch.tensor(np.column_stack([all_x,all_y])/domain_size, dtype=torch.float32)\n",
    "    y0 = torch.tensor(np.array(all_z), dtype=torch.float32)\n",
    "    M0 = min(300, X0.size(0))\n",
    "    inducing0 = X0[torch.randperm(X0.size(0))[:M0]]\n",
    "    gp0_m, gp0_s = GPModel(inducing0), GPModel(inducing0)\n",
    "    train_hetero(X0, y0, gp0_m, gp0_s, iters=200, lr=1e-2)\n",
    "    diagnose_and_save(0, gp0_m, gp0_s, suffix=\"_warmup\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) active-learning loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for step in range(n_steps):\n",
    "    iter_idx = step + 1\n",
    "\n",
    "    # â€• fit heteroscedastic GP on current dynamic data â€•\n",
    "    X = torch.tensor(np.column_stack([all_x,all_y])/domain_size, dtype=torch.float32)\n",
    "    y = torch.tensor(np.array(all_z), dtype=torch.float32)\n",
    "    M = min(300, X.size(0))\n",
    "    inducing = X[torch.randperm(X.size(0))[:M]]\n",
    "    gp_m, gp_s = GPModel(inducing), GPModel(inducing)\n",
    "    train_hetero(X, y, gp_m, gp_s, iters=300, lr=1e-2)\n",
    "\n",
    "    # â€• acquisition: UE / cost for every candidate centre â€•\n",
    "    acq_vals = []\n",
    "    for cx,cy in centres:\n",
    "        blk_xy  = offsets_5x5[:g_curr**2] + np.array([cx,cy])\n",
    "        Xtest   = torch.tensor(blk_xy/domain_size, dtype=torch.float32)\n",
    "        Î¼, Ïƒ, var  = predict_hetero(gp_m, gp_s, Xtest)\n",
    "        unc     = (Ïƒ+var).mean()\n",
    "\n",
    "        est_tmeas = default_t_measurement + prev_hold\n",
    "        est_cost  = evaluate_sample_cost_remaining(\n",
    "                        blk_xy[:,0], blk_xy[:,1],\n",
    "                        t_drift=default_t_drift,\n",
    "                        t_measurement=est_tmeas,\n",
    "                        t_comeback=t_comeback)[0]\n",
    "        acq_vals.append(unc / (t_setup+t_move+est_cost))\n",
    "\n",
    "    # pick best centre\n",
    "    best   = int(np.argmax(acq_vals))\n",
    "    cx,cy  = centres.pop(best)\n",
    "    visited.append((cx,cy))\n",
    "\n",
    "    # â€• block-specific Ïƒ â†’ grid size  (adaptive grid) â€•\n",
    "    blk_xy   = offsets_5x5 + np.array([cx,cy])          # 25 candidate pts\n",
    "    X_blk    = torch.tensor(blk_xy/domain_size, dtype=torch.float32)\n",
    "    _, Ïƒ_blk, _ = predict_hetero(gp_m, gp_s, X_blk)\n",
    "    Ïƒ_est    = float(Ïƒ_blk.mean())\n",
    "    n_req    = int(np.ceil((Ïƒ_est/target_sem)**2))\n",
    "    g_curr   = min(max(int(np.ceil(np.sqrt(n_req))), g_min), g_max)\n",
    "\n",
    "    # slice offsets for actual measurement\n",
    "    blk_xy   = offsets_5x5[:g_curr**2] + np.array([cx,cy])\n",
    "\n",
    "    # â€• drift-hold decision (one-sided) â€•\n",
    "    t_now   = tot_cost_dyn\n",
    "    in_evt  = t_event <= t_now <= t_event_end\n",
    "    err_5   = 0.5*np.exp(-(t_now - t_event)/decay_tau) if in_evt else 0.0\n",
    "\n",
    "    T_var   = 0.0                               # can swap to Ïƒ_est if desired\n",
    "    Î¼_blk   = gp_m(X_blk).mean.mean().item()    # model mean (assumes raw units)\n",
    "    T_big   = max(user_threshold_pct*Î¼_blk/100., 2*T_var)\n",
    "\n",
    "    if prev_hold==0:\n",
    "        hold = med_hold if (iter_idx)%eval_interval==0 else min_hold\n",
    "    else:\n",
    "        hold = min_hold if err_5<=T_var else med_hold if err_5<T_big else max_hold\n",
    "    prev_hold = hold\n",
    "\n",
    "    # â€• measurement & bias injection â€•\n",
    "    xs,ys   = blk_xy[:,0], blk_xy[:,1]\n",
    "    base_z  = np.array([measure_from_Hgrid_hetero(x,y) for x,y in zip(xs,ys)])\n",
    "    if in_evt:\n",
    "        Ïƒ5 = bias_amp5*np.exp(-(t_now-t_event)/decay_tau); Ïƒ0=bias_amp0\n",
    "    else:\n",
    "        Ïƒ5 = Ïƒ0 = 0.0\n",
    "    bias = np.random.uniform(0,Ïƒ5,base_z.shape) if hold==med_hold else \\\n",
    "           np.random.uniform(0,Ïƒ0,base_z.shape) if hold==min_hold else \\\n",
    "           np.zeros_like(base_z)\n",
    "    vals_dyn = base_z - bias\n",
    "\n",
    "    all_x.extend(xs); all_y.extend(ys); all_z.extend(vals_dyn)\n",
    "\n",
    "    # â€• cost bookkeeping (dynamic vs fixed 5Ã—5/80 s) â€•\n",
    "    c_dyn = evaluate_sample_cost_remaining(\n",
    "              xs,ys, t_drift=default_t_drift,\n",
    "              t_measurement=default_t_measurement+hold,\n",
    "              t_comeback=t_comeback)[0]\n",
    "\n",
    "    ref_xy = offsets_ref + np.array([cx,cy])\n",
    "    c_bas  = evaluate_sample_cost_remaining(\n",
    "              ref_xy[:,0], ref_xy[:,1],\n",
    "              t_drift=default_t_drift,\n",
    "              t_measurement=default_t_measurement+max_hold,\n",
    "              t_comeback=t_comeback)[0]\n",
    "\n",
    "    tot_cost_dyn += t_setup+t_move+c_dyn\n",
    "    tot_cost_max += t_setup+t_move+c_bas\n",
    "    cost_dyn.append(tot_cost_dyn); cost_max.append(tot_cost_max)\n",
    "\n",
    "    # â€• prediction & metrics every iteration â€•\n",
    "    grid = np.linspace(0,domain_size,res)\n",
    "    XX,YY = np.meshgrid(grid,grid)\n",
    "    test  = torch.tensor(np.column_stack([XX.ravel(),YY.ravel()])/domain_size,\n",
    "                         dtype=torch.float32)\n",
    "    ZÎ¼, ZÏƒ, _ = predict_hetero(gp_m, gp_s, test)\n",
    "    ZÎ¼ = ZÎ¼.reshape(res,res)\n",
    "    mape_it = mape_percent(Z_true, ZÎ¼); mae_it = mae(Z_true, ZÎ¼)\n",
    "\n",
    "    metrics.append({\n",
    "        \"iteration\": iter_idx,\n",
    "        \"grid_size\": g_curr,\n",
    "        \"n_points\": g_curr**2,\n",
    "        \"hold_s\": hold,\n",
    "        \"cost_dyn_s\": tot_cost_dyn,\n",
    "        \"cost_max_s\": tot_cost_max,\n",
    "        \"mape_%\": mape_it,\n",
    "        \"mae\": mae_it\n",
    "    })\n",
    "\n",
    "    # â€• visualisation every plot_interval â€•\n",
    "    if (iter_idx)%plot_interval==0:\n",
    "        fig,axs = plt.subplots(2,2,figsize=(13,10))\n",
    "\n",
    "        im0 = axs[0,0].imshow(ZÎ¼,origin='lower',\n",
    "                              extent=(0,domain_size,0,domain_size),cmap='plasma')\n",
    "        axs[0,0].set_title(f'Iter {iter_idx}: GP mean');     fig.colorbar(im0,ax=axs[0,0])\n",
    "\n",
    "        ZÏƒ_img = ZÏƒ.reshape(res,res)\n",
    "        im1 = axs[0,1].imshow(ZÏƒ_img,origin='lower',\n",
    "                              extent=(0,domain_size,0,domain_size),cmap='plasma')\n",
    "        axs[0,1].set_title('Predicted Ïƒ');                   fig.colorbar(im1,ax=axs[0,1])\n",
    "\n",
    "        axs[1,0].plot(np.arange(len(all_z)),all_z,'.',ms=2)\n",
    "        axs[1,0].set_title('Dynamic measurements'); axs[1,0].set_xlabel('indent #')\n",
    "        axs[1,0].set_ylabel('Hardness (GPa)')\n",
    "\n",
    "        axs[1,1].plot(cost_dyn,label='Dynamic')\n",
    "        axs[1,1].plot(cost_max,'--',label='Fixed 5Ã—5 / 80 s')\n",
    "        axs[1,1].set_title(f'Cumulative cost   MAPE={mape_it:.2f}% | MAE={mae_it:.4f}')\n",
    "        axs[1,1].set_xlabel('block'); axs[1,1].set_ylabel('time (s)')\n",
    "        axs[1,1].legend(); axs[1,1].grid(ls='--',alpha=.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}_diagnostics.png\"),\n",
    "                    dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ SAVE: metrics CSV & summary plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.DataFrame(metrics, columns=[\n",
    "    \"iteration\",\"grid_size\",\"n_points\",\"hold_s\",\n",
    "    \"cost_dyn_s\",\"cost_max_s\",\"mape_%\",\"mae\"\n",
    "])\n",
    "csv_path = os.path.join(OUTDIR, \"metrics_adaptive_grid_dynamic_hetero.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Cost vs MAPE (dynamic vs baseline)\n",
    "plt.figure(figsize=(7.8,5.0))\n",
    "plt.plot(df[\"cost_dyn_s\"], df[\"mape_%\"], marker='o', label=\"Dynamic (adaptive grid)\")\n",
    "plt.plot(df[\"cost_max_s\"], df[\"mape_%\"], marker='^', linestyle='--', label=\"Baseline cost ref (5Ã—5/80s)\")\n",
    "plt.xlabel(\"Cumulative cost (s)\")\n",
    "plt.ylabel(\"MAPE (%) vs Z_true\")\n",
    "plt.title(\"Cost vs MAPE\")\n",
    "plt.grid(True, ls='--', alpha=.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"cost_vs_mape.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Iteration vs MAPE\n",
    "plt.figure(figsize=(7.8,5.0))\n",
    "plt.plot(df[\"iteration\"], df[\"mape_%\"], marker='o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MAPE (%)\")\n",
    "plt.title(\"Iteration vs MAPE\")\n",
    "plt.grid(True, ls='--', alpha=.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"iteration_vs_mape.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Time vs Grid Size\n",
    "plt.figure(figsize=(7.8,5.0))\n",
    "plt.step(df[\"cost_dyn_s\"], df[\"grid_size\"], where='post')\n",
    "plt.xlabel(\"Cumulative cost (s)\")\n",
    "plt.ylabel(\"Grid size (g Ã— g)\")\n",
    "plt.title(\"Adaptive grid size vs time\")\n",
    "plt.grid(True, ls='--', alpha=.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"time_vs_grid_size.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Time vs Hold (0/5/80)\n",
    "plt.figure(figsize=(7.8,5.0))\n",
    "plt.step(df[\"cost_dyn_s\"], df[\"hold_s\"], where='post')\n",
    "plt.xlabel(\"Cumulative cost (s)\")\n",
    "plt.ylabel(\"Hold time used (s)\")\n",
    "plt.title(\"Dynamic hold vs time\")\n",
    "plt.grid(True, ls='--', alpha=.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"time_vs_hold.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"[Saved] CSV: {csv_path}\")\n",
    "print(f\"[Saved] Figures every {plot_interval} iters in {OUTDIR}/\")\n",
    "print(f\"[Saved] Summary: cost_vs_mape.png, iteration_vs_mape.png, time_vs_grid_size.png, time_vs_hold.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee83c3",
   "metadata": {},
   "source": [
    "# Target SEM =0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70baa23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  ADAPTIVE-GRID  +  DYNAMIC-HOLD  +  HETEROSCEDASTIC-GP  SAMPLER  (SAVING)\n",
    "##############################################################################\n",
    "import os\n",
    "import numpy as np, torch, random, matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED = 42\n",
    "np.random.seed(SEED); torch.manual_seed(SEED); random.seed(SEED)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUTDIR = \"outputs_adaptive_grid_dynamic_hetero_semp1\"\n",
    "OUTDIR = \"outputs_adaptive_grid_dynamic_hetero_semp1\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ fixed parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "domain_size           = 1000          # Î¼m (for normalising GP inputs)\n",
    "t_setup, t_move       = 20, 20        # s\n",
    "default_t_drift       = 300           # s\n",
    "default_t_measurement = 40            # s\n",
    "t_comeback            = 35            # s\n",
    "\n",
    "# grid adaptation\n",
    "g_min, g_max          = 2, 5          # min / max square-grid dimension\n",
    "target_sem            = 0.5          # GPa desired SEM\n",
    "n_initial, n_steps    = 5, 100        # â† 100 iterations as requested\n",
    "plot_interval         = 10            # save plots every N blocks\n",
    "\n",
    "# dynamic hold thresholds\n",
    "min_hold, med_hold, max_hold = 0, 5, 80\n",
    "eval_interval         = 5             # force a 5-s check every N blocks\n",
    "user_threshold_pct    = 4.0           # % of Î¼ to define T_big\n",
    "\n",
    "# drift-event (simulated one-sided bias)\n",
    "t_event, t_event_end  = 7_000., 45_000.\n",
    "decay_tau             = 1_000.\n",
    "bias_amp5, bias_amp0  = 0.3, 0.5\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper: heteroscedastic noise & measurement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Ïƒ0, Î±A, Î±B, Î±C = 0.005, 0.1, 0.05, 0.0250\n",
    "ny, nx = H_grid.shape\n",
    "\n",
    "def noise_std_comp(xi:int, yi:int) -> float:\n",
    "    xi = int(np.clip(xi, 0, nx-1))\n",
    "    yi = int(np.clip(yi, 0, ny-1))\n",
    "    return Ïƒ0 + Î±A*A[yi,xi] + Î±B*B[yi,xi] + Î±C*C[yi,xi]\n",
    "\n",
    "def measure_from_Hgrid_hetero(xi:int, yi:int) -> float:\n",
    "    xi = int(np.clip(xi, 0, nx-1))\n",
    "    yi = int(np.clip(yi, 0, ny-1))\n",
    "    H  = H_grid[yi, xi]\n",
    "    Ïƒ  = noise_std_comp(xi, yi)\n",
    "    return H + np.random.normal(0, Ïƒ)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ reference ground-truth image (for diagnostics) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "res     = 25\n",
    "Z_true  = zoom(H_grid, zoom=res/ny, order=1)          # 25Ã—25 GT map\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ catalogue of possible  Block-centres  (5-Âµm lattice) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "spacing      = 5\n",
    "max_offset   = (g_max-1)*spacing\n",
    "centres      = [(x, y) for x in np.linspace(0, domain_size-max_offset, 100)\n",
    "                       for y in np.linspace(0, domain_size-max_offset, 100)]\n",
    "\n",
    "# 5Ã—5 offset template (slice [:g**2] for smaller grids)\n",
    "ix, iy        = np.meshgrid(np.arange(g_max), np.arange(g_max))\n",
    "offsets_5x5   = np.stack([ix.ravel(), iy.ravel()], axis=1)*spacing\n",
    "offsets_ref   = offsets_5x5.copy()                    # always 25 pts\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ storage arrays â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_x, all_y, all_z = [], [], []           # dynamic branch data\n",
    "cost_dyn, cost_max  = [], []\n",
    "tot_cost_dyn = tot_cost_max = 0.0\n",
    "visited = []\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ metrics table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "metrics = []  # rows per block: iteration, grid_size, n_pts, hold, costs, MAPE, MAE\n",
    "\n",
    "def mape_percent(y_true, y_pred, eps=1e-8):\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) initial 5Ã—5 blocks (80-s hold) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "init_idxs = np.random.choice(len(centres), n_initial, replace=False)\n",
    "for idx in init_idxs:\n",
    "    cx,cy   = centres[idx]\n",
    "    blk_xy  = offsets_5x5 + np.array([cx,cy])\n",
    "    xs,ys   = blk_xy[:,0], blk_xy[:,1]\n",
    "    zs      = [measure_from_Hgrid_hetero(x,y) for x,y in zip(xs,ys)]\n",
    "\n",
    "    all_x.extend(xs); all_y.extend(ys); all_z.extend(zs); visited.append((cx,cy))\n",
    "\n",
    "    t_meas  = default_t_measurement + max_hold\n",
    "    move_cost  = evaluate_sample_cost_remaining(xs,ys,\n",
    "                   t_drift=default_t_drift,\n",
    "                   t_measurement=t_meas,\n",
    "                   t_comeback=t_comeback)[0]\n",
    "    add_c   = t_setup + t_move + move_cost\n",
    "    tot_cost_dyn += add_c; tot_cost_max += add_c\n",
    "    cost_dyn.append(tot_cost_dyn); cost_max.append(tot_cost_max)\n",
    "\n",
    "centres   = [c for i,c in enumerate(centres) if i not in init_idxs]\n",
    "prev_hold = max_hold\n",
    "g_curr    = g_max                                   # start 5Ã—5\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ optional: baseline â€œiter 0â€ diagnostics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def diagnose_and_save(iter_idx, gp_m, gp_s, suffix=\"\"):\n",
    "    grid = np.linspace(0,domain_size,res)\n",
    "    XX,YY = np.meshgrid(grid,grid)\n",
    "    test  = torch.tensor(np.column_stack([XX.ravel(),YY.ravel()])/domain_size,\n",
    "                         dtype=torch.float32)\n",
    "    ZÎ¼, ZÏƒ, _ = predict_hetero(gp_m, gp_s, test)\n",
    "    ZÎ¼ = ZÎ¼.reshape(res,res); ZÏƒ = ZÏƒ.reshape(res,res)\n",
    "    mape = mape_percent(Z_true, ZÎ¼); mae_v = mae(Z_true, ZÎ¼)\n",
    "\n",
    "    fig,axs = plt.subplots(2,2,figsize=(13,10))\n",
    "    im0 = axs[0,0].imshow(ZÎ¼,origin='lower',\n",
    "                          extent=(0,domain_size,0,domain_size),cmap='plasma')\n",
    "    axs[0,0].set_title(f'Iter {iter_idx}: GP mean');     fig.colorbar(im0,ax=axs[0,0])\n",
    "    im1 = axs[0,1].imshow(ZÏƒ,origin='lower',\n",
    "                          extent=(0,domain_size,0,domain_size),cmap='plasma')\n",
    "    axs[0,1].set_title('Predicted Ïƒ');                   fig.colorbar(im1,ax=axs[0,1])\n",
    "\n",
    "    axs[1,0].plot(np.arange(len(all_z)),all_z,'.',ms=2)\n",
    "    axs[1,0].set_title('Dynamic measurements'); axs[1,0].set_xlabel('indent #')\n",
    "    axs[1,0].set_ylabel('Hardness (GPa)')\n",
    "\n",
    "    axs[1,1].plot(cost_dyn,label='Dynamic')\n",
    "    axs[1,1].plot(cost_max,'--',label='Fixed 5Ã—5 / 80 s')\n",
    "    axs[1,1].set_title(f'Cumulative cost   MAPE={mape:.2f}% | MAE={mae_v:.4f}')\n",
    "    axs[1,1].set_xlabel('block'); axs[1,1].set_ylabel('time (s)')\n",
    "    axs[1,1].legend(); axs[1,1].grid(ls='--',alpha=.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}{suffix}_diagnostics.png\"),\n",
    "                dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    return mape, mae_v\n",
    "\n",
    "# Warm-up fit just to produce iter 0000 figure\n",
    "if len(all_x) > 0:\n",
    "    X0 = torch.tensor(np.column_stack([all_x,all_y])/domain_size, dtype=torch.float32)\n",
    "    y0 = torch.tensor(np.array(all_z), dtype=torch.float32)\n",
    "    M0 = min(300, X0.size(0))\n",
    "    inducing0 = X0[torch.randperm(X0.size(0))[:M0]]\n",
    "    gp0_m, gp0_s = GPModel(inducing0), GPModel(inducing0)\n",
    "    train_hetero(X0, y0, gp0_m, gp0_s, iters=200, lr=1e-2)\n",
    "    diagnose_and_save(0, gp0_m, gp0_s, suffix=\"_warmup\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) active-learning loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for step in range(n_steps):\n",
    "    iter_idx = step + 1\n",
    "\n",
    "    # â€• fit heteroscedastic GP on current dynamic data â€•\n",
    "    X = torch.tensor(np.column_stack([all_x,all_y])/domain_size, dtype=torch.float32)\n",
    "    y = torch.tensor(np.array(all_z), dtype=torch.float32)\n",
    "    M = min(300, X.size(0))\n",
    "    inducing = X[torch.randperm(X.size(0))[:M]]\n",
    "    gp_m, gp_s = GPModel(inducing), GPModel(inducing)\n",
    "    train_hetero(X, y, gp_m, gp_s, iters=300, lr=1e-2)\n",
    "\n",
    "    # â€• acquisition: UE / cost for every candidate centre â€•\n",
    "    acq_vals = []\n",
    "    for cx,cy in centres:\n",
    "        blk_xy  = offsets_5x5[:g_curr**2] + np.array([cx,cy])\n",
    "        Xtest   = torch.tensor(blk_xy/domain_size, dtype=torch.float32)\n",
    "        Î¼, Ïƒ, var  = predict_hetero(gp_m, gp_s, Xtest)\n",
    "        unc     = (Ïƒ+var).mean()\n",
    "\n",
    "        est_tmeas = default_t_measurement + prev_hold\n",
    "        est_cost  = evaluate_sample_cost_remaining(\n",
    "                        blk_xy[:,0], blk_xy[:,1],\n",
    "                        t_drift=default_t_drift,\n",
    "                        t_measurement=est_tmeas,\n",
    "                        t_comeback=t_comeback)[0]\n",
    "        acq_vals.append(unc / (t_setup+t_move+est_cost))\n",
    "\n",
    "    # pick best centre\n",
    "    best   = int(np.argmax(acq_vals))\n",
    "    cx,cy  = centres.pop(best)\n",
    "    visited.append((cx,cy))\n",
    "\n",
    "    # â€• block-specific Ïƒ â†’ grid size  (adaptive grid) â€•\n",
    "    blk_xy   = offsets_5x5 + np.array([cx,cy])          # 25 candidate pts\n",
    "    X_blk    = torch.tensor(blk_xy/domain_size, dtype=torch.float32)\n",
    "    _, Ïƒ_blk, _ = predict_hetero(gp_m, gp_s, X_blk)\n",
    "    Ïƒ_est    = float(Ïƒ_blk.mean())\n",
    "    n_req    = int(np.ceil((Ïƒ_est/target_sem)**2))\n",
    "    g_curr   = min(max(int(np.ceil(np.sqrt(n_req))), g_min), g_max)\n",
    "\n",
    "    # slice offsets for actual measurement\n",
    "    blk_xy   = offsets_5x5[:g_curr**2] + np.array([cx,cy])\n",
    "\n",
    "    # â€• drift-hold decision (one-sided) â€•\n",
    "    t_now   = tot_cost_dyn\n",
    "    in_evt  = t_event <= t_now <= t_event_end\n",
    "    err_5   = 0.5*np.exp(-(t_now - t_event)/decay_tau) if in_evt else 0.0\n",
    "\n",
    "    T_var   = 0.0                               # can swap to Ïƒ_est if desired\n",
    "    Î¼_blk   = gp_m(X_blk).mean.mean().item()    # model mean (assumes raw units)\n",
    "    T_big   = max(user_threshold_pct*Î¼_blk/100., 2*T_var)\n",
    "\n",
    "    if prev_hold==0:\n",
    "        hold = med_hold if (iter_idx)%eval_interval==0 else min_hold\n",
    "    else:\n",
    "        hold = min_hold if err_5<=T_var else med_hold if err_5<T_big else max_hold\n",
    "    prev_hold = hold\n",
    "\n",
    "    # â€• measurement & bias injection â€•\n",
    "    xs,ys   = blk_xy[:,0], blk_xy[:,1]\n",
    "    base_z  = np.array([measure_from_Hgrid_hetero(x,y) for x,y in zip(xs,ys)])\n",
    "    if in_evt:\n",
    "        Ïƒ5 = bias_amp5*np.exp(-(t_now-t_event)/decay_tau); Ïƒ0=bias_amp0\n",
    "    else:\n",
    "        Ïƒ5 = Ïƒ0 = 0.0\n",
    "    bias = np.random.uniform(0,Ïƒ5,base_z.shape) if hold==med_hold else \\\n",
    "           np.random.uniform(0,Ïƒ0,base_z.shape) if hold==min_hold else \\\n",
    "           np.zeros_like(base_z)\n",
    "    vals_dyn = base_z - bias\n",
    "\n",
    "    all_x.extend(xs); all_y.extend(ys); all_z.extend(vals_dyn)\n",
    "\n",
    "    # â€• cost bookkeeping (dynamic vs fixed 5Ã—5/80 s) â€•\n",
    "    c_dyn = evaluate_sample_cost_remaining(\n",
    "              xs,ys, t_drift=default_t_drift,\n",
    "              t_measurement=default_t_measurement+hold,\n",
    "              t_comeback=t_comeback)[0]\n",
    "\n",
    "    ref_xy = offsets_ref + np.array([cx,cy])\n",
    "    c_bas  = evaluate_sample_cost_remaining(\n",
    "              ref_xy[:,0], ref_xy[:,1],\n",
    "              t_drift=default_t_drift,\n",
    "              t_measurement=default_t_measurement+max_hold,\n",
    "              t_comeback=t_comeback)[0]\n",
    "\n",
    "    tot_cost_dyn += t_setup+t_move+c_dyn\n",
    "    tot_cost_max += t_setup+t_move+c_bas\n",
    "    cost_dyn.append(tot_cost_dyn); cost_max.append(tot_cost_max)\n",
    "\n",
    "    # â€• prediction & metrics every iteration â€•\n",
    "    grid = np.linspace(0,domain_size,res)\n",
    "    XX,YY = np.meshgrid(grid,grid)\n",
    "    test  = torch.tensor(np.column_stack([XX.ravel(),YY.ravel()])/domain_size,\n",
    "                         dtype=torch.float32)\n",
    "    ZÎ¼, ZÏƒ, _ = predict_hetero(gp_m, gp_s, test)\n",
    "    ZÎ¼ = ZÎ¼.reshape(res,res)\n",
    "    mape_it = mape_percent(Z_true, ZÎ¼); mae_it = mae(Z_true, ZÎ¼)\n",
    "\n",
    "    metrics.append({\n",
    "        \"iteration\": iter_idx,\n",
    "        \"grid_size\": g_curr,\n",
    "        \"n_points\": g_curr**2,\n",
    "        \"hold_s\": hold,\n",
    "        \"cost_dyn_s\": tot_cost_dyn,\n",
    "        \"cost_max_s\": tot_cost_max,\n",
    "        \"mape_%\": mape_it,\n",
    "        \"mae\": mae_it\n",
    "    })\n",
    "\n",
    "    # â€• visualisation every plot_interval â€•\n",
    "    if (iter_idx)%plot_interval==0:\n",
    "        fig,axs = plt.subplots(2,2,figsize=(13,10))\n",
    "\n",
    "        im0 = axs[0,0].imshow(ZÎ¼,origin='lower',\n",
    "                              extent=(0,domain_size,0,domain_size),cmap='plasma')\n",
    "        axs[0,0].set_title(f'Iter {iter_idx}: GP mean');     fig.colorbar(im0,ax=axs[0,0])\n",
    "\n",
    "        ZÏƒ_img = ZÏƒ.reshape(res,res)\n",
    "        im1 = axs[0,1].imshow(ZÏƒ_img,origin='lower',\n",
    "                              extent=(0,domain_size,0,domain_size),cmap='plasma')\n",
    "        axs[0,1].set_title('Predicted Ïƒ');                   fig.colorbar(im1,ax=axs[0,1])\n",
    "\n",
    "        axs[1,0].plot(np.arange(len(all_z)),all_z,'.',ms=2)\n",
    "        axs[1,0].set_title('Dynamic measurements'); axs[1,0].set_xlabel('indent #')\n",
    "        axs[1,0].set_ylabel('Hardness (GPa)')\n",
    "\n",
    "        axs[1,1].plot(cost_dyn,label='Dynamic')\n",
    "        axs[1,1].plot(cost_max,'--',label='Fixed 5Ã—5 / 80 s')\n",
    "        axs[1,1].set_title(f'Cumulative cost   MAPE={mape_it:.2f}% | MAE={mae_it:.4f}')\n",
    "        axs[1,1].set_xlabel('block'); axs[1,1].set_ylabel('time (s)')\n",
    "        axs[1,1].legend(); axs[1,1].grid(ls='--',alpha=.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(OUTDIR, f\"iter_{iter_idx:04d}_diagnostics.png\"),\n",
    "                    dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ SAVE: metrics CSV & summary plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.DataFrame(metrics, columns=[\n",
    "    \"iteration\",\"grid_size\",\"n_points\",\"hold_s\",\n",
    "    \"cost_dyn_s\",\"cost_max_s\",\"mape_%\",\"mae\"\n",
    "])\n",
    "csv_path = os.path.join(OUTDIR, \"metrics_adaptive_grid_dynamic_hetero.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Cost vs MAPE (dynamic vs baseline)\n",
    "plt.figure(figsize=(7.8,5.0))\n",
    "plt.plot(df[\"cost_dyn_s\"], df[\"mape_%\"], marker='o', label=\"Dynamic (adaptive grid)\")\n",
    "plt.plot(df[\"cost_max_s\"], df[\"mape_%\"], marker='^', linestyle='--', label=\"Baseline cost ref (5Ã—5/80s)\")\n",
    "plt.xlabel(\"Cumulative cost (s)\")\n",
    "plt.ylabel(\"MAPE (%) vs Z_true\")\n",
    "plt.title(\"Cost vs MAPE\")\n",
    "plt.grid(True, ls='--', alpha=.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"cost_vs_mape.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Iteration vs MAPE\n",
    "plt.figure(figsize=(7.8,5.0))\n",
    "plt.plot(df[\"iteration\"], df[\"mape_%\"], marker='o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MAPE (%)\")\n",
    "plt.title(\"Iteration vs MAPE\")\n",
    "plt.grid(True, ls='--', alpha=.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"iteration_vs_mape.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Time vs Grid Size\n",
    "plt.figure(figsize=(7.8,5.0))\n",
    "plt.step(df[\"cost_dyn_s\"], df[\"grid_size\"], where='post')\n",
    "plt.xlabel(\"Cumulative cost (s)\")\n",
    "plt.ylabel(\"Grid size (g Ã— g)\")\n",
    "plt.title(\"Adaptive grid size vs time\")\n",
    "plt.grid(True, ls='--', alpha=.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"time_vs_grid_size.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Time vs Hold (0/5/80)\n",
    "plt.figure(figsize=(7.8,5.0))\n",
    "plt.step(df[\"cost_dyn_s\"], df[\"hold_s\"], where='post')\n",
    "plt.xlabel(\"Cumulative cost (s)\")\n",
    "plt.ylabel(\"Hold time used (s)\")\n",
    "plt.title(\"Dynamic hold vs time\")\n",
    "plt.grid(True, ls='--', alpha=.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"time_vs_hold.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"[Saved] CSV: {csv_path}\")\n",
    "print(f\"[Saved] Figures every {plot_interval} iters in {OUTDIR}/\")\n",
    "print(f\"[Saved] Summary: cost_vs_mape.png, iteration_vs_mape.png, time_vs_grid_size.png, time_vs_hold.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9bc66",
   "metadata": {},
   "source": [
    "# Defining local \"safe zones for indentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_plane_xy_to_z(points):\n",
    "    \"\"\"\n",
    "    Fit z â‰ˆ a*x + b*y + c from 3 (or more) (x,y,z) points.\n",
    "    Returns (a, b, c, predict_fn).\n",
    "    \"\"\"\n",
    "    P = np.asarray(points, dtype=float)\n",
    "    if P.ndim != 2 or P.shape[1] != 3 or P.shape[0] < 3:\n",
    "        raise ValueError(\"Provide an array-like of shape (N,3) with N>=3.\")\n",
    "    X = np.c_[P[:,0], P[:,1], np.ones(len(P))]   # [x y 1]\n",
    "    z = P[:,2]\n",
    "    a, b, c = np.linalg.lstsq(X, z, rcond=None)[0]\n",
    "    return a, b, c, (lambda x, y: a*x + b*y + c)\n",
    "\n",
    "# Example:\n",
    "pts = [(X_bl,Y_bl, Z_bl), (X_tr,Y_tr, Z_tr), (X_br,Y_br, Z_br)]\n",
    "a, b, c, f = fit_plane_xy_to_z(pts)\n",
    "print(f\"z = {a:.6f}*x + {b:.6f}*y + {c:.6f}\")\n",
    "print(\"z(0.5, 0.5) =\", f(0.5, 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad88c9",
   "metadata": {},
   "source": [
    "Now lets decide boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# â”€â”€â”€ plane & safety spec â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "a,b,c = 0.012, -0.007, 3.0      # z = ax + by + c\n",
    "x0,y0 = 500., 500.\n",
    "z0    = a*x0 + b*y0 + c\n",
    "dz_safe = 10.0/2                # 5 Âµm\n",
    "\n",
    "# â”€â”€â”€ dense cloud of points â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "N = 60_00\n",
    "xs = np.random.uniform(0,1000,N)\n",
    "ys = np.random.uniform(0,1000,N)\n",
    "zs = a*xs + b*ys + c\n",
    "mask = np.abs(zs - z0) <= dz_safe\n",
    "xs_s, ys_s = xs[mask], ys[mask]\n",
    "\n",
    "# bounding box\n",
    "xmin,xmax = xs_s.min(), xs_s.max()\n",
    "ymin,ymax = ys_s.min(), ys_s.max()\n",
    "\n",
    "# â”€â”€â”€ plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(xs_s, ys_s, s=6, c='tab:blue', alpha=.4, label='safe')\n",
    "ax.plot([xmin,xmax,xmax,xmin,xmin],\n",
    "        [ymin,ymin,ymax,ymax,ymin], 'r-', lw=2, label='bbox')\n",
    "ax.scatter([x0],[y0], c='k', s=60, marker='*', label='centre')\n",
    "\n",
    "ax.set_aspect('equal'); ax.set_xlim(0,1000); ax.set_ylim(0,1000)\n",
    "ax.set_xlabel('x (Âµm)'); ax.set_ylabel('y (Âµm)')\n",
    "ax.set_title(fr\"Safeâ€“travel region  (|z â€“ zâ‚€| â‰¤ {dz_safe:.1f} Âµm)\")\n",
    "ax.legend()\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa945c",
   "metadata": {},
   "source": [
    "Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from  scipy.stats import binned_statistic_2d\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Build 1000 Ã— 1000 ternary-composition grid + ground-truth hardness\n",
    "# ------------------------------------------------------------------\n",
    "nx = ny = 1000\n",
    "x_coords, y_coords = np.meshgrid(np.linspace(0, 1, nx),\n",
    "                                 np.linspace(0, 1, ny))\n",
    "A = x_coords\n",
    "B = y_coords\n",
    "C = np.maximum(1.0 - A - B, 0.01)          # keep C â‰¥ 0\n",
    "total = A + B + C                          # renormalise so A+B+C = 1\n",
    "A /= total ;  B /= total ;  C /= total\n",
    "\n",
    "def hardness_model(a,b,c):\n",
    "    base         = 3*a + 5*b + 7*c                    # rule-of-mixtures\n",
    "    ss_strength  = -4*a*b - 3*b*c - 2*c*a             # solid-solution term\n",
    "    saturation   = -6*c**2 * (1 - c)                  # saturation penalty\n",
    "    return base + ss_strength + saturation\n",
    "\n",
    "H_grid = hardness_model(A,B,C)                       # ground-truth hardness\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Composition-dependent noise:  Ïƒ(A,B,C)\n",
    "# ------------------------------------------------------------------\n",
    "Ïƒ0, Î±A, Î±B, Î±C = 0.005, 0.1, 0.05, 0.0250\n",
    "noise_std_grid = Ïƒ0 + Î±A*A + Î±B*B + Î±C*C             # same shape as H_grid\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  One synthetic â€œmeasuredâ€ grid  (H + Gaussian noise)\n",
    "# ------------------------------------------------------------------\n",
    "rng = np.random.default_rng(123)\n",
    "H_meas = H_grid + rng.normal(0, noise_std_grid)      # element-wise Ïƒ\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Re-bin Ïƒ onto a regular (A,B) composition grid for plotting\n",
    "# ------------------------------------------------------------------\n",
    "nbins = 200                                          # resolution in Aâ€“B space\n",
    "Ïƒ_AB, A_edges, B_edges, _ = binned_statistic_2d(\n",
    "        A.ravel(), B.ravel(), noise_std_grid.ravel(),\n",
    "        statistic='mean', bins=nbins, range=[[0,1],[0,1]])\n",
    "Ïƒ_AB = np.flipud(Ïƒ_AB.T)                             # put origin bottom-left\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Plot:  2 Ã— 2   (all imshow, no 3-D)\n",
    "# ------------------------------------------------------------------\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 11))\n",
    "\n",
    "# (0,0) ground-truth hardness in xâ€“y space\n",
    "im0 = axs[0,0].imshow(H_grid, origin='lower', cmap='viridis',\n",
    "                      extent=(0, nx, 0, ny))\n",
    "axs[0,0].set_title(\"Ground-Truth Hardness   $H(x,y)$\")\n",
    "axs[0,0].set_xlabel(\"x-index\") ; axs[0,0].set_ylabel(\"y-index\")\n",
    "fig.colorbar(im0, ax=axs[0,0], shrink=0.8, label=\"GPa\")\n",
    "\n",
    "# (0,1) noise Ïƒ in xâ€“y space\n",
    "im1 = axs[0,1].imshow(noise_std_grid, origin='lower', cmap='inferno',\n",
    "                      extent=(0, nx, 0, ny))\n",
    "axs[0,1].set_title(\"Noise Std   $\\\\sigma(x,y)$\")\n",
    "axs[0,1].set_xlabel(\"x-index\") ; axs[0,1].set_ylabel(\"y-index\")\n",
    "fig.colorbar(im1, ax=axs[0,1], shrink=0.8, label=\"Ïƒ\")\n",
    "\n",
    "# (1,0) noise Ïƒ on regular composition grid (Fraction A vs Fraction B)\n",
    "im2 = axs[1,0].imshow(Ïƒ_AB, origin='lower', cmap='inferno',\n",
    "                      extent=(0,1,0,1), aspect='auto')\n",
    "axs[1,0].set_title(r\"Noise Std   $\\sigma(A,B)$\")\n",
    "axs[1,0].set_xlabel(\"Fraction A\") ; axs[1,0].set_ylabel(\"Fraction B\")\n",
    "fig.colorbar(im2, ax=axs[1,0], shrink=0.8, label=\"Ïƒ\")\n",
    "\n",
    "# (1,1) one synthetic measured grid  (H + noise)\n",
    "im3 = axs[1,1].imshow(H_meas, origin='lower', cmap='plasma',\n",
    "                      extent=(0, nx, 0, ny))\n",
    "axs[1,1].set_title(\"Measured Grid   $H_{meas}(x,y)$\")\n",
    "axs[1,1].set_xlabel(\"x-index\") ; axs[1,1].set_ylabel(\"y-index\")\n",
    "fig.colorbar(im3, ax=axs[1,1], shrink=0.8, label=\"GPa\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22487af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ïƒ0, Î±A, Î±B, Î±C = 0.005, 0.1, 0.05, 0.0250\n",
    "def noise_std_comp(xi:int, yi:int) -> float:\n",
    "    \"\"\"Ïƒ(A,B,C) at integer pixel (xi,yi).\"\"\"\n",
    "    xi = int(np.clip(xi, 0, nx-1))\n",
    "    yi = int(np.clip(yi, 0, ny-1))\n",
    "    return Ïƒ0 + Î±A*A[yi,xi] + Î±B*B[yi,xi] + Î±C*C[yi,xi]\n",
    "\n",
    "def measure_from_Hgrid_hetero(xi:int, yi:int) -> float:\n",
    "    \"\"\"Hardness + Gaussian noise whose std depends on local composition.\"\"\"\n",
    "    xi = int(np.clip(xi, 0, nx-1))\n",
    "    yi = int(np.clip(yi, 0, ny-1))\n",
    "    H  = H_grid[yi, xi]\n",
    "    Ïƒ  = noise_std_comp(xi, yi)\n",
    "    return H + np.random.normal(0, Ïƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PLOT HELPER (v3) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def plot_state(iter_no:int,\n",
    "               gp_m, gp_s,                     # trained GPs\n",
    "               Z_true:np.ndarray,              # coarse GT map  (resÃ—res)\n",
    "               rings:dict,                     # {ring:[(x,y)â€¦]}\n",
    "               a:float, b:float, c:float,      # plane coeffs\n",
    "               dz_safe:float,\n",
    "               field_lim:float,                # Â±Âµm plot window\n",
    "               g_centre:tuple,                 # (xcentre,ycentre) this stay\n",
    "               global_history:list,            # all global centres so far\n",
    "               local_centres_done:list,        # centres visited this stay\n",
    "               local_indents:list,             # every (x,y) indent this stay\n",
    "               cost_dyn:list, cost_base:list): # cumulative-time traces\n",
    "    \"\"\"\n",
    "    Four-panel status figure:\n",
    "        (0,0) safe band & meta-grid\n",
    "        (0,1) GP mean\n",
    "        (1,0) TOTAL predictive Ïƒ  (model + noise)\n",
    "        (1,1) cumulative time cost\n",
    "    \"\"\"\n",
    "    # â”€â”€â”€â”€â”€ 1. predictive maps on same coarse grid as Z_true â”€â”€â”€â”€â”€\n",
    "    res  = Z_true.shape[0]\n",
    "    grid = np.linspace(0, domain_size, res)\n",
    "    XX, YY = np.meshgrid(grid, grid)\n",
    "    test = torch.tensor(np.column_stack([XX.ravel(), YY.ravel()]) / domain_size,\n",
    "                        dtype=torch.float32)\n",
    "    ZÎ¼, ZÏƒ_model, Zvar_noise = predict_hetero(gp_m, gp_s, test)\n",
    "    ZÎ¼         = ZÎ¼.reshape(res, res)\n",
    "    # ZÏƒ_total   = np.sqrt(ZÏƒ_model**2 + Zvar_noise).reshape(res, res)\n",
    "    # total predictive Ïƒ  (model + noise)\n",
    "    ZÏƒ_total = np.sqrt(ZÏƒ_model**2 + Zvar_noise).reshape(res, res)\n",
    "\n",
    "\n",
    "    mape = np.mean(np.abs((Z_true - ZÎ¼) / Z_true)) * 100\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(13, 10))\n",
    "\n",
    "    # â”€â”€â”€ (0,0) SAFE BAND + META GRID â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ax   = axs[0, 0]\n",
    "    xc, yc = g_centre\n",
    "    center = np.array([xc, yc])\n",
    "\n",
    "    # safe-zone lines\n",
    "    xx = np.linspace(-field_lim, field_lim, 2)\n",
    "    upper = (( dz_safe) - a * xx - c) / b\n",
    "    lower = ((-dz_safe) - a * xx - c) / b\n",
    "    ax.plot(xx, upper, 'r--'); ax.plot(xx, lower, 'r--')\n",
    "    ax.fill_between(xx, upper, lower, color='orange', alpha=.15,\n",
    "                    label='safe zone')\n",
    "\n",
    "    # ring rectangles centred on current global point\n",
    "    grad_norm = np.hypot(a, b)\n",
    "    e_perp = np.array([a, b]) / grad_norm\n",
    "    e_par  = np.array([b, -a]) / grad_norm\n",
    "    STEP = 450\n",
    "\n",
    "    for k in range(1, len(rings)):        # skip k=0 (single point)\n",
    "        r = k * STEP\n",
    "        rect_xy = np.array([[xc - r, yc - r],\n",
    "                            [xc + r, yc - r],\n",
    "                            [xc + r, yc + r],\n",
    "                            [xc - r, yc + r],\n",
    "                            [xc - r, yc - r]])\n",
    "        ax.plot(rect_xy[:,0], rect_xy[:,1], 'g-', lw=.8, alpha=.6)\n",
    "\n",
    "\n",
    "    # global centres (black)\n",
    "    if global_history:\n",
    "        gxy = np.asarray(global_history)\n",
    "        ax.scatter(gxy[:,0], gxy[:,1], c='k', s=28, marker='o',\n",
    "                   label='global centres', zorder=5)\n",
    "\n",
    "    # visited local centres (blue line)\n",
    "    if local_centres_done:\n",
    "        lxy = np.asarray(local_centres_done)\n",
    "        ax.plot(lxy[:,0], lxy[:,1], 'b-o', ms=4, lw=1.0,\n",
    "                label='local centres', zorder=6)\n",
    "\n",
    "    # indents this stay (dots)\n",
    "    if local_indents is not None and len(local_indents):\n",
    "        pts = np.asarray(local_indents)\n",
    "        ax.scatter(pts[:,0], pts[:,1], c='royalblue', s=8, alpha=.6,\n",
    "                   label='indents', zorder=4)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(-field_lim, field_lim); ax.set_ylim(-field_lim, field_lim)\n",
    "    ax.set_xlabel('x (Âµm)'); ax.set_ylabel('y (Âµm)')\n",
    "    ax.set_title(f'Iter {iter_no}: safe band & meta-grid')\n",
    "    ax.legend(fontsize=8, loc='upper left')\n",
    "\n",
    "    # â”€â”€â”€ (0,1) GP mean â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    im0 = axs[0,1].imshow(ZÎ¼, origin='lower', extent=(0,1,0,1), cmap='viridis')\n",
    "    axs[0,1].set_title('GP mean');  fig.colorbar(im0, ax=axs[0,1])\n",
    "\n",
    "    # â”€â”€â”€ (1,0) TOTAL predictive Ïƒ (model + noise) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    im1 = axs[1,0].imshow(ZÏƒ_total, origin='lower', extent=(0,1,0,1),\n",
    "                          cmap='inferno')\n",
    "    axs[1,0].set_title('Total predictive Ïƒ'); fig.colorbar(im1, ax=axs[1,0])\n",
    "\n",
    "    # â”€â”€â”€ (1,1) cumulative time cost â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    axs[1,1].plot(cost_dyn,      label='dynamic')\n",
    "    axs[1,1].plot(cost_base, '--', label='baseline 5Ã—5 / 80 s')\n",
    "    axs[1,1].set_xlabel('block'); axs[1,1].set_ylabel('time (s)')\n",
    "    axs[1,1].set_title(f'Cumulative cost   MAPE {mape:.2f}%')\n",
    "    axs[1,1].grid(ls='--', alpha=.5); axs[1,1].legend()\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PLOT HELPER (v5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def plot_state(iter_no:int,\n",
    "               gp_m, gp_s,\n",
    "               Z_true, rings,\n",
    "               a,b,c, dz_safe,\n",
    "               field_lim,\n",
    "               g_centre,\n",
    "               global_history,\n",
    "               local_centres_done,\n",
    "               local_indents,\n",
    "               cost_dyn, cost_base):\n",
    "    \"\"\"\n",
    "    6-panel figure:\n",
    "      (0,0) safe band & meta-grid\n",
    "      (0,1) GP mean\n",
    "      (0,2) model Ïƒ   (epistemic)\n",
    "      (1,0) noise Ïƒ   (aleatoric)\n",
    "      (1,1) empty (spacer)\n",
    "      (1,2) cumulative cost\n",
    "    \"\"\"\n",
    "    # â”€â”€ predictive maps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    res  = Z_true.shape[0]\n",
    "    grid = np.linspace(0, domain_size, res)\n",
    "    XX, YY = np.meshgrid(grid, grid)\n",
    "    test = torch.tensor(np.column_stack([XX.ravel(), YY.ravel()]) / domain_size,\n",
    "                        dtype=torch.float32)\n",
    "\n",
    "    Î¼, noise_var, var_lat = predict_hetero(gp_m, gp_s, test)\n",
    "    Î¼        = Î¼.reshape(res, res)\n",
    "    Ïƒ_model  = np.sqrt(var_lat).reshape(res, res)\n",
    "    Ïƒ_noise  = np.sqrt(noise_var).reshape(res, res)\n",
    "\n",
    "    mape = np.mean(np.abs((Z_true - Î¼) / Z_true)) * 100\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(17, 10))\n",
    "    axs[1,1].axis('off')                     # spacer cell\n",
    "\n",
    "    # â”€â”€ (0,0) safe band & grid â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ax = axs[0,0]\n",
    "    xc, yc = g_centre\n",
    "\n",
    "    z0   = a*xc + b*yc + c   \n",
    "    xx = np.linspace(-field_lim, field_lim, 2)\n",
    "    upper = (( z0+dz_safe) - a*xx - c) / b\n",
    "    lower = ((z0-dz_safe) - a*xx - c) / b\n",
    "    ax.plot(xx, upper, 'r--'); ax.plot(xx, lower, 'r--')\n",
    "    ax.fill_between(xx, upper, lower, color='orange', alpha=.15, label='safe zone')\n",
    "\n",
    "    # green ring rectangles centred *exactly* on (xc, yc)\n",
    "    STEP = 450\n",
    "    for k in range(1, len(rings)):\n",
    "        r = k * STEP\n",
    "        rect = np.array([[xc-r, yc-r],\n",
    "                         [xc+r, yc-r],\n",
    "                         [xc+r, yc+r],\n",
    "                         [xc-r, yc+r],\n",
    "                         [xc-r, yc-r]])\n",
    "        ax.plot(rect[:,0], rect[:,1], 'g-', lw=.8, alpha=.6)\n",
    "\n",
    "    # centres & indents\n",
    "    if global_history:\n",
    "        gxy = np.asarray(global_history)\n",
    "        ax.scatter(gxy[:,0], gxy[:,1], c='k', s=28, label='global centres', zorder=5)\n",
    "    if local_centres_done:\n",
    "        lxy = np.asarray(local_centres_done)\n",
    "        ax.plot(lxy[:,0], lxy[:,1], 'b-o', ms=4, lw=1., label='local centres', zorder=6)\n",
    "    if local_indents is not None and len(local_indents):\n",
    "        pts = np.asarray(local_indents)\n",
    "        ax.scatter(pts[:,0], pts[:,1], c='royalblue', s=8, alpha=.6,\n",
    "                   label='indents', zorder=4)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(-field_lim, field_lim); ax.set_ylim(-field_lim, field_lim)\n",
    "    ax.set_xlabel('x (Âµm)'); ax.set_ylabel('y (Âµm)')\n",
    "    ax.set_title(f'Iter {iter_no}: safe band & meta-grid')\n",
    "    ax.legend(fontsize=8, loc='upper left')\n",
    "\n",
    "    # â”€â”€ (0,1) GP mean â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    im = axs[0,1].imshow(Î¼, origin='lower', extent=(0,1,0,1), cmap='plasma')\n",
    "    axs[0,1].set_title('GP mean'); fig.colorbar(im, ax=axs[0,1])\n",
    "\n",
    "    # â”€â”€ (0,2) model Ïƒ (epistemic) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    im = axs[0,2].imshow(Ïƒ_model, origin='lower', extent=(0,1,0,1), cmap='inferno')\n",
    "    axs[0,2].set_title('Model Ïƒ'); fig.colorbar(im, ax=axs[0,2])\n",
    "\n",
    "    # â”€â”€ (1,0) noise Ïƒ (aleatoric) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    im = axs[1,0].imshow(Ïƒ_noise, origin='lower', extent=(0,1,0,1), cmap='magma')\n",
    "    axs[1,0].set_title('Noise Ïƒ'); fig.colorbar(im, ax=axs[1,0])\n",
    "\n",
    "    # â”€â”€ (1,2) cumulative cost â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    axs[1,2].plot(cost_dyn, label='dynamic')\n",
    "    axs[1,2].plot(cost_base, '--', label='baseline 5Ã—5 / 80 s')\n",
    "    axs[1,2].set_xlabel('block'); axs[1,2].set_ylabel('time (s)')\n",
    "    axs[1,2].set_title(f'Cumulative cost   MAPE {mape:.2f}%')\n",
    "    axs[1,2].grid(ls='--', alpha=.5); axs[1,2].legend()\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac3013",
   "metadata": {},
   "source": [
    "# Meta Grid + Adaptive Grid sizing + Adaptive hold for Drift + Cost aware decision making "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea399823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "from math import sqrt\n",
    "\n",
    "# â€•â€•â€• ASSUMED PREDEFINED in your notebook: â€•â€•â€•\n",
    "#   â€¢ GPModel, train_hetero, predict_hetero\n",
    "#   â€¢ evaluate_sample_cost_remaining\n",
    "#   â€¢ measure_from_Hgrid_hetero\n",
    "#   â€¢ H_grid, A, B, C  (your ternaryâ€model arrays)\n",
    "# â€•â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) SAFEâ€BAND PLANE + RING OFFSETS (at origin)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "a, b, c       = 0.02, 0.03, 0.00\n",
    "dz_safe       = 5.0\n",
    "grad_norm     = sqrt(a*a + b*b)\n",
    "BAND_HALF     = dz_safe / grad_norm\n",
    "\n",
    "e_perp = np.array([ a,  b]) / grad_norm\n",
    "e_par  = np.array([ b, -a]) / grad_norm\n",
    "\n",
    "STEP_RING = 45\n",
    "rings = {}\n",
    "ring = 0\n",
    "while True:\n",
    "    r = ring * STEP_RING\n",
    "    if ring == 0:\n",
    "        pts = [(0,0)]\n",
    "    else:\n",
    "        us = np.arange(-r, r+STEP_RING, STEP_RING)\n",
    "        vs = np.arange(-r+STEP_RING, r, STEP_RING)\n",
    "        pts = [(u, r) for u in us] + [(u, -r) for u in us] \\\n",
    "            + [(r, v) for v in vs] + [(-r, v) for v in vs]\n",
    "    rings[ring] = pts\n",
    "    ring += 1\n",
    "    if r > BAND_HALF + 1e-6:\n",
    "        break\n",
    "\n",
    "def point_in_band_shifted(xc, yc, z0):\n",
    "    return abs(a*xc + b*yc + c - z0) <= dz_safe + 1e-9\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) GLOBAL GRID: 0â€“10â€‰mm at 50â€‰Âµm pitch\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "domain_size = 10_00\n",
    "STEP_GLOBAL = 10\n",
    "xs2 = np.arange(0, domain_size+1, STEP_GLOBAL)\n",
    "ys2 = np.arange(0, domain_size+1, STEP_GLOBAL)\n",
    "global_pool = [(x,y) for x in xs2 for y in ys2]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) CONSTANTS, OFFSETS & GROUNDâ€TRUTH\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t_setup, t_move       = 20, 20\n",
    "default_t_drift       = 300\n",
    "default_t_measurement = 40\n",
    "t_comeback            = 35\n",
    "\n",
    "g_min, g_max          = 2, 5\n",
    "target_sem            = 0.5        # forces g_curr = g_max if zero\n",
    "n_initial, n_steps    = 2, 40\n",
    "plot_interval         = 1\n",
    "\n",
    "min_hold, med_hold, max_hold = 0, 5, 80\n",
    "eval_interval                = 5\n",
    "user_threshold_pct           = 4.0\n",
    "\n",
    "t_event, t_event_end = 7_000., 45_000.\n",
    "decay_tau            = 1_000.\n",
    "bias_amp5, bias_amp0 = 0.30, 0.50\n",
    "\n",
    "num_center = 1  # how many local grids per block\n",
    "\n",
    "spacing     = 5\n",
    "ix, iy      = np.meshgrid(np.arange(g_max), np.arange(g_max))\n",
    "offsets_5x5 = np.stack([ix.ravel(), iy.ravel()], axis=1) * spacing\n",
    "offsets_ref = offsets_5x5.copy()\n",
    "\n",
    "ny, nx = H_grid.shape\n",
    "res    = 25\n",
    "Z_true = zoom(H_grid, zoom=res/ny, order=1)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) PLOT HELPER (6 panels)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def plot_state(step, gp_m, gp_s, Z_true,\n",
    "               rings, a,b,c, dz_safe, field_lim, g_centre,\n",
    "               visited_globals, drilled_centres, drilled_points,\n",
    "               cost_dyn, cost_base, title_suffix=\"\"):\n",
    "\n",
    "    grid = np.linspace(0, field_lim, res)\n",
    "    XX, YY = np.meshgrid(grid, grid)\n",
    "    test = torch.tensor(\n",
    "        np.column_stack([XX.ravel(), YY.ravel()]) / field_lim,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    Î¼_map, noise_map, var_lat_map = predict_hetero(gp_m, gp_s, test)\n",
    "    Î¼_map       = Î¼_map.reshape(res,res)\n",
    "    Ïƒ_model_map = np.sqrt(var_lat_map).reshape(res,res)\n",
    "    Ïƒ_noise_map = noise_map.reshape(res,res)\n",
    "    mape        = np.mean(np.abs((Z_true - Î¼_map)/Z_true))*100\n",
    "\n",
    "    fig, axs = plt.subplots(2,3, figsize=(18,10))\n",
    "    axs[1,1].axis('off')\n",
    "\n",
    "    gx, gy = g_centre\n",
    "    z0 = a*gx + b*gy + c\n",
    "    xx = np.linspace(-field_lim, field_lim, 2)\n",
    "    upper = ((z0+dz_safe)-a*xx-c)/b\n",
    "    lower = ((z0-dz_safe)-a*xx-c)/b\n",
    "\n",
    "    ax = axs[0,0]\n",
    "    ax.plot(xx,upper,'r--'); ax.plot(xx,lower,'r--')\n",
    "    ax.fill_between(xx,upper,lower,color='orange',alpha=0.2)\n",
    "    for k, uv_list in rings.items():\n",
    "        if k==0: continue\n",
    "        r = k*STEP_RING\n",
    "        corners = [(-r,-r),(r,-r),(r,r),(-r,r),(-r,-r)]\n",
    "        rect = np.array([\n",
    "            (gx + u*e_par[0] + v*e_perp[0],\n",
    "             gy + u*e_par[1] + v*e_perp[1])\n",
    "            for u,v in corners])\n",
    "        ax.plot(rect[:,0], rect[:,1],'g-',lw=0.8,alpha=0.6)\n",
    "\n",
    "    if visited_globals:\n",
    "        vg = np.array(visited_globals)\n",
    "        ax.scatter(vg[:,0],vg[:,1],c='k',s=30, label='global centres')\n",
    "    if drilled_centres:\n",
    "        dc = np.array(drilled_centres)\n",
    "        ax.plot(dc[:,0],dc[:,1],'b-o',ms=4,label='local centres')\n",
    "    if drilled_points:\n",
    "        dp = np.array(drilled_points)\n",
    "        ax.scatter(dp[:,0],dp[:,1],c='royalblue',s=8,alpha=0.6,label='indents')\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f\"Iter {step}{title_suffix}\")\n",
    "    ax.legend(fontsize=8, loc='upper left')\n",
    "\n",
    "    im = axs[0,1].imshow(Î¼_map,origin='lower',extent=(0,1,0,1),cmap='plasma')\n",
    "    axs[0,1].set_title('GP mean');     fig.colorbar(im,ax=axs[0,1])\n",
    "\n",
    "    im = axs[0,2].imshow(Ïƒ_model_map,origin='lower',extent=(0,1,0,1),cmap='inferno')\n",
    "    axs[0,2].set_title('Model Ïƒ');    fig.colorbar(im,ax=axs[0,2])\n",
    "\n",
    "    im = axs[1,0].imshow(Ïƒ_noise_map,origin='lower',extent=(0,1,0,1),cmap='magma')\n",
    "    axs[1,0].set_title('Noise Ïƒ');    fig.colorbar(im,ax=axs[1,0])\n",
    "\n",
    "    axs[1,2].plot(cost_dyn,      label='dynamic')\n",
    "    axs[1,2].plot(cost_base,'--',label='baseline 5Ã—5/80s')\n",
    "    axs[1,2].set_title(f'Cumulative cost   MAPE {mape:.2f}%')\n",
    "    axs[1,2].set_xlabel('block'); axs[1,2].set_ylabel('time (s)')\n",
    "    axs[1,2].grid(ls='--',alpha=0.5); axs[1,2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) INITIAL GLOBAL DRILLS (no safeâ€band)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_x, all_y, all_z = [], [], []\n",
    "cost_dyn, cost_base = [], []\n",
    "tot_cost_dyn = tot_cost_base = 0.0\n",
    "visited_globals = []\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "init_idxs = rng.choice(len(global_pool), size=n_initial, replace=False)\n",
    "for idx in sorted(init_idxs, reverse=True):\n",
    "    gx, gy = global_pool.pop(idx)\n",
    "    visited_globals.append((gx,gy))\n",
    "    print(f\"[Init] drilling global centre = ({gx},{gy}) @ 5Ã—5, hold={max_hold}s\")\n",
    "\n",
    "    blk = offsets_5x5 + np.array([gx,gy])\n",
    "    xs, ys = blk[:,0], blk[:,1]\n",
    "    zs = np.array([measure_from_Hgrid_hetero(x,y) for x,y in zip(xs,ys)],\n",
    "                  dtype=np.float32)\n",
    "\n",
    "    all_x.extend(xs); all_y.extend(ys); all_z.extend(zs.tolist())\n",
    "\n",
    "    tm = default_t_measurement + max_hold\n",
    "    cm = evaluate_sample_cost_remaining(xs, ys,\n",
    "           t_drift       = default_t_drift,\n",
    "           t_measurement = tm,\n",
    "           t_comeback    = t_comeback)[0]\n",
    "    step_cost = t_setup + t_move + cm\n",
    "\n",
    "    tot_cost_dyn  += step_cost\n",
    "    tot_cost_base += step_cost\n",
    "    cost_dyn.append(tot_cost_dyn)\n",
    "    cost_base.append(tot_cost_base)\n",
    "\n",
    "g_curr    = g_max\n",
    "prev_hold = max_hold\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) MAIN ACTIVEâ€LEARNING LOOP\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for step in range(1, n_steps+1):\n",
    "    print(f\"\\n===== ITER {step} =====\")\n",
    "    # â”€â”€â”€ 6.1) Fit heteroâ€GP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    train_inputs  = torch.tensor(\n",
    "        np.column_stack([all_x, all_y]) / domain_size,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    train_targets = torch.tensor(\n",
    "        np.array(all_z, dtype=np.float32),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    M = min(300, train_inputs.size(0))\n",
    "    inducing = train_inputs[torch.randperm(train_inputs.size(0))[:M]] \\\n",
    "            + 1e-3*torch.randn(M,2)\n",
    "    gp_m, gp_s = GPModel(inducing), GPModel(inducing)\n",
    "    train_hetero(train_inputs, train_targets, gp_m, gp_s, iters=250, lr=1e-2)\n",
    "    print(f\"â†’ GP fit on {train_targets.shape[0]} points\")\n",
    "\n",
    "    # â”€â”€â”€ 6.2) GLOBAL acquisition & debug â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    acqs = []\n",
    "    for (cx, cy) in global_pool:\n",
    "        block = offsets_5x5[:g_curr**2] + np.array([cx, cy])\n",
    "        Xb    = torch.tensor(block / domain_size, dtype=torch.float32)\n",
    "        _, _, var_lat = predict_hetero(gp_m, gp_s, Xb)\n",
    "        sigma_block  = np.sqrt(var_lat).mean()\n",
    "        est_cost     = evaluate_sample_cost_remaining(\n",
    "                        block[:,0], block[:,1],\n",
    "                        t_drift       = default_t_drift,\n",
    "                        t_measurement = default_t_measurement + prev_hold,\n",
    "                        t_comeback    = t_comeback\n",
    "                    )[0]\n",
    "        acqs.append(sigma_block / (t_setup + t_move + est_cost))\n",
    "\n",
    "    acqs = np.array(acqs)\n",
    "\n",
    "    # debug top-4\n",
    "    ranked4 = sorted(zip(global_pool, acqs),\n",
    "                    key=lambda iv: iv[1], reverse=True)[:4]\n",
    "    print(f\"[Iter {step}] Top-4 globals (cx,cy â†’ UE/cost):\")\n",
    "    for (cx, cy), score in ranked4:\n",
    "        print(f\"   ({cx:6.1f},{cy:6.1f}) â†’ {score:.6f}\")\n",
    "\n",
    "    # pick best\n",
    "    best_idx = int(np.argmax(acqs))\n",
    "    gx, gy   = global_pool.pop(best_idx)\n",
    "    visited_globals.append((gx, gy))\n",
    "    print(f\"â†’ Selected global = ({gx:.1f},{gy:.1f}), score = {acqs[best_idx]:.6f}\")\n",
    "\n",
    "\n",
    "    # 6.3) gridâ€size adapt via block Ïƒ\n",
    "    blk25 = offsets_5x5 + np.array([gx,gy])\n",
    "    X25   = torch.tensor(blk25/domain_size,dtype=torch.float32)\n",
    "    _, noise25, var25 = predict_hetero(gp_m, gp_s, X25)\n",
    "    Ïƒ_blk = np.sqrt(var25).mean().item()\n",
    "    if target_sem>0:\n",
    "        n_req = int(np.ceil((Ïƒ_blk/target_sem)**2))\n",
    "        g_curr = min(max(int(np.ceil(np.sqrt(n_req))),g_min),g_max)\n",
    "    else:\n",
    "        g_curr = g_max\n",
    "    print(f\"â†’ block Ïƒ={Ïƒ_blk:.4f} â†’ g_curr={g_curr}\")\n",
    "\n",
    "    # 6.4) find local centres in band\n",
    "    z0 = a*gx + b*gy + c\n",
    "    local_centres = []\n",
    "    for k, uv_list in rings.items():\n",
    "        for u,v in uv_list:\n",
    "            xc = gx + u*e_par[0] + v*e_perp[0]\n",
    "            yc = gy + u*e_par[1] + v*e_perp[1]\n",
    "            if point_in_band_shifted(xc,yc,z0):\n",
    "                local_centres.append((xc,yc))\n",
    "                if len(local_centres)>=num_center: break\n",
    "        if len(local_centres)>=num_center: break\n",
    "    print(f\"â†’ Local centres: {local_centres}\")\n",
    "\n",
    "    # 6.5) compute cost_vec\n",
    "    all_loc = np.vstack([offsets_5x5[:g_curr**2]+np.array(c)\n",
    "                         for c in local_centres])\n",
    "    cost_vec = evaluate_sample_cost_remaining(\n",
    "        all_loc[:,0], all_loc[:,1],\n",
    "        t_drift       = default_t_drift,\n",
    "        t_measurement = default_t_measurement+prev_hold,\n",
    "        t_comeback    = t_comeback\n",
    "    )\n",
    "    print(f\"â†’ cost_vec length {len(cost_vec)}\")\n",
    "\n",
    "        # â€¦ after youâ€™ve built local_centres and precomputed cost_vec â€¦\n",
    "\n",
    "    # 6.6) LOCAL loop with â€œstayâ€vsâ€goâ€ decision\n",
    "    drilled_centres, drilled_points = [], []\n",
    "    local_i = 0\n",
    "    gpts    = g_curr**2\n",
    "\n",
    "    while local_i < len(local_centres):\n",
    "        lx, ly = local_centres[local_i]\n",
    "        print(f\"    â†’ drilling local centre #{local_i} = ({lx:.1f},{ly:.1f})\")\n",
    "        blk_local = offsets_5x5[:gpts] + np.array([lx, ly])\n",
    "        xs_loc, ys_loc = blk_local[:,0], blk_local[:,1]\n",
    "        zs_loc = np.array([measure_from_Hgrid_hetero(x,y)\n",
    "                           for x,y in zip(xs_loc,ys_loc)], dtype=np.float32)\n",
    "\n",
    "        # record the drill\n",
    "        drilled_centres.append((lx,ly))\n",
    "        drilled_points.extend(blk_local.tolist())\n",
    "        all_x.extend(xs_loc); all_y.extend(ys_loc); all_z.extend(zs_loc.tolist())\n",
    "\n",
    "        # â¶ compute Î”â€cost of drilling this one local grid\n",
    "        i0 = local_i * gpts\n",
    "        i1 = i0 + gpts\n",
    "        c0 = cost_vec[i0]\n",
    "        c1 = cost_vec[i1] if i1 < len(cost_vec) else 0.0\n",
    "        Î”c = c0 - c1\n",
    "        tot_cost_dyn += Î”c\n",
    "        cost_dyn.append(tot_cost_dyn)\n",
    "        print(f\"       Î”-cost = {Î”c:.2f} s, tot_cost_dyn = {tot_cost_dyn:.2f} s\")\n",
    "\n",
    "        # quick GP update with new local measurements\n",
    "\n",
    "        # build tensors for the new local block\n",
    "        X_loc = torch.tensor(blk_local / domain_size, dtype=torch.float32)  # shape (gpts,2)\n",
    "        y_loc = torch.tensor(zs_loc,            dtype=torch.float32)        # shape (gpts,)\n",
    "\n",
    "        # concatenate into your training set\n",
    "        train_inputs  = torch.cat([train_inputs,  X_loc], dim=0)\n",
    "        train_targets = torch.cat([train_targets, y_loc], dim=0)\n",
    "\n",
    "        print(\"  â†’ After concat: inputs:\", train_inputs.shape,\n",
    "            \" targets:\", train_targets.shape)\n",
    "\n",
    "        # refit or continue training\n",
    "        train_hetero(train_inputs, train_targets, gp_m, gp_s, iters=120, lr=5e-3)\n",
    "       \n",
    "        \n",
    "        # â¶ recompute best possible GLOBAL gain (UE/cost) over remaining global_pool\n",
    "        best_global = -np.inf\n",
    "        for (cxg, cyg) in global_pool:\n",
    "            cand = offsets_5x5[:gpts] + np.array([cxg, cyg])\n",
    "            _, _, var_lat = predict_hetero(\n",
    "                gp_m, gp_s,\n",
    "                torch.tensor(cand/domain_size, dtype=torch.float32)\n",
    "            )\n",
    "            ug = np.sqrt(var_lat).mean()\n",
    "            cm = evaluate_sample_cost_remaining(\n",
    "                cand[:,0], cand[:,1],\n",
    "                t_drift       = default_t_drift,\n",
    "                t_measurement = default_t_measurement + prev_hold,\n",
    "                t_comeback    = t_comeback\n",
    "            )[0]\n",
    "            best_global = max(best_global, ug / (t_setup + t_move + cm))\n",
    "\n",
    "        # â· compute the GAIN of drilling **one more** local grid\n",
    "        if local_i+1 < len(local_centres):\n",
    "            start = (local_i+1)*gpts\n",
    "            nxt_block = all_loc[start : start + gpts]\n",
    "            _, _, var_nxt = predict_hetero(\n",
    "                gp_m, gp_s,\n",
    "                torch.tensor(nxt_block/domain_size, dtype=torch.float32)\n",
    "            )\n",
    "            ug_next = np.sqrt(var_nxt).mean()\n",
    "            # Î”â€cost for that next grid:\n",
    "            c0n = cost_vec[(local_i+1)*gpts]\n",
    "            c1n = cost_vec[(local_i+2)*gpts] if (local_i+2)*gpts < len(cost_vec) else 0.0\n",
    "            delta_next = c0n - c1n\n",
    "            gain_loc = ug_next / max(delta_next, 1e-6)\n",
    "        else:\n",
    "            gain_loc = -np.inf\n",
    "\n",
    "        print(f\"       gain_loc = {gain_loc:.6f}, best_global = {best_global:.6f}\")\n",
    "\n",
    "        # â¸ the â€œstayâ€vsâ€goâ€ test\n",
    "        if gain_loc >= best_global:\n",
    "            print(\"       â‡’ STAY local for one more grid\")\n",
    "            local_i += 1\n",
    "            continue\n",
    "        else:\n",
    "            print(\"       â‡’ GO to next global centre\")\n",
    "            break\n",
    "\n",
    "\n",
    "    # 6.7) driftâ€hold decision\n",
    "    t_now = tot_cost_dyn\n",
    "    in_evt = (t_event<=t_now<=t_event_end)\n",
    "    err_5  = (0.5*np.exp(-(t_now-t_event)/decay_tau)) if in_evt else 0.0\n",
    "\n",
    "    Î¼_blk = gp_m(X25).mean.mean().item()\n",
    "    # Î¼_blk = Î¼_blk*(y.max()-y.min()) + y.min()\n",
    "    y_arr = np.array(all_z, dtype=np.float32)\n",
    "    y_range = y_arr.max() - y_arr.min()\n",
    "    Î¼_blk = Î¼_blk * y_range + y_arr.min()\n",
    "    T_var = 0.0\n",
    "    T_var = -1 ######################## deciding it cannot be 0\n",
    "    T_big = max(user_threshold_pct*Î¼_blk/100.,2*T_var)\n",
    "    if prev_hold==0:\n",
    "        hold = med_hold if (step%eval_interval)==0 else min_hold\n",
    "    else:\n",
    "        hold = (min_hold if err_5<=T_var\n",
    "                else med_hold if err_5<T_big\n",
    "                else max_hold)\n",
    "    prev_hold = hold\n",
    "    print(f\"â†’ hold={hold}s  (err_5={err_5:.4f}, T_big={T_big:.4f})\")\n",
    "\n",
    "    # 6.8) pad baseline\n",
    "    ref_xy = offsets_ref + np.array([gx,gy])\n",
    "    cb     = evaluate_sample_cost_remaining(\n",
    "        ref_xy[:,0], ref_xy[:,1],\n",
    "        t_drift       = default_t_drift,\n",
    "        t_measurement = default_t_measurement+max_hold,\n",
    "        t_comeback    = t_comeback)[0]\n",
    "    one5x5 = t_setup + t_move + cb\n",
    "    while len(cost_base)<len(cost_dyn):\n",
    "        tot_cost_base += one5x5\n",
    "        cost_base.append(tot_cost_base)\n",
    "    print(f\"â†’ baseline padded to {len(cost_base)} steps\")\n",
    "\n",
    "    # 6.9) plot\n",
    "    if step%plot_interval==0:\n",
    "        plot_state(step, gp_m, gp_s, Z_true,\n",
    "                   rings, a,b,c, dz_safe, domain_size,\n",
    "                   (gx,gy), visited_globals,\n",
    "                   drilled_centres, drilled_points,\n",
    "                   cost_dyn, cost_base,\n",
    "                   title_suffix=\" (postâ€train)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
